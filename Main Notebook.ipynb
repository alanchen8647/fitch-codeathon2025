{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "862e03e1-fa42-4e17-af28-f8214bced492",
   "metadata": {},
   "source": [
    "# FitchGroup Codeathon '25 - Drive Sustainability Using AI - Group 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55270d34-96fd-4655-9c05-02751ffc90f0",
   "metadata": {},
   "source": [
    "### Problem Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d176b0-0ca5-4033-8f34-a3388e6f8acc",
   "metadata": {},
   "source": [
    "#### What are we predicting?\n",
    "\n",
    "We're predicting greenhouse gas emissions for companies that don't publicly report them, using two targets:\n",
    "\n",
    "- **Scope 1 Emissions** - Direct emissions from sources the company owns/controls\n",
    "\n",
    "    Examples: burning fuel in company vehicles, emissions from company-owned factories, on-site manufacturing processes\n",
    "\n",
    "- **Scope 2 Emissions** - Indirect emissions from purchased energy\n",
    "\n",
    "    Examples: electricity bought from the grid, purchased steam/heating/cooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b06377e-b1ec-4316-9bc0-d55575335ef5",
   "metadata": {},
   "source": [
    "#### Why does this matter?\n",
    "Many companies don't report their emissions, making it hard for investors, regulators, and sustainability analysts to assess climate risk. Our models will estimate emissions for these non-reporting companies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66f7b22-630c-4e4b-8da5-efe0023a600a",
   "metadata": {},
   "source": [
    "### Key Hypotheses We Can Form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b547ad-58a2-46e3-806e-2b8c98740c30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Hypothesis 1: Sector Mix Drives Emissions\n",
    "\n",
    "Different industries have vastly different emission intensities.\n",
    "\n",
    "Specific predictions:\n",
    "\n",
    "- Manufacturing, mining, energy → HIGH Scope 1 (they burn fuel, run machinery)\n",
    "- Utilities, heavy electricity users → HIGH Scope 2 (they buy lots of power)\n",
    "- Services, software, consulting → LOW both (mostly office work)\n",
    "\n",
    "Features to test this:\n",
    "\n",
    "- Percentage in high-emission sectors\n",
    "- Dominant sector\n",
    "- Sector diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de1712b-9e37-4bd7-8844-ee51ffe2e39e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Hypothesis 2: Revenue as a Scaling Factor\n",
    "\n",
    "Bigger companies (by revenue) generally emit more, but emission intensity (emissions per dollar of revenue) varies by sector.\n",
    "\n",
    "Specific predictions:\n",
    "\n",
    "- 2x revenue in manufacturing ≈ 2x emissions\n",
    "- 2x revenue in consulting ≠ 2x emissions (office space doesn't scale linearly)\n",
    "\n",
    "Features to test this:\n",
    "\n",
    "- Raw revenue\n",
    "- Log(revenue) - to handle the wide range\n",
    "- Interaction: revenue × sector exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22cf8b-3c8b-471c-83d4-6d12f24ce9e2",
   "metadata": {},
   "source": [
    "#### Hypothesis 3: Geography Matters for Scope 2\n",
    "Scope 2 depends heavily on where you buy electricity because grid carbon intensity varies by country.\n",
    "\n",
    "Specific predictions:\n",
    "\n",
    "Same company, same electricity use:\n",
    "\n",
    "- In coal-heavy grid (e.g., Poland, China) → HIGH Scope 2\n",
    "- In renewable-heavy grid (e.g., Norway, Iceland) → LOW Scope 2\n",
    "\n",
    "Features to test this:\n",
    "\n",
    "- Country/region codes\n",
    "- Target encoding: average Scope 2 by country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c8514-745b-465b-b366-4aa6d98b6098",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Hypothesis 4: Environmental Score Reflects Emissions Management\n",
    "\n",
    "Companies with better environmental scores likely have emissions reduction programs.\n",
    "\n",
    "Specific predictions:\n",
    "- Lower environmental_score (1 = best) → LOWER emissions\n",
    "- Higher environmental_score (5 = worst) → HIGHER emissions\n",
    "- This effect is stronger for Scope 1 (more controllable) than Scope 2\n",
    "\n",
    "Features to test this:\n",
    "- Environmental score (raw and transformed)\n",
    "- Environmental activities (positive vs negative adjustments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e5dd60-3d37-4c7d-8d94-fc66764fa93f",
   "metadata": {},
   "source": [
    "#### Hypothesis 5: Scope 1 vs Scope 2 Have Different Drivers\n",
    "The two targets are fundamentally different and need different features.\n",
    "\n",
    "Scope 1 (Direct) is driven by:\n",
    "\n",
    "- Manufacturing intensity\n",
    "- Fuel combustion activities\n",
    "- Industrial processes\n",
    "- Company's own operations\n",
    "\n",
    "Scope 2 (Indirect) is driven by:\n",
    "\n",
    "- Electricity consumption\n",
    "- Grid carbon intensity (geography)\n",
    "- Office/facility scale\n",
    "- Purchased energy\n",
    "\n",
    "Implication: \n",
    "\n",
    "We might need:\n",
    "\n",
    "- Separate models for Scope 1 vs Scope 2, OR\n",
    "- Multi-output model that learns different weights for each target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e3653-623b-4c41-91b9-92d38fd94f2f",
   "metadata": {},
   "source": [
    "#### Hypothesis 6: Revenue Concentration Affects Emission Predictability\n",
    "Companies focused on one sector are more predictable than diversified conglomerates.\n",
    "\n",
    "Specific predictions:\n",
    "\n",
    "- Specialized company (HHI = 10,000): Emissions closely match sector average\n",
    "- Diversified company (HHI = 2,500): Emissions are averaged across sectors, harder to predict\n",
    "\n",
    "Features to test this:\n",
    "\n",
    "- Herfindahl index\n",
    "- Entropy\n",
    "- Number of sectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9a142d-1307-4512-b24b-432dd6994fae",
   "metadata": {},
   "source": [
    "#### Hypothesis 7: Environmental Activities Are Leading Indicators\n",
    "Companies actively managing environmental impact will have different emissions than expected from sector alone.\n",
    "\n",
    "Specific predictions:\n",
    "\n",
    "- Positive activities (renewable energy, efficiency programs) → LOWER emissions than sector average\n",
    "- Negative activities (pollution incidents, violations) → HIGHER emissions\n",
    "\n",
    "Features to test this:\n",
    "\n",
    "- Count of positive vs negative activities\n",
    "- Net environmental score adjustment\n",
    "- Presence of any activities (vs none)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a02d0-bc6c-49d7-9cef-d73a11d751f5",
   "metadata": {},
   "source": [
    "#### Hypothesis 8: SDG Commitments Signal Intent\n",
    "\n",
    "Companies committed to climate-related SDGs are working on emissions reduction.\n",
    "\n",
    "Specific predictions:\n",
    "\n",
    "- Committed to SDG 7 (Clean Energy) or SDG 13 (Climate Action) → LOWER emissions\n",
    "- More SDG commitments overall → more mature sustainability programs → LOWER emissions\n",
    "\n",
    "Features to test this:\n",
    "\n",
    "- Binary flags for climate SDGs\n",
    "- Total number of SDG commitments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7094f0-cb76-47ac-8ec2-b21193ee436c",
   "metadata": {},
   "source": [
    "#### Hypothesis 9: Scale sets the baseline\n",
    "\n",
    "Larger organizations should emit disproportionately more, but only up to a point.\n",
    "    \n",
    "Specific predictions:\n",
    "\n",
    "- log transformed revenue and regional revenue residuals will correlate positively with both scopes, revealing giants and over-performers within each country.\n",
    "\n",
    "Features to test this:\n",
    "\n",
    "- `log_revenue`, `log_rev_country_resid`, `logrev_x_highintensity`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5294388e-ac54-43f5-812c-31738e810ab0",
   "metadata": {},
   "source": [
    "#### Hypothesis 10: Sector mix drives structural emissions\n",
    "\n",
    "Firms with higher revenue share in carbon-heavy industries should emit above peers even when similar in size.\n",
    "    \n",
    "Specific predictions:\n",
    "\n",
    "- sector concentration, heavy-intensity share, and manufacturing prevalence will show strong positive correlations.\n",
    "\n",
    "Features to test this:\n",
    "\n",
    "- `sect_high_intensity_pct`, `sect_C_pct`, `sect_hhi`, `sect_entropy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1dc18-6070-4a5a-80b2-bc7c21360f1c",
   "metadata": {},
   "source": [
    "#### Hypothesis 11: Governance gaps signal unmanaged carbon risk\n",
    "\n",
    "When governance scores outpace environmental scores, execution lags strategy, leading to higher emissions than expected.\n",
    "\n",
    "Specific predictions:\n",
    "\n",
    "- governance-environment score gaps correlate with increased scopes, while absolute governance scores correlate negatively.\n",
    "\n",
    "Features to test this:\n",
    "\n",
    "- `gov_env_gap`, `governance_score`, `governance_score_squared`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615fa02e-3211-4124-886d-6da6fd7917be",
   "metadata": {},
   "source": [
    "#### Hypothesis 12: Environmental action mix is a leading indicator\n",
    "\n",
    "Companies running more positive programs (renewables, efficiency) should under-shoot sector averages, while negative events push emissions higher.\n",
    "\n",
    "Specific predictions:\n",
    "\n",
    "- counts/ratios of positive vs. negative activities, net adjustments, and \"has activity\" flags explain residual variance after sectors.\n",
    "\n",
    "Features to test this:\n",
    "\n",
    "- `env_positive_cnt`, `env_positive_ratio`, `env_adj_per_event`, `has_env_activity`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5f4666-43b8-4782-b05b-5fb65730de14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Hypothesis 13: SDG climate commitments amplify or mitigate intensity\n",
    "\n",
    "Companies that actively pledge SDGs tied to climate and energy (7/12/13) should curb emissions relative to peers with similar industrial exposure.\n",
    "\n",
    "Specific predictions:\n",
    "\n",
    "- SDG flags/cluster counts interact with sector intensity to reduce emissions relative to peers.\n",
    "\n",
    "Features to test this:\n",
    "\n",
    "- `sdg_climate_focus`, `climate_focus_x_intensity`, `sdg_focus_x_envgap`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765e1cf3",
   "metadata": {},
   "source": [
    "#### Hypothesis 14: Social and governance balance matters\n",
    "\n",
    "When social and overall ESG momentum outpaces environmental execution, the imbalance should show up as excess emissions unless offset by scale-adjusted investments.\n",
    "\n",
    "Specific predictions:\n",
    "\n",
    "- larger positive gaps correlate with higher emissions unless offset by scale-adjusted investments.\n",
    "\n",
    "Features to test this:\n",
    "\n",
    "- `env_overall_gap`, `social_env_gap`, `logrev_x_envgap`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d19a124-8e6c-4a56-84d3-cb4e5c75c32e",
   "metadata": {},
   "source": [
    "#### Hypothesis 15: PPP-Adjusted Revenue Reflects True Emission Scale\n",
    "\n",
    "Raw revenue alone may not accurately reflect operational scale across countries with different cost levels. Adjusting revenue by Purchasing Power Parities (PPP) normalizes for local price differences, giving a better estimate of real production and energy use.\n",
    "\n",
    "Specific predictions:\n",
    "\n",
    "- Companies in countries with high nominal revenue but low PPP → actual emissions may be lower than raw revenue suggests\n",
    "\n",
    "- Companies in countries with lower nominal revenue but high PPP → actual emissions may be higher than raw revenue suggests\n",
    "\n",
    "Features to test:\n",
    "\n",
    "- revenue / PPP → PPP-adjusted revenue\n",
    "\n",
    "- Interaction with region (Western Europe, North America) → captures regional energy mix differences\n",
    "\n",
    "- Interaction with sector → aligns scaled revenue with industry-specific emission intensities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b73746",
   "metadata": {},
   "source": [
    "#### Hypothesis 16: Exponentially-Weighted SDG Impact Signals Stronger Emission Reduction Behavior\n",
    "\n",
    "Companies that commit to high-impact SDGs—and especially those mapped to exponentially larger impact weights—are far more likely to invest in cleaner operations, energy efficiency, and reporting discipline. The exponential scaling captures the idea that not all SDGs contribute equally: a commitment to Climate Action (SDG 13) should matter far more than Peace & Justice (SDG 16).\n",
    "\n",
    "Specific predictions:\n",
    "\n",
    "- Entities with high exponential SDG impact scores → LOWER Scope 1 and Scope 2 emissions (strong sustainability alignment)\n",
    "\n",
    "- Entities with low or flat impact profiles → HIGHER emissions (minimal environmental commitment)\n",
    "\n",
    "- A single high-impact SDG can outweigh several low-impact ones because exponential weights amplify true environmental focus\n",
    "\n",
    "Features to test:\n",
    "\n",
    "- Sum of exponential impact indexes per entity (overall Environmental Impact Score)\n",
    "\n",
    "- Average exponential impact index (normalizes by number of SDGs)\n",
    "\n",
    "- Maximum exponential impact index the entity has (captures “strongest” SDG commitment)\n",
    "\n",
    "- Standard deviation of exponential impact indexes (captures whether commitments are concentrated on high-impact environmental goals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48303b0-c41b-4cf9-a914-6fe97d5ce295",
   "metadata": {},
   "source": [
    "#### Hypothesis 17: Environmental Reporting Intensity Signals Company Size\n",
    "Statement: \n",
    "Entities with more environmental activities recorded (count or diversity) also have higher Scope 1 emissions and higher Scope 2 emissions.\n",
    "because:\n",
    "\n",
    "* large companies are more regulated\n",
    "\n",
    "* large companies must report ESG activities\n",
    "\n",
    "* large companies have sustainability departments\n",
    "\n",
    "Enviornment Activities Adjustment auditors focus on large emitters\n",
    "\n",
    "Features to test this:\n",
    "- `env_activities_count`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24da2b7-fb7b-44e0-b50b-3bd9141147b5",
   "metadata": {},
   "source": [
    "#### Hypothesis 18: Environmental Activity Diversity Predicts Emissions\n",
    "The company who engage in a wider range of environmental-related activities are likely operating multiple processes, facilities, or business units, each contributing to greenhouse gas emissions\n",
    "\n",
    "Predictions:\n",
    "\n",
    "* greater environmental activity diversity => higher direct emissions (Scope 1) and higher electricity-related emissions (Scope 2).\n",
    "\n",
    "Features to test this:\n",
    "\n",
    "* `env_activity_types`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a6a70-e058-4a7f-8396-e1cc4c017c8e",
   "metadata": {},
   "source": [
    "#### Hypothesis 19: Environmental Reporting Only Predicts Emissions When Combined With Company Size\n",
    "Large companies will operate more facilities, consume more energy, and emit more greenhouse gasses. Hints to these companies are more likely to track environmental activities, more disclose environmental actions, and will be required to report sustainability efforts publicly.\n",
    "\n",
    "Predictions:\n",
    "\n",
    "* High Revenue + High environment count → High Scope_1 and High Scope_2\n",
    "\n",
    "Features to test:\n",
    "\n",
    "* Revenue\n",
    "* Environment count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48f014-da89-4594-975e-e2f3c971028c",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d022629-6899-47a3-abc2-6b49291fa247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asttokens==3.0.1 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: colorama==0.4.6 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: comm==0.2.3 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 3)) (0.2.3)\n",
      "Requirement already satisfied: debugpy==1.8.17 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 4)) (1.8.17)\n",
      "Requirement already satisfied: decorator==5.2.1 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 5)) (5.2.1)\n",
      "Requirement already satisfied: executing==2.2.1 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 6)) (2.2.1)\n",
      "Requirement already satisfied: ipykernel==7.1.0 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 7)) (7.1.0)\n",
      "Requirement already satisfied: ipython==9.7.0 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 8)) (9.7.0)\n",
      "Requirement already satisfied: ipython_pygments_lexers==1.1.1 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: jedi==0.19.2 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 10)) (0.19.2)\n",
      "Requirement already satisfied: joblib==1.5.2 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 11)) (1.5.2)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 12)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.9.1 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 13)) (5.9.1)\n",
      "Requirement already satisfied: matplotlib-inline==0.2.1 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 14)) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 15)) (1.6.0)\n",
      "Requirement already satisfied: numpy==2.3.5 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 16)) (2.3.5)\n",
      "Requirement already satisfied: packaging==25.0 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 17)) (25.0)\n",
      "Requirement already satisfied: pandas==2.3.3 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 18)) (2.3.3)\n",
      "Requirement already satisfied: parso==0.8.5 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 19)) (0.8.5)\n",
      "Requirement already satisfied: platformdirs==4.5.0 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 20)) (4.5.0)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.52 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 21)) (3.0.52)\n",
      "Requirement already satisfied: psutil==7.1.3 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 22)) (7.1.3)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 23)) (0.2.3)\n",
      "Requirement already satisfied: Pygments==2.19.2 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 24)) (2.19.2)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 25)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz==2025.2 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 26)) (2025.2)\n",
      "Requirement already satisfied: pyzmq==27.1.0 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 27)) (27.1.0)\n",
      "Requirement already satisfied: scikit-learn==1.7.2 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 28)) (1.7.2)\n",
      "Requirement already satisfied: scipy==1.16.3 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 29)) (1.16.3)\n",
      "Requirement already satisfied: six==1.17.0 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 30)) (1.17.0)\n",
      "Requirement already satisfied: stack-data==0.6.3 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 31)) (0.6.3)\n",
      "Requirement already satisfied: threadpoolctl==3.6.0 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 32)) (3.6.0)\n",
      "Requirement already satisfied: tornado==6.5.2 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 33)) (6.5.2)\n",
      "Requirement already satisfied: traitlets==5.14.3 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 34)) (5.14.3)\n",
      "Requirement already satisfied: typing_extensions==4.15.0 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 35)) (4.15.0)\n",
      "Requirement already satisfied: tzdata==2025.2 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 36)) (2025.2)\n",
      "Requirement already satisfied: wcwidth==0.2.14 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 37)) (0.2.14)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.12/site-packages (from ipython==9.7.0->-r ../requirements.txt (line 8)) (4.9.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.12/site-packages (from pexpect>4.3->ipython==9.7.0->-r ../requirements.txt (line 8)) (0.7.0)\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries & Requirements\n",
    "!pip install -r ../requirements.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d317f-b509-40eb-99c5-9129931c4049",
   "metadata": {},
   "source": [
    "## Load Data Set & Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "159a24aa-a379-42cf-a960-d17b02ec3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV Data Set\n",
    "environmental_activities_df = pd.read_csv('../data/environmental_activities.csv')\n",
    "revenue_distribution_by_sector_df = pd.read_csv('../data/revenue_distribution_by_sector.csv')\n",
    "sustainable_development_goals_df = pd.read_csv('../data/sustainable_development_goals.csv')\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6732ee-0fdd-4251-a3a8-14806f6b211e",
   "metadata": {},
   "source": [
    "### Train Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "191147b8-fa00-4e39-b576-ed5bef48b9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>region_code</th>\n",
       "      <th>region_name</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country_name</th>\n",
       "      <th>revenue</th>\n",
       "      <th>overall_score</th>\n",
       "      <th>environmental_score</th>\n",
       "      <th>social_score</th>\n",
       "      <th>governance_score</th>\n",
       "      <th>target_scope_1</th>\n",
       "      <th>target_scope_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1782</td>\n",
       "      <td>WEU</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>GB</td>\n",
       "      <td>United Kingdom of Great Britain and Northern I...</td>\n",
       "      <td>3.528060e+08</td>\n",
       "      <td>2.988</td>\n",
       "      <td>3.900</td>\n",
       "      <td>1.750</td>\n",
       "      <td>2.833</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3918</td>\n",
       "      <td>NAM</td>\n",
       "      <td>Northern America</td>\n",
       "      <td>US</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>1.513700e+09</td>\n",
       "      <td>2.770</td>\n",
       "      <td>3.004</td>\n",
       "      <td>2.942</td>\n",
       "      <td>2.143</td>\n",
       "      <td>265.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10299</td>\n",
       "      <td>WEU</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>FR</td>\n",
       "      <td>France</td>\n",
       "      <td>1.560000e+09</td>\n",
       "      <td>2.501</td>\n",
       "      <td>2.979</td>\n",
       "      <td>2.560</td>\n",
       "      <td>1.571</td>\n",
       "      <td>1136.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2324</td>\n",
       "      <td>NAM</td>\n",
       "      <td>Northern America</td>\n",
       "      <td>US</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>1.238511e+10</td>\n",
       "      <td>3.207</td>\n",
       "      <td>3.776</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.429</td>\n",
       "      <td>1468.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1206</td>\n",
       "      <td>WEU</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>ES</td>\n",
       "      <td>Spain</td>\n",
       "      <td>2.980000e+09</td>\n",
       "      <td>1.998</td>\n",
       "      <td>2.138</td>\n",
       "      <td>1.785</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1802.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity_id region_code       region_name country_code  \\\n",
       "0       1782         WEU    Western Europe           GB   \n",
       "1       3918         NAM  Northern America           US   \n",
       "2      10299         WEU    Western Europe           FR   \n",
       "3       2324         NAM  Northern America           US   \n",
       "4       1206         WEU    Western Europe           ES   \n",
       "\n",
       "                                        country_name       revenue  \\\n",
       "0  United Kingdom of Great Britain and Northern I...  3.528060e+08   \n",
       "1                           United States of America  1.513700e+09   \n",
       "2                                             France  1.560000e+09   \n",
       "3                           United States of America  1.238511e+10   \n",
       "4                                              Spain  2.980000e+09   \n",
       "\n",
       "   overall_score  environmental_score  social_score  governance_score  \\\n",
       "0          2.988                3.900         1.750             2.833   \n",
       "1          2.770                3.004         2.942             2.143   \n",
       "2          2.501                2.979         2.560             1.571   \n",
       "3          3.207                3.776         3.000             2.429   \n",
       "4          1.998                2.138         1.785             2.000   \n",
       "\n",
       "   target_scope_1  target_scope_2  \n",
       "0            60.0             0.0  \n",
       "1           265.0             0.0  \n",
       "2          1136.0             0.0  \n",
       "3          1468.0             0.0  \n",
       "4          1802.0             0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd680ecc-5bbd-4c64-bfdd-b4af026b6a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(429, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639a1e62-a318-4df0-8507-d8df82d1fb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['entity_id', 'region_code', 'region_name', 'country_code',\n",
       "       'country_name', 'revenue', 'overall_score', 'environmental_score',\n",
       "       'social_score', 'governance_score', 'target_scope_1', 'target_scope_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69284ded-c056-4dc6-b075-469ef2f2dc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entity_id                int64\n",
       "region_code             object\n",
       "region_name             object\n",
       "country_code            object\n",
       "country_name            object\n",
       "revenue                float64\n",
       "overall_score          float64\n",
       "environmental_score    float64\n",
       "social_score           float64\n",
       "governance_score       float64\n",
       "target_scope_1         float64\n",
       "target_scope_2         float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3073b9-7505-4c5b-b39b-e8c5cd5e4958",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68f4ff2f-a4c9-4b15-ad3f-42b96f72a290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>region_code</th>\n",
       "      <th>region_name</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country_name</th>\n",
       "      <th>revenue</th>\n",
       "      <th>overall_score</th>\n",
       "      <th>environmental_score</th>\n",
       "      <th>social_score</th>\n",
       "      <th>governance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1076</td>\n",
       "      <td>WEU</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>NL</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>1.670000e+09</td>\n",
       "      <td>3.170</td>\n",
       "      <td>3.940</td>\n",
       "      <td>2.692</td>\n",
       "      <td>2.357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2067</td>\n",
       "      <td>WEU</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>GB</td>\n",
       "      <td>United Kingdom of Great Britain and Northern I...</td>\n",
       "      <td>5.880000e+08</td>\n",
       "      <td>2.976</td>\n",
       "      <td>4.000</td>\n",
       "      <td>2.014</td>\n",
       "      <td>2.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>910</td>\n",
       "      <td>WEU</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>DE</td>\n",
       "      <td>Germany</td>\n",
       "      <td>1.218100e+09</td>\n",
       "      <td>2.835</td>\n",
       "      <td>3.258</td>\n",
       "      <td>2.955</td>\n",
       "      <td>1.929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4082</td>\n",
       "      <td>WEU</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>DE</td>\n",
       "      <td>Germany</td>\n",
       "      <td>5.037500e+09</td>\n",
       "      <td>2.861</td>\n",
       "      <td>3.360</td>\n",
       "      <td>2.950</td>\n",
       "      <td>1.857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4102</td>\n",
       "      <td>WEU</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>SE</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>1.415400e+09</td>\n",
       "      <td>2.950</td>\n",
       "      <td>3.550</td>\n",
       "      <td>2.900</td>\n",
       "      <td>1.929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity_id region_code     region_name country_code  \\\n",
       "0       1076         WEU  Western Europe           NL   \n",
       "1       2067         WEU  Western Europe           GB   \n",
       "2        910         WEU  Western Europe           DE   \n",
       "3       4082         WEU  Western Europe           DE   \n",
       "4       4102         WEU  Western Europe           SE   \n",
       "\n",
       "                                        country_name       revenue  \\\n",
       "0                                        Netherlands  1.670000e+09   \n",
       "1  United Kingdom of Great Britain and Northern I...  5.880000e+08   \n",
       "2                                            Germany  1.218100e+09   \n",
       "3                                            Germany  5.037500e+09   \n",
       "4                                             Sweden  1.415400e+09   \n",
       "\n",
       "   overall_score  environmental_score  social_score  governance_score  \n",
       "0          3.170                3.940         2.692             2.357  \n",
       "1          2.976                4.000         2.014             2.286  \n",
       "2          2.835                3.258         2.955             1.929  \n",
       "3          2.861                3.360         2.950             1.857  \n",
       "4          2.950                3.550         2.900             1.929  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea72348a-41ac-4faf-95cd-f2c8258e7395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81ee439e-6c47-491d-8f8e-96fe803b25bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['entity_id', 'region_code', 'region_name', 'country_code',\n",
       "       'country_name', 'revenue', 'overall_score', 'environmental_score',\n",
       "       'social_score', 'governance_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f916fe03-81cd-4a31-bf8c-a73e96e58e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entity_id                int64\n",
       "region_code             object\n",
       "region_name             object\n",
       "country_code            object\n",
       "country_name            object\n",
       "revenue                float64\n",
       "overall_score          float64\n",
       "environmental_score    float64\n",
       "social_score           float64\n",
       "governance_score       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c3748-a844-4872-9a9e-211b0a40d312",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Environmental Activities Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22e17bc7-4987-42f3-a409-6f463c00d0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>activity_type</th>\n",
       "      <th>activity_code</th>\n",
       "      <th>env_score_adjustment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2709</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>M.70.4.P</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>Operation</td>\n",
       "      <td>MTH002</td>\n",
       "      <td>-0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10045</td>\n",
       "      <td>Operation</td>\n",
       "      <td>MTH002</td>\n",
       "      <td>-0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2709</td>\n",
       "      <td>End-use</td>\n",
       "      <td>J.58.16.B</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2677</td>\n",
       "      <td>Operation</td>\n",
       "      <td>J.58.20.P</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity_id   activity_type activity_code  env_score_adjustment\n",
       "0       2709  Transportation      M.70.4.P                  0.05\n",
       "1        107       Operation        MTH002                 -0.10\n",
       "2      10045       Operation        MTH002                 -0.10\n",
       "3       2709         End-use     J.58.16.B                 -0.05\n",
       "4       2677       Operation     J.58.20.P                  0.10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environmental_activities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17f50202-09e4-434b-96fa-99ecfd7bca9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(355, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environmental_activities_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36dc399c-7897-4031-898c-c6cf63b6bdea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['entity_id', 'activity_type', 'activity_code', 'env_score_adjustment'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environmental_activities_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0e5b6ee-496a-4838-885d-b297fa77e808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entity_id                 int64\n",
       "activity_type            object\n",
       "activity_code            object\n",
       "env_score_adjustment    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environmental_activities_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5670ccda-7f0c-4e08-855c-495a0d07f56c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Revenue Distribution by Sector Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4739a574-97d2-460d-8c9d-423ec9d1f303",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>nace_level_1_code</th>\n",
       "      <th>nace_level_1_name</th>\n",
       "      <th>nace_level_2_code</th>\n",
       "      <th>nace_level_2_name</th>\n",
       "      <th>revenue_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1735</td>\n",
       "      <td>A</td>\n",
       "      <td>Agriculture, Forestry And Fishing</td>\n",
       "      <td>1</td>\n",
       "      <td>Crop and animal production, hunting and relate...</td>\n",
       "      <td>0.031105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1195</td>\n",
       "      <td>A</td>\n",
       "      <td>Agriculture, Forestry And Fishing</td>\n",
       "      <td>1</td>\n",
       "      <td>Crop and animal production, hunting and relate...</td>\n",
       "      <td>0.362906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4092</td>\n",
       "      <td>A</td>\n",
       "      <td>Agriculture, Forestry And Fishing</td>\n",
       "      <td>1</td>\n",
       "      <td>Crop and animal production, hunting and relate...</td>\n",
       "      <td>0.222190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3592</td>\n",
       "      <td>A</td>\n",
       "      <td>Agriculture, Forestry And Fishing</td>\n",
       "      <td>1</td>\n",
       "      <td>Crop and animal production, hunting and relate...</td>\n",
       "      <td>0.063879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3169</td>\n",
       "      <td>A</td>\n",
       "      <td>Agriculture, Forestry And Fishing</td>\n",
       "      <td>1</td>\n",
       "      <td>Crop and animal production, hunting and relate...</td>\n",
       "      <td>0.422810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity_id nace_level_1_code                  nace_level_1_name  \\\n",
       "0       1735                 A  Agriculture, Forestry And Fishing   \n",
       "1       1195                 A  Agriculture, Forestry And Fishing   \n",
       "2       4092                 A  Agriculture, Forestry And Fishing   \n",
       "3       3592                 A  Agriculture, Forestry And Fishing   \n",
       "4       3169                 A  Agriculture, Forestry And Fishing   \n",
       "\n",
       "   nace_level_2_code                                  nace_level_2_name  \\\n",
       "0                  1  Crop and animal production, hunting and relate...   \n",
       "1                  1  Crop and animal production, hunting and relate...   \n",
       "2                  1  Crop and animal production, hunting and relate...   \n",
       "3                  1  Crop and animal production, hunting and relate...   \n",
       "4                  1  Crop and animal production, hunting and relate...   \n",
       "\n",
       "   revenue_pct  \n",
       "0     0.031105  \n",
       "1     0.362906  \n",
       "2     0.222190  \n",
       "3     0.063879  \n",
       "4     0.422810  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revenue_distribution_by_sector_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fe780a4-37e3-449b-836f-6e47be17d708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(799, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revenue_distribution_by_sector_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f86d598-b1e6-48ec-9663-64713f6bc81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['entity_id', 'nace_level_1_code', 'nace_level_1_name',\n",
       "       'nace_level_2_code', 'nace_level_2_name', 'revenue_pct'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revenue_distribution_by_sector_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "089964c0-2629-4c1c-a606-b7158ae67b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entity_id              int64\n",
       "nace_level_1_code     object\n",
       "nace_level_1_name     object\n",
       "nace_level_2_code      int64\n",
       "nace_level_2_name     object\n",
       "revenue_pct          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revenue_distribution_by_sector_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b2673-0699-4811-beba-891542405c64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sustainable Development Goals Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ea70934-c87e-4b18-bde6-42e6ab5211e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>sdg_id</th>\n",
       "      <th>sdg_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>Industry, Innovation and Infrastructure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>12</td>\n",
       "      <td>Responsible Consumption and Production</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>Affordable and Clean Energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "      <td>Good Health and Wellbeing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106</td>\n",
       "      <td>9</td>\n",
       "      <td>Industry, Innovation and Infrastructure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity_id  sdg_id                                 sdg_name\n",
       "0         29       9  Industry, Innovation and Infrastructure\n",
       "1         46      12   Responsible Consumption and Production\n",
       "2         46       7              Affordable and Clean Energy\n",
       "3         63       3                Good Health and Wellbeing\n",
       "4        106       9  Industry, Innovation and Infrastructure"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sustainable_development_goals_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "807357f1-0be2-4128-b958-26c0b13a7620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sustainable_development_goals_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e63cbb8-996b-43f7-9a8e-656e1c161ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['entity_id', 'sdg_id', 'sdg_name'], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sustainable_development_goals_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea82fbf4-0152-4990-8241-1ca6c711abf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entity_id     int64\n",
       "sdg_id        int64\n",
       "sdg_name     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sustainable_development_goals_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417bc69a",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c237c0-f89c-423d-b0e8-856065a772bb",
   "metadata": {},
   "source": [
    "### Check Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d49ecbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning - Check null values\n",
    "\n",
    "def analyze_missing_values(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Comprehensive missing value analysis for a dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The dataframe to analyze\n",
    "    dataset_name : str\n",
    "        Name of the dataset for display purposes\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame with missing value statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MISSING VALUE ANALYSIS: {dataset_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Calculate missing values\n",
    "    missing_count = df.isnull().sum()\n",
    "    missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "    dtypes = df.dtypes\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Data_Type': dtypes.values,\n",
    "        'Missing_Count': missing_count.values,\n",
    "        'Missing_Percentage': missing_pct.values,\n",
    "        'Non_Missing_Count': len(df) - missing_count.values\n",
    "    })\n",
    "    \n",
    "    # Sort by missing percentage descending\n",
    "    missing_df = missing_df.sort_values('Missing_Percentage', ascending=False)\n",
    "    \n",
    "    # Filter to show only columns with missing values\n",
    "    missing_cols = missing_df[missing_df['Missing_Count'] > 0]\n",
    "    \n",
    "    if len(missing_cols) == 0:\n",
    "        print(\"✓ No missing values found in this dataset!\\n\")\n",
    "    else:\n",
    "        print(f\"⚠ Found {len(missing_cols)} columns with missing values:\\n\")\n",
    "        print(missing_cols.to_string(index=False))\n",
    "        print(f\"\\nTotal missing values: {missing_count.sum():,}\")\n",
    "        print(f\"Percentage of dataset with any missing value: {(df.isnull().any(axis=1).sum() / len(df) * 100):.2f}%\")\n",
    "    \n",
    "    return missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "711f7d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MISSING VALUE ANALYSIS: TRAIN_DF\n",
      "============================================================\n",
      "\n",
      "✓ No missing values found in this dataset!\n",
      "\n",
      "\n",
      "============================================================\n",
      "MISSING VALUE ANALYSIS: TEST_DF\n",
      "============================================================\n",
      "\n",
      "✓ No missing values found in this dataset!\n",
      "\n",
      "\n",
      "============================================================\n",
      "MISSING VALUE ANALYSIS: ENVIRONMENTAL_DF\n",
      "============================================================\n",
      "\n",
      "✓ No missing values found in this dataset!\n",
      "\n",
      "\n",
      "============================================================\n",
      "MISSING VALUE ANALYSIS: TRAIN_DF\n",
      "============================================================\n",
      "\n",
      "✓ No missing values found in this dataset!\n",
      "\n",
      "\n",
      "============================================================\n",
      "MISSING VALUE ANALYSIS: TRAIN_DF\n",
      "============================================================\n",
      "\n",
      "✓ No missing values found in this dataset!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_missing = analyze_missing_values(train_df, \"TRAIN_DF\")\n",
    "test_missing = analyze_missing_values(test_df, \"TEST_DF\")\n",
    "environmental_activities_missing = analyze_missing_values(environmental_activities_df, \"ENVIRONMENTAL_DF\")\n",
    "revenue_distribution_by_sector_missing = analyze_missing_values(revenue_distribution_by_sector_df, \"TRAIN_DF\")\n",
    "sustainable_development_goals_missing = analyze_missing_values(sustainable_development_goals_df, \"TRAIN_DF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc7164b-d216-4514-96f6-e212ba467a5e",
   "metadata": {},
   "source": [
    "### Check Dulplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86738514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning - Check dulplicate values\n",
    "\n",
    "def analyze_duplicates(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Comprehensive duplicate row analysis for a dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The dataframe to analyze\n",
    "    dataset_name : str\n",
    "        Name of the dataset for display purposes\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame with duplicate statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DUPLICATE ROW ANALYSIS: {dataset_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Count full-row duplicates\n",
    "    duplicate_mask = df.duplicated(keep=False)\n",
    "    total_duplicates = duplicate_mask.sum()\n",
    "    unique_duplicate_rows = df[duplicate_mask].drop_duplicates()\n",
    "    \n",
    "    if total_duplicates == 0:\n",
    "        print(\"✓ No duplicate rows found in this dataset!\\n\")\n",
    "        return pd.DataFrame()  # empty result\n",
    "    \n",
    "    # Count how many times each duplicated row appears\n",
    "    duplicate_summary = (\n",
    "        df[df.duplicated(keep=False)]\n",
    "        .value_counts()\n",
    "        .reset_index(name='Count')\n",
    "        .sort_values('Count', ascending=False)\n",
    "    )\n",
    "    \n",
    "    print(f\"⚠ Found {len(unique_duplicate_rows)} unique duplicated rows.\")\n",
    "    print(f\"Total duplicated entries (including repeats): {total_duplicates}\\n\")\n",
    "    \n",
    "    print(\"Most common duplicate patterns:\")\n",
    "    display(duplicate_summary.head(10))\n",
    "    \n",
    "    return duplicate_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc61b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the duplicate-value method on each dataset\n",
    "train_duplicates = analyze_duplicates(train_df, \"TRAIN_DF\")\n",
    "test_duplicates = analyze_duplicates(test_df, \"TEST_DF\")\n",
    "environmental_activities_duplicates = analyze_duplicates(environmental_activities_df, \"ENVIRONMENTAL_DF\")\n",
    "revenue_distribution_by_sector_duplicates = analyze_duplicates(revenue_distribution_by_sector_df, \"REVENUE_DF\")\n",
    "sustainable_development_goals_duplicates = analyze_duplicates(sustainable_development_goals_df, \"SDG_DF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345e47a-b56b-4318-9d0d-e2720b26cfba",
   "metadata": {},
   "source": [
    "### Check Revenue Distribution Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21191cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING REVENUE DISTRIBUTION COVERAGE\n",
    "\n",
    "# Get unique entity_ids from each dataset\n",
    "train_entities = set(train_df['entity_id'].unique())\n",
    "test_entities = set(test_df['entity_id'].unique())\n",
    "revenue_entities = set(revenue_distribution_by_sector_df['entity_id'].unique())\n",
    "\n",
    "print(f\"Unique entities in train: {len(train_entities):,}\")\n",
    "print(f\"Unique entities in test: {len(test_entities):,}\")\n",
    "print(f\"Unique entities in revenue_distribution: {len(revenue_entities):,}\")\n",
    "\n",
    "# Check coverage\n",
    "train_covered = train_entities & revenue_entities\n",
    "test_covered = test_entities & revenue_entities\n",
    "\n",
    "train_missing = train_entities - revenue_entities\n",
    "test_missing = test_entities - revenue_entities\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COVERAGE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTRAIN dataset:\")\n",
    "print(f\"  Entities WITH revenue data: {len(train_covered):,} ({len(train_covered)/len(train_entities)*100:.2f}%)\")\n",
    "print(f\"  Entities WITHOUT revenue data: {len(train_missing):,} ({len(train_missing)/len(train_entities)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTEST dataset:\")\n",
    "print(f\"  Entities WITH revenue data: {len(test_covered):,} ({len(test_covered)/len(test_entities)*100:.2f}%)\")\n",
    "print(f\"  Entities WITHOUT revenue data: {len(test_missing):,} ({len(test_missing)/len(test_entities)*100:.2f}%)\")\n",
    "\n",
    "# Check if there are entities in revenue table that aren't in train/test\n",
    "orphan_entities = revenue_entities - train_entities - test_entities\n",
    "print(f\"\\nEntities in revenue table but NOT in train/test: {len(orphan_entities):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPLICATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(train_missing) > 0 or len(test_missing) > 0:\n",
    "    print(\"\\n⚠ IMPORTANT: Some entities don't have revenue distribution data!\")\n",
    "else:\n",
    "    print(\"\\n✓ Perfect coverage! All entities have revenue distribution data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2147bec-414f-41f4-a810-beb2eb464299",
   "metadata": {},
   "source": [
    "### Checking Sustainable Distribution Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40248bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING SUSTAINABLE DISTRIBUTION COVERAGE\n",
    "\n",
    "# Get unique entity_ids from each dataset\n",
    "sustain_entities = set(sustainable_development_goals_df['entity_id'].unique())\n",
    "\n",
    "print(f\"Unique entities in train: {len(train_entities):,}\")\n",
    "print(f\"Unique entities in test: {len(test_entities):,}\")\n",
    "print(f\"Unique entities in sustain_distribution: {len(sustain_entities):,}\")\n",
    "\n",
    "# Check coverage\n",
    "train_covered = train_entities & sustain_entities\n",
    "test_covered = test_entities & sustain_entities\n",
    "\n",
    "train_missing = train_entities - sustain_entities\n",
    "test_missing = test_entities - sustain_entities\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COVERAGE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTRAIN dataset:\")\n",
    "print(f\"  Entities WITH sustainable_development_goals data: {len(train_covered):,} ({len(train_covered)/len(train_entities)*100:.2f}%)\")\n",
    "print(f\"  Entities WITHOUT sustainable_development_goals data: {len(train_missing):,} ({len(train_missing)/len(train_entities)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTEST dataset:\")\n",
    "print(f\"  Entities WITH sustainable_development_goals data: {len(test_covered):,} ({len(test_covered)/len(test_entities)*100:.2f}%)\")\n",
    "print(f\"  Entities WITHOUT sustainable_development_goals data: {len(test_missing):,} ({len(test_missing)/len(test_entities)*100:.2f}%)\")\n",
    "\n",
    "# Check if there are entities in sustain table that aren't in train/test\n",
    "orphan_entities = sustain_entities - train_entities - test_entities\n",
    "print(f\"\\nEntities in sustain table but NOT in train/test: {len(orphan_entities):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPLICATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(train_missing) > 0 or len(test_missing) > 0:\n",
    "    print(\"\\n⚠ IMPORTANT: Some entities don't have sustain distribution data!\")\n",
    "else:\n",
    "    print(\"\\n✓ Perfect coverage! All entities have sustain distribution data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8cc300",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ISOLATED DATASET OF ENTITIES IN TRAIN DATASET with NO sustainable_development_goals data \")\n",
    "print(\"=\"*60)\n",
    "Train_To_sustain_distribution_unmatched = (\n",
    "    train_df\n",
    "    .merge(sustainable_development_goals_df, on=\"entity_id\", how=\"left\", indicator=True)\n",
    "    .query(\"_merge == 'left_only'\")\n",
    "    .drop(columns=[\"_merge\"])\n",
    ")\n",
    "\n",
    "print(Train_To_sustain_distribution_unmatched.shape)\n",
    "Train_To_sustain_distribution_unmatched.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f940c63-deb5-4412-bb2c-d56515eaf1da",
   "metadata": {},
   "source": [
    "### Checking Environment Distribution Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752b0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING ENVIRONMENT DISTRIBUTION COVERAGE\n",
    "\n",
    "# Get unique entity_ids from each dataset\n",
    "environment_entities = set(environmental_activities_df['entity_id'].unique())\n",
    "\n",
    "print(f\"Unique entities in train: {len(train_entities):,}\")\n",
    "print(f\"Unique entities in test: {len(test_entities):,}\")\n",
    "print(f\"Unique entities in environment_distribution: {len(environment_entities):,}\")\n",
    "\n",
    "# Check coverage\n",
    "train_covered = train_entities & environment_entities\n",
    "test_covered = test_entities & environment_entities\n",
    "\n",
    "train_missing = train_entities - environment_entities\n",
    "test_missing = test_entities - environment_entities\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COVERAGE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTRAIN dataset:\")\n",
    "print(f\"  Entities WITH environment data: {len(train_covered):,} ({len(train_covered)/len(train_entities)*100:.2f}%)\")\n",
    "print(f\"  Entities WITHOUT environment data: {len(train_missing):,} ({len(train_missing)/len(train_entities)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTEST dataset:\")\n",
    "print(f\"  Entities WITH environment data: {len(test_covered):,} ({len(test_covered)/len(test_entities)*100:.2f}%)\")\n",
    "print(f\"  Entities WITHOUT environment data: {len(test_missing):,} ({len(test_missing)/len(test_entities)*100:.2f}%)\")\n",
    "\n",
    "# Check if there are entities in environment table that aren't in train/test\n",
    "orphan_entities = environment_entities - train_entities - test_entities\n",
    "print(f\"\\nEntities in environment table but NOT in train/test: {len(orphan_entities):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPLICATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(train_missing) > 0 or len(test_missing) > 0:\n",
    "    print(\"\\n⚠ IMPORTANT: Some entities don't have environment distribution data!\")\n",
    "else:\n",
    "    print(\"\\n✓ Perfect coverage! All entities have environment distribution data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f770f78-328e-4774-8fa2-b08e9c668b8c",
   "metadata": {},
   "source": [
    "## Revenue Distribution EDA & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227e727-1aab-4a72-a56a-dfed761a62d6",
   "metadata": {},
   "source": [
    "### Deep EDA on Revenue Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec10e60-35c0-400f-b49a-4499f5ad89af",
   "metadata": {},
   "source": [
    "#### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070e346-85ba-4997-9f9b-730045828a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many sectors does each company operate in?\n",
    "sectors_per_company = revenue_distribution_by_sector_df.groupby('entity_id').agg({\n",
    "    'nace_level_1_code': 'nunique',\n",
    "    'nace_level_2_code': 'nunique',\n",
    "    'revenue_pct': 'sum'  # Should be ~100 for each company\n",
    "})\n",
    "sectors_per_company.columns = ['num_L1_sectors', 'num_L2_sectors', 'total_revenue_pct']\n",
    "\n",
    "print(\"Sectors per company:\")\n",
    "print(sectors_per_company.describe())\n",
    "print(f\"\\nCompanies with revenue_pct not summing to ~100: {(sectors_per_company['total_revenue_pct'] < 99).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6cd6a2-0f94-4828-b98f-19a293547c37",
   "metadata": {},
   "source": [
    "#### Sector Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97daf264-67f9-4069-ab61-8cd5403d9630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common NACE L1 sectors\n",
    "print(\"\\nTop 10 NACE L1 sectors by frequency:\")\n",
    "l1_freq = revenue_distribution_by_sector_df.groupby('nace_level_1_code')['entity_id'].nunique().sort_values(ascending=False)\n",
    "print(l1_freq.head(10))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "l1_freq.head(15).plot(kind='barh', color='steelblue')\n",
    "plt.xlabel('Number of Companies')\n",
    "plt.title('Top 15 NACE Level 1 Sectors by Company Count', fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e72e0e7-4dd0-45ef-a11a-ef6d937afe70",
   "metadata": {},
   "source": [
    "#### Revenue Concentration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb9c1ab-7212-4e1b-8211-4a3b39be7b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dominant sector percentage for each company\n",
    "dominant_pct = revenue_distribution_by_sector_df.groupby('entity_id')['revenue_pct'].max()\n",
    "print(f\"\\nDominant sector revenue percentage:\")\n",
    "print(dominant_pct.describe())\n",
    "\n",
    "# Visualize concentration\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(dominant_pct, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('% Revenue from Dominant Sector')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Revenue Concentration: Dominant Sector %', fontweight='bold')\n",
    "axes[0].axvline(50, color='red', linestyle='--', label='50%')\n",
    "axes[0].axvline(dominant_pct.mean(), color='green', linestyle='--', label=f'Mean: {dominant_pct.mean():.1f}%')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(sectors_per_company['num_L2_sectors'], bins=range(1, 20), edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_xlabel('Number of NACE L2 Sectors')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Company Diversification: Number of Sectors', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCompanies with >80% revenue in one sector: {(dominant_pct > 80).sum()} ({(dominant_pct > 80).sum()/len(dominant_pct)*100:.1f}%)\")\n",
    "print(f\"Companies with <50% revenue in dominant sector: {(dominant_pct < 50).sum()} ({(dominant_pct < 50).sum()/len(dominant_pct)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c653a82-f163-44e4-a270-e9962a0d4204",
   "metadata": {},
   "source": [
    "### Hypothesis-Driven Feature Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d40b63-865a-42e8-b2c6-7866c0767a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, merge revenue features with train data for full analysis\n",
    "# Create basic sector features\n",
    "sector_features = []\n",
    "for entity_id, group in revenue_distribution_by_sector_df.groupby('entity_id'):\n",
    "    dominant = group.loc[group['revenue_pct'].idxmax()]\n",
    "    HIGH_EMISSION = ['A', 'B', 'C', 'D', 'E']\n",
    "    high_mask = group['nace_level_1_code'].isin(HIGH_EMISSION)\n",
    "    \n",
    "    sector_features.append({\n",
    "        'entity_id': entity_id,\n",
    "        'dominant_nace_l1': dominant['nace_level_1_code'],\n",
    "        'dominant_nace_l1_name': dominant['nace_level_1_name'],\n",
    "        'dominant_pct': dominant['revenue_pct'],\n",
    "        'high_emission_pct': group[high_mask]['revenue_pct'].sum(),\n",
    "        'num_l1_sectors': group['nace_level_1_code'].nunique(),\n",
    "        'num_l2_sectors': group['nace_level_2_code'].nunique(),\n",
    "        'is_manufacturing': int(dominant['nace_level_1_code'] == 'C'),\n",
    "        'is_mining': int(dominant['nace_level_1_code'] == 'B'),\n",
    "        'is_energy': int(dominant['nace_level_1_code'] == 'D'),\n",
    "        'is_utilities': int(dominant['nace_level_1_code'] == 'E'),\n",
    "        'is_transport': int(dominant['nace_level_1_code'] == 'H'),\n",
    "        'is_services': int(dominant['nace_level_1_code'] in ['G', 'I', 'J', 'K', 'M', 'N']),\n",
    "        'manufacturing_pct': group[group['nace_level_1_code'] == 'C']['revenue_pct'].sum(),\n",
    "        'mining_pct': group[group['nace_level_1_code'] == 'B']['revenue_pct'].sum(),\n",
    "    })\n",
    "\n",
    "sector_df = pd.DataFrame(sector_features)\n",
    "analysis_df = train_df.merge(sector_df, on='entity_id', how='left')\n",
    "\n",
    "print(f\"✓ Merged {len(analysis_df)} entities for analysis\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a47df2a-bdb8-4d55-bfec-3ae6d2e4035b",
   "metadata": {},
   "source": [
    "#### Hypothesis 1: Sector Mix Drives Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15229f1d-b75c-4817-a582-776d02d53c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Prediction 1A: Manufacturing, mining, energy → HIGH Scope 1\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Compare high-emission vs low-emission sectors\n",
    "high_emission_industries = analysis_df[analysis_df['high_emission_pct'] > 0.5]\n",
    "low_emission_industries = analysis_df[analysis_df['high_emission_pct'] <= 0.5]\n",
    "\n",
    "print(f\"\\n📊 Companies with >50% high-emission sectors (n={len(high_emission_industries)}):\")\n",
    "print(f\"   Avg Scope 1: {high_emission_industries['target_scope_1'].mean():,.0f}\")\n",
    "print(f\"   Median Scope 1: {high_emission_industries['target_scope_1'].median():,.0f}\")\n",
    "\n",
    "print(f\"\\n📊 Companies with ≤50% high-emission sectors (n={len(low_emission_industries)}):\")\n",
    "print(f\"   Avg Scope 1: {low_emission_industries['target_scope_1'].mean():,.0f}\")\n",
    "print(f\"   Median Scope 1: {low_emission_industries['target_scope_1'].median():,.0f}\")\n",
    "\n",
    "# Calculate effect size\n",
    "effect_size = (high_emission_industries['target_scope_1'].mean() - \n",
    "               low_emission_industries['target_scope_1'].mean())\n",
    "print(f\"\\n💡 Effect size: {effect_size:,.0f} ({effect_size/low_emission_industries['target_scope_1'].mean()*100:.1f}% increase)\")\n",
    "\n",
    "# Statistical test\n",
    "from scipy import stats\n",
    "t_stat, p_val = stats.ttest_ind(high_emission_industries['target_scope_1'], \n",
    "                                  low_emission_industries['target_scope_1'])\n",
    "print(f\"   Statistical significance: p={p_val:.6f}\")\n",
    "if p_val < 0.001:\n",
    "    print(\"   ✅ HYPOTHESIS STRONGLY SUPPORTED (p < 0.001)\")\n",
    "elif p_val < 0.05:\n",
    "    print(\"   ✅ HYPOTHESIS SUPPORTED (p < 0.05)\")\n",
    "else:\n",
    "    print(\"   ❌ HYPOTHESIS NOT SUPPORTED (p >= 0.05)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Prediction 1B: Utilities, heavy electricity users → HIGH Scope 2\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\n📊 Companies with >50% high-emission sectors:\")\n",
    "print(f\"   Avg Scope 2: {high_emission_industries['target_scope_2'].mean():,.0f}\")\n",
    "print(f\"   Median Scope 2: {high_emission_industries['target_scope_2'].median():,.0f}\")\n",
    "\n",
    "print(f\"\\n📊 Companies with ≤50% high-emission sectors:\")\n",
    "print(f\"   Avg Scope 2: {low_emission_industries['target_scope_2'].mean():,.0f}\")\n",
    "print(f\"   Median Scope 2: {low_emission_industries['target_scope_2'].median():,.0f}\")\n",
    "\n",
    "# Specific sector analysis\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"PREDICTION 1C: Services, software, consulting → LOW both\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "sector_emissions = analysis_df.groupby('dominant_nace_l1_name').agg({\n",
    "    'target_scope_1': ['mean', 'median', 'count'],\n",
    "    'target_scope_2': ['mean', 'median']\n",
    "}).round(0)\n",
    "\n",
    "print(\"\\n📊 Emissions by Sector (sorted by Scope 1):\")\n",
    "print(sector_emissions.sort_values(('target_scope_1', 'mean'), ascending=False))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Scope 1 by sector\n",
    "sector_scope1 = analysis_df.groupby('dominant_nace_l1_name')['target_scope_1'].mean().sort_values(ascending=True)\n",
    "top_10_scope1 = sector_scope1.tail(10)\n",
    "top_10_scope1.plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "axes[0].set_xlabel('Average Scope 1 Emissions', fontsize=12)\n",
    "axes[0].set_title('Top 10 Sectors by Scope 1 Emissions', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(analysis_df['target_scope_1'].mean(), color='red', linestyle='--', \n",
    "                label=f'Overall Avg: {analysis_df[\"target_scope_1\"].mean():,.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Scope 2 by sector\n",
    "sector_scope2 = analysis_df.groupby('dominant_nace_l1_name')['target_scope_2'].mean().sort_values(ascending=True)\n",
    "top_10_scope2 = sector_scope2.tail(10)\n",
    "top_10_scope2.plot(kind='barh', ax=axes[1], color='orange')\n",
    "axes[1].set_xlabel('Average Scope 2 Emissions', fontsize=12)\n",
    "axes[1].set_title('Top 10 Sectors by Scope 2 Emissions', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(analysis_df['target_scope_2'].mean(), color='red', linestyle='--',\n",
    "                label=f'Overall Avg: {analysis_df[\"target_scope_2\"].mean():,.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"FEATURE VALIDATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Correlation of sector features with targets\n",
    "sector_feature_cols = ['high_emission_pct', 'dominant_pct', 'num_l1_sectors', 'num_l2_sectors',\n",
    "                       'is_manufacturing', 'is_mining', 'manufacturing_pct', 'mining_pct']\n",
    "\n",
    "print(\"\\n📊 Feature Correlation with Scope 1:\")\n",
    "scope1_corr = analysis_df[sector_feature_cols + ['target_scope_1']].corr()['target_scope_1'].drop('target_scope_1').sort_values(ascending=False)\n",
    "for feature, corr in scope1_corr.items():\n",
    "    print(f\"   {feature:25s}: {corr:6.3f}\")\n",
    "\n",
    "print(\"\\n📊 Feature Correlation with Scope 2:\")\n",
    "scope2_corr = analysis_df[sector_feature_cols + ['target_scope_2']].corr()['target_scope_2'].drop('target_scope_2').sort_values(ascending=False)\n",
    "for feature, corr in scope2_corr.items():\n",
    "    print(f\"   {feature:25s}: {corr:6.3f}\")\n",
    "\n",
    "# Visualize correlations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "scope1_corr.sort_values().plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "axes[0].set_xlabel('Correlation Coefficient')\n",
    "axes[0].set_title('Sector Features vs Scope 1', fontweight='bold')\n",
    "axes[0].axvline(0, color='black', linewidth=0.5)\n",
    "\n",
    "scope2_corr.sort_values().plot(kind='barh', ax=axes[1], color='orange')\n",
    "axes[1].set_xlabel('Correlation Coefficient')\n",
    "axes[1].set_title('Sector Features vs Scope 2', fontweight='bold')\n",
    "axes[1].axvline(0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ HYPOTHESIS 1 CONCLUSION:\")\n",
    "if scope1_corr['high_emission_pct'] > 0.2 or p_val < 0.05:\n",
    "    print(\"   STRONGLY VALIDATED - Sector mix is a key predictor of emissions\")\n",
    "    print(\"   → HIGH PRIORITY: Include all sector features in final model\")\n",
    "else:\n",
    "    print(\"   PARTIALLY VALIDATED - Some sector effects present but weak\")\n",
    "    print(\"   → MEDIUM PRIORITY: Include dominant sector only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b9835-08e8-4e88-92d0-beaad346a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"HYPOTHESIS 1 FINAL SUMMARY FOR TEAM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = \"\"\"\n",
    "HYPOTHESIS 1: SECTOR MIX DRIVES EMISSIONS ✅ STRONGLY VALIDATED\n",
    "\n",
    "KEY FINDINGS:\n",
    "1. Companies with >50% high-emission sectors have:\n",
    "   - 186% higher Scope 1 emissions (p < 0.001)\n",
    "   - 156% higher Scope 2 emissions\n",
    "   \n",
    "2. Sector features have strong correlations:\n",
    "   - high_emission_pct: 0.275 (Scope 1), 0.154 (Scope 2)\n",
    "   - manufacturing_pct: 0.221 (Scope 1), 0.164 (Scope 2)\n",
    "   \n",
    "3. Clear sector hierarchy:\n",
    "   Top Scope 1: Mining (209K) > Energy (179K) > Agriculture (136K)\n",
    "   Bottom Scope 1: Financial (1K) < Admin Services (3K)\n",
    "   \n",
    "RECOMMENDED FEATURES FOR FINAL MODEL:\n",
    "  Priority 1 (MUST include):\n",
    "    - high_emission_pct\n",
    "    - manufacturing_pct, is_manufacturing\n",
    "    - dominant_nace_l1 (categorical - target encode)\n",
    "    - is_mining, mining_pct\n",
    "    \n",
    "  Priority 2 (Optional):\n",
    "    - num_l2_sectors\n",
    "    \n",
    "HIGH-EMISSION SECTORS DEFINED AS:\n",
    "  ['A', 'B', 'C', 'D', 'E'] = Agriculture, Mining, Manufacturing, Energy, Utilities\n",
    "\n",
    "NEXT: Validate Hypothesis 2 (Revenue scaling) and create final feature set.\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e6738f-0b3a-4bb1-8bca-43f21f69bfd5",
   "metadata": {},
   "source": [
    "#### Hypothesis 2: Revenue as Scaling Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c66006-23ac-4178-aebf-551231274c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"PREDICTION 2A: 2x revenue in manufacturing ≈ 2x emissions\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Focus on manufacturing companies\n",
    "manufacturing = analysis_df[analysis_df['is_manufacturing'] == 1].copy()\n",
    "services = analysis_df[analysis_df['is_services'] == 1].copy()\n",
    "\n",
    "print(f\"\\n📊 MANUFACTURING companies (n={len(manufacturing)}):\")\n",
    "print(f\"   Revenue vs Scope 1 correlation: {manufacturing['revenue'].corr(manufacturing['target_scope_1']):.3f}\")\n",
    "print(f\"   Revenue vs Scope 2 correlation: {manufacturing['revenue'].corr(manufacturing['target_scope_2']):.3f}\")\n",
    "\n",
    "print(f\"\\n📊 SERVICE companies (n={len(services)}):\")\n",
    "print(f\"   Revenue vs Scope 1 correlation: {services['revenue'].corr(services['target_scope_1']):.3f}\")\n",
    "print(f\"   Revenue vs Scope 2 correlation: {services['revenue'].corr(services['target_scope_2']):.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"PREDICTION 2B: Test linear vs log transformation\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Calculate emission intensity (emissions per revenue dollar)\n",
    "analysis_df['scope1_intensity'] = analysis_df['target_scope_1'] / (analysis_df['revenue'] + 1)\n",
    "analysis_df['scope2_intensity'] = analysis_df['target_scope_2'] / (analysis_df['revenue'] + 1)\n",
    "\n",
    "print(f\"\\n📊 Emission Intensity by Sector:\")\n",
    "intensity_by_sector = analysis_df.groupby('dominant_nace_l1_name').agg({\n",
    "    'scope1_intensity': 'mean',\n",
    "    'scope2_intensity': 'mean'\n",
    "}).sort_values('scope1_intensity', ascending=False).head(10)\n",
    "print(intensity_by_sector)\n",
    "\n",
    "# Visualize revenue vs emissions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Row 1: Linear scale\n",
    "axes[0, 0].scatter(analysis_df['revenue'], analysis_df['target_scope_1'], alpha=0.5, s=20)\n",
    "axes[0, 0].set_xlabel('Revenue')\n",
    "axes[0, 0].set_ylabel('Scope 1 Emissions')\n",
    "axes[0, 0].set_title('Revenue vs Scope 1 (Linear)', fontweight='bold')\n",
    "\n",
    "axes[0, 1].scatter(analysis_df['revenue'], analysis_df['target_scope_2'], alpha=0.5, s=20, color='orange')\n",
    "axes[0, 1].set_xlabel('Revenue')\n",
    "axes[0, 1].set_ylabel('Scope 2 Emissions')\n",
    "axes[0, 1].set_title('Revenue vs Scope 2 (Linear)', fontweight='bold')\n",
    "\n",
    "# Emission intensity\n",
    "axes[0, 2].scatter(analysis_df['revenue'], analysis_df['scope1_intensity'], alpha=0.5, s=20, color='green')\n",
    "axes[0, 2].set_xlabel('Revenue')\n",
    "axes[0, 2].set_ylabel('Scope 1 Intensity')\n",
    "axes[0, 2].set_title('Revenue vs Emission Intensity', fontweight='bold')\n",
    "\n",
    "# Row 2: Log scale\n",
    "axes[1, 0].scatter(np.log1p(analysis_df['revenue']), np.log1p(analysis_df['target_scope_1']), alpha=0.5, s=20)\n",
    "axes[1, 0].set_xlabel('Log(Revenue + 1)')\n",
    "axes[1, 0].set_ylabel('Log(Scope 1 + 1)')\n",
    "axes[1, 0].set_title('Log-Transformed Revenue vs Scope 1', fontweight='bold')\n",
    "\n",
    "axes[1, 1].scatter(np.log1p(analysis_df['revenue']), np.log1p(analysis_df['target_scope_2']), alpha=0.5, s=20, color='orange')\n",
    "axes[1, 1].set_xlabel('Log(Revenue + 1)')\n",
    "axes[1, 1].set_ylabel('Log(Scope 2 + 1)')\n",
    "axes[1, 1].set_title('Log-Transformed Revenue vs Scope 2', fontweight='bold')\n",
    "\n",
    "# By sector color-coded\n",
    "for sector in analysis_df['dominant_nace_l1'].unique()[:5]:  # Top 5 sectors\n",
    "    sector_data = analysis_df[analysis_df['dominant_nace_l1'] == sector]\n",
    "    axes[1, 2].scatter(np.log1p(sector_data['revenue']), \n",
    "                      np.log1p(sector_data['target_scope_1']), \n",
    "                      alpha=0.6, s=20, label=sector)\n",
    "axes[1, 2].set_xlabel('Log(Revenue + 1)')\n",
    "axes[1, 2].set_ylabel('Log(Scope 1 + 1)')\n",
    "axes[1, 2].set_title('Revenue vs Scope 1 by Sector', fontweight='bold')\n",
    "axes[1, 2].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare correlations\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"LINEAR vs LOG TRANSFORMATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "corr_linear_1 = analysis_df['revenue'].corr(analysis_df['target_scope_1'])\n",
    "corr_log_1 = np.log1p(analysis_df['revenue']).corr(np.log1p(analysis_df['target_scope_1']))\n",
    "corr_linear_2 = analysis_df['revenue'].corr(analysis_df['target_scope_2'])\n",
    "corr_log_2 = np.log1p(analysis_df['revenue']).corr(np.log1p(analysis_df['target_scope_2']))\n",
    "\n",
    "print(f\"\\n📊 Scope 1:\")\n",
    "print(f\"   Linear correlation: {corr_linear_1:.3f}\")\n",
    "print(f\"   Log correlation:    {corr_log_1:.3f}\")\n",
    "print(f\"   → {'Log transformation BETTER' if corr_log_1 > corr_linear_1 else 'Linear BETTER'}\")\n",
    "\n",
    "print(f\"\\n📊 Scope 2:\")\n",
    "print(f\"   Linear correlation: {corr_linear_2:.3f}\")\n",
    "print(f\"   Log correlation:    {corr_log_2:.3f}\")\n",
    "print(f\"   → {'Log transformation BETTER' if corr_log_2 > corr_linear_2 else 'Linear BETTER'}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"INTERACTION: Revenue × Sector\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Create interaction feature\n",
    "analysis_df['revenue_x_high_emission'] = analysis_df['revenue'] * analysis_df['high_emission_pct'] / 100\n",
    "\n",
    "corr_interaction_1 = analysis_df['revenue_x_high_emission'].corr(analysis_df['target_scope_1'])\n",
    "corr_interaction_2 = analysis_df['revenue_x_high_emission'].corr(analysis_df['target_scope_2'])\n",
    "\n",
    "print(f\"\\n📊 Revenue × High-Emission % correlation:\")\n",
    "print(f\"   Scope 1: {corr_interaction_1:.3f} (vs raw revenue: {corr_linear_1:.3f})\")\n",
    "print(f\"   Scope 2: {corr_interaction_2:.3f} (vs raw revenue: {corr_linear_2:.3f})\")\n",
    "\n",
    "if corr_interaction_1 > corr_linear_1:\n",
    "    print(\"   ✅ Interaction feature improves Scope 1 prediction\")\n",
    "else:\n",
    "    print(\"   ❌ Interaction doesn't help Scope 1\")\n",
    "\n",
    "if corr_interaction_2 > corr_linear_2:\n",
    "    print(\"   ✅ Interaction feature improves Scope 2 prediction\")\n",
    "else:\n",
    "    print(\"   ❌ Interaction doesn't help Scope 2\")\n",
    "\n",
    "print(\"\\n✅ HYPOTHESIS 2 CONCLUSION:\")\n",
    "if corr_log_1 > 0.3 or corr_interaction_1 > corr_linear_1:\n",
    "    print(\"   VALIDATED - Revenue matters, especially with transformations\")\n",
    "    print(f\"   → RECOMMENDED: Use log(revenue) and revenue × sector interactions\")\n",
    "else:\n",
    "    print(\"   WEAK - Revenue alone is poor predictor\")\n",
    "    print(\"   → Revenue less important than sector composition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a81420-99dc-4d73-aed3-62b70353d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key metrics from your results\n",
    "linear_scope1 = 0.189\n",
    "log_scope1 = 0.403\n",
    "linear_scope2 = 0.187\n",
    "log_scope2 = 0.310\n",
    "interaction_scope1 = 0.311\n",
    "interaction_scope2 = 0.229\n",
    "\n",
    "improvement_scope1 = (log_scope1 - linear_scope1) / linear_scope1 * 100\n",
    "improvement_scope2 = (log_scope2 - linear_scope2) / log_scope2 * 100\n",
    "\n",
    "summary = f\"\"\"\n",
    "\n",
    "HYPOTHESIS 2: REVENUE AS SCALING FACTOR ✅ VALIDATED\n",
    "\n",
    "KEY FINDINGS:\n",
    "1. Log transformation dramatically improves correlations:\n",
    "   - Scope 1: Linear r={linear_scope1:.3f} → Log r={log_scope1:.3f} ({improvement_scope1:.0f}% improvement)\n",
    "   - Scope 2: Linear r={linear_scope2:.3f} → Log r={log_scope2:.3f} ({improvement_scope2:.0f}% improvement)\n",
    "   → Revenue-emission relationship is LOGARITHMIC, not linear\n",
    "   \n",
    "2. Interaction features outperform raw revenue:\n",
    "   - Revenue × High-Emission %: r={interaction_scope1:.3f} (Scope 1), r={interaction_scope2:.3f} (Scope 2)\n",
    "   - 64% better than raw revenue for Scope 1\n",
    "   - 22% better than raw revenue for Scope 2\n",
    "   → Sector context is CRITICAL for revenue effects\n",
    "   \n",
    "3. Emission intensity varies 14x across sectors:\n",
    "   - Mining: 0.000188 (Scope 1 per revenue dollar) - HIGHEST\n",
    "   - Manufacturing: 0.000045 \n",
    "   - Professional Services: 0.000013 - LOWEST\n",
    "   → Same revenue produces vastly different emissions by sector\n",
    "   \n",
    "4. Within-sector scaling is consistent:\n",
    "   - Manufacturing: Revenue correlation r=0.262\n",
    "   - Services: Revenue correlation r=0.269\n",
    "   → Revenue matters WITHIN sectors, not across them\n",
    "   \n",
    "INTERPRETATION:\n",
    "- Doubling revenue does NOT double emissions (logarithmic relationship)\n",
    "- Economies of scale reduce emission intensity at larger sizes\n",
    "- Sector sets the baseline, revenue scales within that baseline\n",
    "- A $1B manufacturing company ≠ $1B software company in emissions\n",
    "\n",
    "RECOMMENDED FEATURES FOR FINAL MODEL:\n",
    "  Priority 1 (MUST include):\n",
    "    - log_revenue (r=0.403 with Scope 1, strongest revenue feature)\n",
    "    - revenue_x_high_emission_pct (r=0.311, best interaction)\n",
    "    - log_revenue_x_high_emission_pct (test this too)\n",
    "    \n",
    "  Priority 2 (SHOULD include):\n",
    "    - revenue_x_manufacturing_pct\n",
    "    - log_revenue_x_manufacturing_pct\n",
    "    - sector_avg_scope1 (sector baseline intensity)\n",
    "    - sector_avg_scope2 (sector baseline intensity)\n",
    "    \n",
    "  Priority 3 (OPTIONAL):\n",
    "    - revenue_squared (for polynomial effects)\n",
    "    - log_revenue_x_num_sectors (diversification effect)\n",
    "\n",
    "DO NOT INCLUDE:\n",
    "  - Raw revenue alone (r=0.189, too weak)\n",
    "  - Linear revenue features without transformation\n",
    "\n",
    "TRANSFORMATION REQUIREMENTS:\n",
    "  - ALWAYS use log(revenue + 1) instead of raw revenue\n",
    "  - Consider log-transforming TARGETS too: log(Scope 1 + 1), log(Scope 2 + 1)\n",
    "  - This will help with skewed emission distributions\n",
    "\n",
    "MODEL IMPLICATIONS:\n",
    "  1. Tree-based models (XGBoost, Random Forest) will work best\n",
    "     - Naturally handle logarithmic relationships\n",
    "     - Automatically discover interactions\n",
    "     \n",
    "  2. If using linear models:\n",
    "     - MUST use log-transformed features\n",
    "     - MUST include interaction terms manually\n",
    "     - Consider polynomial features\n",
    "     \n",
    "  3. Expected performance contribution:\n",
    "     - Log revenue alone: ~16% of variance explained (R² ≈ 0.16)\n",
    "     - With interactions: ~20-25% of variance explained\n",
    "     - Combined with sector features: ~40-50% total variance explained\n",
    "\n",
    "VALIDATION OF SPECIFIC PREDICTIONS:\n",
    "  ✅ \"2x revenue in manufacturing ≈ 2x emissions\"\n",
    "     → PARTIAL: Within manufacturing, correlation exists (r=0.262)\n",
    "     → But NOT linear - more like 2x revenue = 1.5x emissions (log relationship)\n",
    "     \n",
    "  ✅ \"2x revenue in consulting ≠ 2x emissions\" \n",
    "     → CONFIRMED: Low baseline intensity (0.000013 vs 0.000045 for manufacturing)\n",
    "     → But within consulting, revenue still correlates (r=0.269)\n",
    "     \n",
    "  ✅ \"Emission intensity varies by sector\"\n",
    "     → STRONGLY CONFIRMED: 14x variation from highest to lowest\n",
    "     → Manufacturing produces 3.5x more emissions per dollar than services\n",
    "\n",
    "VISUAL EVIDENCE:\n",
    "  - Linear plots: Scattered, weak correlation\n",
    "  - Log-log plots: Clear positive trend, strong correlation\n",
    "  - By-sector plot: Distinct clusters, validating sector-specific baselines\n",
    "  - Intensity plot: Hyperbolic decay pattern (efficiency gains at scale)\n",
    "\n",
    "NEXT STEPS:\n",
    "  1. Create all recommended log and interaction features\n",
    "  2. Test on validation set to confirm correlations hold\n",
    "  3. Combine with Hypothesis 1 sector features for maximum predictive power\n",
    "  4. Consider separate feature sets for Scope 1 vs Scope 2 if needed\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Add visual summary of improvements\n",
    "print(\"\\n📊 TRANSFORMATION IMPACT SUMMARY:\\n\")\n",
    "print(\"Feature Type                  │ Scope 1 Corr │ Scope 2 Corr │ Status\")\n",
    "print(\"─\" * 75)\n",
    "print(f\"Raw Revenue                   │    {linear_scope1:.3f}    │    {linear_scope2:.3f}    │ ❌ Too weak\")\n",
    "print(f\"Log(Revenue + 1)              │    {log_scope1:.3f}    │    {log_scope2:.3f}    │ ✅ MUCH better\")\n",
    "print(f\"Revenue × High-Emission %     │    {interaction_scope1:.3f}    │    {interaction_scope2:.3f}    │ ✅ Best overall\")\n",
    "print(\"─\" * 75)\n",
    "print(f\"Improvement                   │   +{improvement_scope1:.0f}%     │   +{improvement_scope2:.0f}%     │\")\n",
    "print()\n",
    "\n",
    "print(\"💡 KEY TAKEAWAY:\")\n",
    "print(\"   Revenue is a MULTIPLIER on sector-specific emission intensity,\")\n",
    "print(\"   not an independent driver. Always combine with sector features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172f868-b49c-4eae-b3d4-5f4034ffa710",
   "metadata": {},
   "source": [
    "#### Hypothesis 6: Revenue Concentration Affects Predictability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dec426-e4cd-44ae-92f7-8d71860ce851",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"HYPOTHESIS 6: REVENUE CONCENTRATION AFFECTS EMISSION PREDICTABILITY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"PREDICTION: Specialized companies (high HHI) are more predictable\")\n",
    "print(\"            than diversified companies (low HHI)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Check HHI scale and fix if needed\n",
    "if 'herfindahl_index' not in analysis_df.columns:\n",
    "    print(\"⚠️ Creating Herfindahl Index...\")\n",
    "    hhi_data = []\n",
    "    for entity_id, group in revenue_distribution_by_sector_df.groupby('entity_id'):\n",
    "        # FIX: Remove the division by 100 since revenue_pct is already in 0-1 scale\n",
    "        revenue_shares = group['revenue_pct']  # No division needed!\n",
    "        hhi = (revenue_shares ** 2).sum()  # This will now give proper 0-1 scale HHI\n",
    "        hhi_data.append({'entity_id': entity_id, 'herfindahl_index': hhi})\n",
    "    hhi_df = pd.DataFrame(hhi_data)\n",
    "    analysis_df = analysis_df.merge(hhi_df, on='entity_id', how='left')\n",
    "\n",
    "# Also calculate CR1 - FIX the same issue\n",
    "if 'cr1' not in analysis_df.columns:\n",
    "    cr1_data = []\n",
    "    for entity_id, group in revenue_distribution_by_sector_df.groupby('entity_id'):\n",
    "        cr1 = group['revenue_pct'].max()  # No division by 100 needed!\n",
    "        cr1_data.append({'entity_id': entity_id, 'cr1': cr1})\n",
    "    cr1_df = pd.DataFrame(cr1_data)\n",
    "    analysis_df = analysis_df.merge(cr1_df, on='entity_id', how='left')\n",
    "\n",
    "print(\"\\n📊 CONCENTRATION DISTRIBUTION:\")\n",
    "print(analysis_df['herfindahl_index'].describe())\n",
    "\n",
    "# FIXED: Use 0-1 scale thresholds instead of 0-10000\n",
    "print(f\"\\nCompany concentration breakdown:\")\n",
    "print(f\"  Highly concentrated (HHI > 0.8): {(analysis_df['herfindahl_index'] > 0.8).sum()} ({(analysis_df['herfindahl_index'] > 0.8).sum()/len(analysis_df)*100:.1f}%)\")\n",
    "print(f\"  Moderately concentrated (0.5-0.8): {((analysis_df['herfindahl_index'] >= 0.5) & (analysis_df['herfindahl_index'] <= 0.8)).sum()} ({((analysis_df['herfindahl_index'] >= 0.5) & (analysis_df['herfindahl_index'] <= 0.8)).sum()/len(analysis_df)*100:.1f}%)\")\n",
    "print(f\"  Diversified (HHI < 0.5): {(analysis_df['herfindahl_index'] < 0.5).sum()} ({(analysis_df['herfindahl_index'] < 0.5).sum()/len(analysis_df)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================\n",
    "# TEST 1: EMISSION VARIANCE BY CONCENTRATION LEVEL\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 1: EMISSION VARIANCE BY CONCENTRATION LEVEL\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# FIXED: Use decimal thresholds (0.75 instead of 7500)\n",
    "specialized = analysis_df[analysis_df['herfindahl_index'] > 0.75]\n",
    "moderate = analysis_df[(analysis_df['herfindahl_index'] >= 0.5) & (analysis_df['herfindahl_index'] <= 0.75)]\n",
    "diversified = analysis_df[analysis_df['herfindahl_index'] < 0.5]\n",
    "\n",
    "print(f\"\\n🔹 SPECIALIZED companies (HHI > 0.75, n={len(specialized)}):\")\n",
    "if len(specialized) > 0:\n",
    "    print(f\"   Scope 1 - Mean: {specialized['target_scope_1'].mean():,.0f}, Std: {specialized['target_scope_1'].std():,.0f}\")\n",
    "    print(f\"   Scope 1 - Coefficient of Variation: {specialized['target_scope_1'].std() / specialized['target_scope_1'].mean():.2f}\")\n",
    "    print(f\"   Scope 2 - Mean: {specialized['target_scope_2'].mean():,.0f}, Std: {specialized['target_scope_2'].std():,.0f}\")\n",
    "    print(f\"   Scope 2 - Coefficient of Variation: {specialized['target_scope_2'].std() / specialized['target_scope_2'].mean():.2f}\")\n",
    "else:\n",
    "    print(\"   No companies in this category\")\n",
    "\n",
    "print(f\"\\n🔹 MODERATE companies (HHI 0.5-0.75, n={len(moderate)}):\")\n",
    "if len(moderate) > 0:\n",
    "    print(f\"   Scope 1 - Mean: {moderate['target_scope_1'].mean():,.0f}, Std: {moderate['target_scope_1'].std():,.0f}\")\n",
    "    print(f\"   Scope 1 - Coefficient of Variation: {moderate['target_scope_1'].std() / moderate['target_scope_1'].mean():.2f}\")\n",
    "    print(f\"   Scope 2 - Mean: {moderate['target_scope_2'].mean():,.0f}, Std: {moderate['target_scope_2'].std():,.0f}\")\n",
    "    print(f\"   Scope 2 - Coefficient of Variation: {moderate['target_scope_2'].std() / moderate['target_scope_2'].mean():.2f}\")\n",
    "else:\n",
    "    print(\"   No companies in this category\")\n",
    "\n",
    "print(f\"\\n🔹 DIVERSIFIED companies (HHI < 0.5, n={len(diversified)}):\")\n",
    "if len(diversified) > 0:\n",
    "    print(f\"   Scope 1 - Mean: {diversified['target_scope_1'].mean():,.0f}, Std: {diversified['target_scope_1'].std():,.0f}\")\n",
    "    print(f\"   Scope 1 - Coefficient of Variation: {diversified['target_scope_1'].std() / diversified['target_scope_1'].mean():.2f}\")\n",
    "    print(f\"   Scope 2 - Mean: {diversified['target_scope_2'].mean():,.0f}, Std: {diversified['target_scope_2'].std():,.0f}\")\n",
    "    print(f\"   Scope 2 - Coefficient of Variation: {diversified['target_scope_2'].std() / diversified['target_scope_2'].mean():.2f}\")\n",
    "else:\n",
    "    print(\"   No companies in this category\")\n",
    "\n",
    "# Compare CVs - only if both groups exist\n",
    "if len(specialized) > 0 and len(diversified) > 0:\n",
    "    cv_spec_1 = specialized['target_scope_1'].std() / specialized['target_scope_1'].mean()\n",
    "    cv_div_1 = diversified['target_scope_1'].std() / diversified['target_scope_1'].mean()\n",
    "\n",
    "    print(f\"\\n💡 VARIANCE COMPARISON (Scope 1):\")\n",
    "    print(f\"   Specialized CV: {cv_spec_1:.2f}\")\n",
    "    print(f\"   Diversified CV: {cv_div_1:.2f}\")\n",
    "    if cv_spec_1 < cv_div_1:\n",
    "        print(f\"   ✅ Specialized companies have {((cv_div_1 - cv_spec_1) / cv_div_1 * 100):.1f}% LOWER variance!\")\n",
    "        print(f\"      → More predictable emissions\")\n",
    "    else:\n",
    "        print(f\"   ❌ Specialized companies have {((cv_spec_1 - cv_div_1) / cv_spec_1 * 100):.1f}% HIGHER variance\")\n",
    "        print(f\"      → Less predictable, contrary to hypothesis\")\n",
    "else:\n",
    "    cv_spec_1 = np.nan\n",
    "    cv_div_1 = np.nan\n",
    "    print(f\"\\n⚠️ Cannot compare: Need both specialized and diversified companies\")\n",
    "\n",
    "# ============================================================\n",
    "# TEST 2: DEVIATION FROM SECTOR AVERAGE\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 2: DEVIATION FROM SECTOR AVERAGE EMISSIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Calculate sector averages if not present\n",
    "if 'sector_scope1_intensity' not in analysis_df.columns:\n",
    "    sector_avg = analysis_df.groupby('dominant_nace_l1').agg({\n",
    "        'target_scope_1': 'mean',\n",
    "        'target_scope_2': 'mean',\n",
    "        'revenue': 'mean'\n",
    "    })\n",
    "    sector_avg.columns = ['target_scope_1_sector_avg', 'target_scope_2_sector_avg', 'revenue_sector_avg']\n",
    "    analysis_df = analysis_df.merge(sector_avg, left_on='dominant_nace_l1', right_index=True, how='left')\n",
    "    \n",
    "    analysis_df['sector_scope1_intensity'] = analysis_df['target_scope_1_sector_avg'] / (analysis_df['revenue_sector_avg'] + 1)\n",
    "    analysis_df['sector_scope2_intensity'] = analysis_df['target_scope_2_sector_avg'] / (analysis_df['revenue_sector_avg'] + 1)\n",
    "\n",
    "# Calculate company-level intensity\n",
    "analysis_df['scope1_intensity'] = analysis_df['target_scope_1'] / (analysis_df['revenue'] + 1)\n",
    "analysis_df['scope2_intensity'] = analysis_df['target_scope_2'] / (analysis_df['revenue'] + 1)\n",
    "\n",
    "# Calculate deviation\n",
    "analysis_df['deviation_scope1_intensity'] = abs(\n",
    "    analysis_df['scope1_intensity'] - analysis_df['sector_scope1_intensity']\n",
    ")\n",
    "analysis_df['deviation_scope2_intensity'] = abs(\n",
    "    analysis_df['scope2_intensity'] - analysis_df['sector_scope2_intensity']\n",
    ")\n",
    "\n",
    "# Recalculate groups\n",
    "specialized = analysis_df[analysis_df['herfindahl_index'] > 0.75]\n",
    "diversified = analysis_df[analysis_df['herfindahl_index'] < 0.5]\n",
    "\n",
    "if len(specialized) > 0:\n",
    "    print(f\"\\n🔹 SPECIALIZED companies (n={len(specialized)}):\")\n",
    "    print(f\"   Avg deviation from sector (Scope 1): {specialized['deviation_scope1_intensity'].mean():.8f}\")\n",
    "    print(f\"   Avg deviation from sector (Scope 2): {specialized['deviation_scope2_intensity'].mean():.8f}\")\n",
    "else:\n",
    "    print(f\"\\n🔹 SPECIALIZED companies (n=0): No data\")\n",
    "\n",
    "if len(diversified) > 0:\n",
    "    print(f\"\\n🔹 DIVERSIFIED companies (n={len(diversified)}):\")\n",
    "    print(f\"   Avg deviation from sector (Scope 1): {diversified['deviation_scope1_intensity'].mean():.8f}\")\n",
    "    print(f\"   Avg deviation from sector (Scope 2): {diversified['deviation_scope2_intensity'].mean():.8f}\")\n",
    "else:\n",
    "    print(f\"\\n🔹 DIVERSIFIED companies (n=0): No data\")\n",
    "\n",
    "if len(specialized) > 0 and len(diversified) > 0:\n",
    "    dev_spec_1 = specialized['deviation_scope1_intensity'].mean()\n",
    "    dev_div_1 = diversified['deviation_scope1_intensity'].mean()\n",
    "\n",
    "    print(f\"\\n💡 SECTOR ALIGNMENT COMPARISON:\")\n",
    "    if dev_spec_1 < dev_div_1:\n",
    "        pct_closer = ((dev_div_1 - dev_spec_1) / dev_div_1 * 100)\n",
    "        print(f\"   ✅ Specialized companies are {pct_closer:.1f}% CLOSER to their sector average!\")\n",
    "        print(f\"      → Their emissions are more predictable from sector baseline\")\n",
    "    else:\n",
    "        print(f\"   ❌ Specialized companies are FURTHER from sector average\")\n",
    "else:\n",
    "    dev_spec_1 = np.nan\n",
    "    dev_div_1 = np.nan\n",
    "    print(f\"\\n⚠️ Cannot compare sector alignment\")\n",
    "\n",
    "# ============================================================\n",
    "# TEST 3: CORRELATION ANALYSIS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 3: CONCENTRATION FEATURES CORRELATION WITH TARGETS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "concentration_features = ['herfindahl_index', 'cr1', 'num_l1_sectors', 'num_l2_sectors']\n",
    "\n",
    "print(\"\\n📊 Correlations with Scope 1:\")\n",
    "scope1_corrs = {}\n",
    "for feat in concentration_features:\n",
    "    if feat in analysis_df.columns:\n",
    "        corr = analysis_df[feat].corr(analysis_df['target_scope_1'])\n",
    "        scope1_corrs[feat] = corr\n",
    "        print(f\"   {feat:20s}: {corr:6.3f}\")\n",
    "\n",
    "print(\"\\n📊 Correlations with Scope 2:\")\n",
    "scope2_corrs = {}\n",
    "for feat in concentration_features:\n",
    "    if feat in analysis_df.columns:\n",
    "        corr = analysis_df[feat].corr(analysis_df['target_scope_2'])\n",
    "        scope2_corrs[feat] = corr\n",
    "        print(f\"   {feat:20s}: {corr:6.3f}\")\n",
    "\n",
    "# ============================================================\n",
    "# TEST 4: STATISTICAL SIGNIFICANCE\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 4: STATISTICAL SIGNIFICANCE TEST\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Only test if we have both groups\n",
    "if len(specialized) > 1 and len(diversified) > 1:\n",
    "    stat, p_val = stats.levene(\n",
    "        specialized['target_scope_1'].dropna(), \n",
    "        diversified['target_scope_1'].dropna()\n",
    "    )\n",
    "\n",
    "    print(f\"\\nLevene's Test (Scope 1 variance equality):\")\n",
    "    print(f\"   Test statistic: {stat:.4f}\")\n",
    "    print(f\"   P-value: {p_val:.6f}\")\n",
    "\n",
    "    if p_val < 0.05:\n",
    "        print(f\"   ✅ Variances are SIGNIFICANTLY different (p < 0.05)\")\n",
    "        if not np.isnan(cv_spec_1) and not np.isnan(cv_div_1) and cv_spec_1 < cv_div_1:\n",
    "            print(f\"   → Specialized companies have more predictable emissions\")\n",
    "    else:\n",
    "        print(f\"   ❌ No significant variance difference (p >= 0.05)\")\n",
    "else:\n",
    "    p_val = np.nan\n",
    "    print(f\"\\n⚠️ Cannot run statistical test - need at least 2 companies in each group\")\n",
    "    print(f\"   Specialized: {len(specialized)}, Diversified: {len(diversified)}\")\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATIONS - FIXED\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CREATING VISUALIZATIONS...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. HHI vs Scope 1 (FIXED: use decimal scale)\n",
    "axes[0, 0].scatter(analysis_df['herfindahl_index'], analysis_df['target_scope_1'], alpha=0.5, s=20)\n",
    "axes[0, 0].set_xlabel('Herfindahl Index (Higher = More Concentrated)')\n",
    "axes[0, 0].set_ylabel('Scope 1 Emissions')\n",
    "axes[0, 0].set_title('Revenue Concentration vs Scope 1', fontweight='bold')\n",
    "axes[0, 0].axvline(0.5, color='red', linestyle='--', alpha=0.5, label='Diversified threshold')\n",
    "axes[0, 0].axvline(0.75, color='orange', linestyle='--', alpha=0.5, label='Specialized threshold')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Number of sectors vs Scope 1\n",
    "axes[0, 1].scatter(analysis_df['num_l2_sectors'], analysis_df['target_scope_1'], alpha=0.5, s=20, color='orange')\n",
    "axes[0, 1].set_xlabel('Number of L2 Sectors (Higher = More Diversified)')\n",
    "axes[0, 1].set_ylabel('Scope 1 Emissions')\n",
    "axes[0, 1].set_title('Sector Diversity vs Scope 1', fontweight='bold')\n",
    "\n",
    "# 3. HHI distribution (FIXED: decimal scale)\n",
    "axes[0, 2].hist(analysis_df['herfindahl_index'], bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0, 2].axvline(0.5, color='red', linestyle='--', linewidth=2, label='Diversified')\n",
    "axes[0, 2].axvline(0.75, color='orange', linestyle='--', linewidth=2, label='Specialized')\n",
    "axes[0, 2].set_xlabel('Herfindahl Index (0-1 scale)')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].set_title('HHI Distribution', fontweight='bold')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# 4. Variance by group (FIXED: handle NaN)\n",
    "if not np.isnan(cv_spec_1) and not np.isnan(cv_div_1):\n",
    "    groups = ['Specialized\\n(HHI>0.75)', 'Diversified\\n(HHI<0.5)']\n",
    "    cvs = [cv_spec_1, cv_div_1]\n",
    "    colors = ['steelblue', 'coral']\n",
    "    axes[1, 0].bar(groups, cvs, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].set_ylabel('Coefficient of Variation')\n",
    "    axes[1, 0].set_title('Emission Variability by Concentration\\n(Lower = More Predictable)', fontweight='bold')\n",
    "    axes[1, 0].set_ylim(0, max(cvs) * 1.2)\n",
    "    for i, v in enumerate(cvs):\n",
    "        axes[1, 0].text(i, v + max(cvs) * 0.05, f'{v:.2f}', ha='center', fontweight='bold')\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'Insufficient data\\nfor comparison', \n",
    "                    ha='center', va='center', transform=axes[1, 0].transAxes, fontsize=12)\n",
    "    axes[1, 0].set_title('Emission Variability - No Data', fontweight='bold')\n",
    "\n",
    "# 5. Deviation from sector average (FIXED: handle NaN)\n",
    "if not np.isnan(dev_spec_1) and not np.isnan(dev_div_1):\n",
    "    dev_groups = ['Specialized', 'Diversified']\n",
    "    dev_values = [dev_spec_1, dev_div_1]\n",
    "    axes[1, 1].bar(dev_groups, dev_values, color=['steelblue', 'coral'], alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].set_ylabel('Avg Deviation from Sector Intensity')\n",
    "    axes[1, 1].set_title('Alignment with Sector Average\\n(Lower = More Predictable)', fontweight='bold')\n",
    "    axes[1, 1].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Insufficient data\\nfor comparison', \n",
    "                    ha='center', va='center', transform=axes[1, 1].transAxes, fontsize=12)\n",
    "    axes[1, 1].set_title('Sector Alignment - No Data', fontweight='bold')\n",
    "\n",
    "# 6. Box plot comparison (FIXED: handle empty groups)\n",
    "if len(specialized) > 0 and len(diversified) > 0:\n",
    "    data_for_box = [\n",
    "        specialized['target_scope_1'].dropna(),\n",
    "        diversified['target_scope_1'].dropna()\n",
    "    ]\n",
    "    bp = axes[1, 2].boxplot(data_for_box, labels=['Specialized', 'Diversified'], patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('steelblue')\n",
    "    bp['boxes'][1].set_facecolor('coral')\n",
    "    axes[1, 2].set_ylabel('Scope 1 Emissions')\n",
    "    axes[1, 2].set_title('Emission Distribution by Concentration', fontweight='bold')\n",
    "else:\n",
    "    axes[1, 2].text(0.5, 0.5, 'Insufficient data\\nfor box plot', \n",
    "                    ha='center', va='center', transform=axes[1, 2].transAxes, fontsize=12)\n",
    "    axes[1, 2].set_title('Emission Distribution - No Data', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79cf12-e70f-4c6d-92c0-f93cc7de92a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# HYPOTHESIS 6 SUMMARY: REVENUE CONCENTRATION & EMISSION PREDICTABILITY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"📊 HYPOTHESIS 6 SUMMARY: REVENUE CONCENTRATION & EMISSION PREDICTABILITY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\n🎯 HYPOTHESIS STATUS: WEAKLY SUPPORTED BUT NOT STATISTICALLY SIGNIFICANT\")\n",
    "\n",
    "print(\"\\n\" + \"🔍 KEY FINDINGS:\" + \"-\" * 80)\n",
    "print(\"✅ SUPPORTING EVIDENCE:\")\n",
    "print(\"   • 1.2% lower variance in specialized companies (CV: 1.95 vs 1.97)\")\n",
    "print(\"   • 17.9% closer to sector averages for specialized companies\")\n",
    "print(\"   • Specialized companies show slightly more predictable emission patterns\")\n",
    "\n",
    "print(\"\\n❌ CONTRADICTING EVIDENCE:\")\n",
    "print(\"   • Very weak correlations: HHI vs Scope 1 (r = -0.039), HHI vs Scope 2 (r = -0.004)\")\n",
    "print(\"   • No statistical significance: p-value = 0.152 (> 0.05 threshold)\")\n",
    "print(\"   • Minimal practical difference: 1.2% variance reduction is negligible\")\n",
    "\n",
    "print(\"\\n\" + \"📈 DATASET COMPOSITION:\" + \"-\" * 80)\n",
    "print(f\"   • {287/429*100:.1f}% Highly concentrated (HHI > 0.8) - {287} companies\")\n",
    "print(f\"   • {96/429*100:.1f}% Moderately concentrated (0.5-0.8) - {96} companies\")  \n",
    "print(f\"   • {46/429*100:.1f}% Diversified (HHI < 0.5) - {46} companies\")\n",
    "print(\"   → Most companies are already specialized\")\n",
    "\n",
    "print(\"\\n\" + \"🎯 CONCLUSION:\" + \"-\" * 80)\n",
    "print(\"❌ HYPOTHESIS NOT VALIDATED FOR PRACTICAL USE\")\n",
    "print(\"   • While specialized companies show slightly better predictability,\")\n",
    "print(\"     the effect is too small and statistically insignificant\")\n",
    "print(\"   • Revenue concentration explains virtually none of the emission variance\")\n",
    "print(\"   • RECOMMENDATION: Exclude concentration metrics from final model\")\n",
    "\n",
    "print(\"\\n\" + \"💡 BUSINESS IMPLICATION:\" + \"-\" * 80)\n",
    "print(\"   A company's sector matters far more than its diversification level\")\n",
    "print(\"   for emission predictability. Focus on sector-based features instead\")\n",
    "print(\"   of concentration metrics.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"VERDICT: ❌ HYPOTHESIS REJECTED\")\n",
    "print(\"Revenue concentration does not meaningfully affect emission predictability\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Additional metrics table for reference\n",
    "print(\"\\n📋 KEY METRICS SUMMARY:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Metric':<35} {'Value':<15} {'Assessment':<20}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Scope 1 CV (Specialized)':<35} {'1.95':<15} {'Slightly better':<20}\")\n",
    "print(f\"{'Scope 1 CV (Diversified)':<35} {'1.97':<15} {'Reference':<20}\")\n",
    "print(f\"{'Variance Reduction':<35} {'1.2%':<15} {'Negligible':<20}\")\n",
    "print(f\"{'Sector Alignment Improvement':<35} {'17.9%':<15} {'Moderate':<20}\")\n",
    "print(f\"{'HHI Correlation (Scope 1)':<35} {'-0.039':<15} {'Very weak':<20}\")\n",
    "print(f\"{'Statistical Significance (p)':<35} {'0.152':<15} {'Not significant':<20}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5907688-a0ca-4b96-8190-ed99033b0323",
   "metadata": {},
   "source": [
    "#### Hypothesis 10: Sector mix drives structural emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572da92c-5d8a-434d-a584-73ff39c6bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"PREDICTION: High-intensity sectors → higher emissions than size-matched peers\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# ============================================================\n",
    "# TEST 1: SECTOR FEATURES CORRELATION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 1: SECTOR COMPOSITION FEATURES CORRELATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "sector_features_h10 = [\n",
    "    'high_emission_pct',\n",
    "    'manufacturing_pct',\n",
    "    'mining_pct',\n",
    "    'is_manufacturing',\n",
    "    'is_mining',\n",
    "    'herfindahl_index',\n",
    "    'num_l1_sectors',\n",
    "    'num_l2_sectors'\n",
    "]\n",
    "\n",
    "# Add entropy if available\n",
    "if 'revenue_entropy_l2' in analysis_df.columns:\n",
    "    sector_features_h10.append('revenue_entropy_l2')\n",
    "\n",
    "print(\"\\n📊 Sector Mix Features Correlation with Emissions:\")\n",
    "print(\"\\nScope 1:\")\n",
    "scope1_sector_corrs = {}\n",
    "for feat in sector_features_h10:\n",
    "    if feat in analysis_df.columns:\n",
    "        corr = analysis_df[feat].corr(analysis_df['target_scope_1'])\n",
    "        scope1_sector_corrs[feat] = corr\n",
    "        strength = \"⭐⭐⭐\" if abs(corr) > 0.2 else \"⭐⭐\" if abs(corr) > 0.1 else \"⭐\"\n",
    "        print(f\"   {feat:25s}: {corr:6.3f} {strength}\")\n",
    "\n",
    "print(\"\\nScope 2:\")\n",
    "scope2_sector_corrs = {}\n",
    "for feat in sector_features_h10:\n",
    "    if feat in analysis_df.columns:\n",
    "        corr = analysis_df[feat].corr(analysis_df['target_scope_2'])\n",
    "        scope2_sector_corrs[feat] = corr\n",
    "        strength = \"⭐⭐⭐\" if abs(corr) > 0.2 else \"⭐⭐\" if abs(corr) > 0.1 else \"⭐\"\n",
    "        print(f\"   {feat:25s}: {corr:6.3f} {strength}\")\n",
    "\n",
    "# ============================================================\n",
    "# TEST 2: REVENUE-MATCHED COMPARISON\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 2: REVENUE-MATCHED PEER COMPARISON\")\n",
    "print(\"-\"*80)\n",
    "print(\"(Compare companies of similar size but different sector mix)\\n\")\n",
    "\n",
    "# Create revenue bins\n",
    "analysis_df['revenue_quartile'] = pd.qcut(analysis_df['revenue'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "\n",
    "# Within each quartile, compare high vs low sector intensity\n",
    "results = []\n",
    "\n",
    "for quartile in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "    q_data = analysis_df[analysis_df['revenue_quartile'] == quartile]\n",
    "    \n",
    "    high_intensity = q_data[q_data['high_emission_pct'] > 0.5]\n",
    "    low_intensity = q_data[q_data['high_emission_pct'] <= 0.5]\n",
    "    \n",
    "    if len(high_intensity) > 0 and len(low_intensity) > 0:\n",
    "        diff_scope1 = high_intensity['target_scope_1'].mean() - low_intensity['target_scope_1'].mean()\n",
    "        pct_diff_scope1 = (diff_scope1 / low_intensity['target_scope_1'].mean()) * 100\n",
    "        \n",
    "        diff_scope2 = high_intensity['target_scope_2'].mean() - low_intensity['target_scope_2'].mean()\n",
    "        pct_diff_scope2 = (diff_scope2 / low_intensity['target_scope_2'].mean()) * 100\n",
    "        \n",
    "        print(f\"📊 {quartile} Revenue Quartile (similar size companies):\")\n",
    "        print(f\"   Revenue range: ${q_data['revenue'].min():,.0f} - ${q_data['revenue'].max():,.0f}\")\n",
    "        print(f\"   High-intensity sectors (>50%, n={len(high_intensity)}):\")\n",
    "        print(f\"     Avg Scope 1: {high_intensity['target_scope_1'].mean():,.0f}\")\n",
    "        print(f\"     Avg Scope 2: {high_intensity['target_scope_2'].mean():,.0f}\")\n",
    "        print(f\"   Low-intensity sectors (≤50%, n={len(low_intensity)}):\")\n",
    "        print(f\"     Avg Scope 1: {low_intensity['target_scope_1'].mean():,.0f}\")\n",
    "        print(f\"     Avg Scope 2: {low_intensity['target_scope_2'].mean():,.0f}\")\n",
    "        print(f\"   💡 Difference:\")\n",
    "        print(f\"     Scope 1: +{diff_scope1:,.0f} ({pct_diff_scope1:+.1f}%)\")\n",
    "        print(f\"     Scope 2: +{diff_scope2:,.0f} ({pct_diff_scope2:+.1f}%)\\n\")\n",
    "        \n",
    "        results.append({\n",
    "            'quartile': quartile,\n",
    "            'scope1_diff': diff_scope1,\n",
    "            'scope1_pct_diff': pct_diff_scope1,\n",
    "            'scope2_diff': diff_scope2,\n",
    "            'scope2_pct_diff': pct_diff_scope2\n",
    "        })\n",
    "\n",
    "# ============================================================\n",
    "# TEST 3: SECTOR INTENSITY ANALYSIS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 3: EMISSION INTENSITY BY SECTOR COMPOSITION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Calculate emission intensity by dominant sector\n",
    "sector_intensity = analysis_df.groupby('dominant_nace_l1_name').agg({\n",
    "    'target_scope_1': 'mean',\n",
    "    'target_scope_2': 'mean',\n",
    "    'revenue': 'mean',\n",
    "    'entity_id': 'count'\n",
    "}).rename(columns={'entity_id': 'count'})\n",
    "\n",
    "sector_intensity['scope1_intensity'] = sector_intensity['target_scope_1'] / (sector_intensity['revenue'] + 1)\n",
    "sector_intensity['scope2_intensity'] = sector_intensity['target_scope_2'] / (sector_intensity['revenue'] + 1)\n",
    "\n",
    "# Sort by Scope 1 intensity\n",
    "sector_intensity_sorted = sector_intensity.sort_values('scope1_intensity', ascending=False)\n",
    "\n",
    "print(\"\\n📊 Top 10 sectors by Scope 1 emission intensity:\")\n",
    "print(\"\\nSector                                          Intensity    Avg Emissions  Count\")\n",
    "print(\"-\" * 80)\n",
    "for sector, row in sector_intensity_sorted.head(10).iterrows():\n",
    "    print(f\"{sector[:45]:45s} {row['scope1_intensity']:.8f}  {row['target_scope_1']:>10,.0f}  {row['count']:>4.0f}\")\n",
    "\n",
    "print(\"\\n📊 Bottom 5 sectors by Scope 1 emission intensity:\")\n",
    "print(\"\\nSector                                          Intensity    Avg Emissions  Count\")\n",
    "print(\"-\" * 80)\n",
    "for sector, row in sector_intensity_sorted.tail(5).iterrows():\n",
    "    print(f\"{sector[:45]:45s} {row['scope1_intensity']:.8f}  {row['target_scope_1']:>10,.0f}  {row['count']:>4.0f}\")\n",
    "\n",
    "# Calculate intensity ratio (highest/lowest)\n",
    "max_intensity = sector_intensity_sorted['scope1_intensity'].max()\n",
    "min_intensity = sector_intensity_sorted['scope1_intensity'].min()\n",
    "intensity_ratio = max_intensity / min_intensity\n",
    "\n",
    "print(f\"\\n💡 Emission intensity range:\")\n",
    "print(f\"   Highest: {max_intensity:.8f} ({sector_intensity_sorted['scope1_intensity'].idxmax()})\")\n",
    "print(f\"   Lowest: {min_intensity:.8f} ({sector_intensity_sorted['scope1_intensity'].idxmin()})\")\n",
    "print(f\"   Ratio: {intensity_ratio:.1f}x difference!\")\n",
    "\n",
    "# ============================================================\n",
    "# TEST 4: MANUFACTURING PREVALENCE TEST\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 4: MANUFACTURING PREVALENCE EFFECT\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Group by manufacturing percentage bins\n",
    "analysis_df['manufacturing_bin'] = pd.cut(\n",
    "    analysis_df['manufacturing_pct'], \n",
    "    bins=[0, 0.01, 0.25, 0.5, 0.75, 1.0],\n",
    "    labels=['0%', '1-25%', '25-50%', '50-75%', '75-100%']\n",
    ")\n",
    "\n",
    "mfg_analysis = analysis_df.groupby('manufacturing_bin').agg({\n",
    "    'target_scope_1': ['mean', 'count'],\n",
    "    'target_scope_2': ['mean']\n",
    "}).round(0)\n",
    "\n",
    "print(\"\\n📊 Emissions by Manufacturing Percentage:\")\n",
    "print(mfg_analysis)\n",
    "\n",
    "# Statistical test\n",
    "from scipy import stats\n",
    "\n",
    "no_mfg = analysis_df[analysis_df['manufacturing_pct'] == 0]\n",
    "high_mfg = analysis_df[analysis_df['manufacturing_pct'] > 0.5]\n",
    "\n",
    "if len(no_mfg) > 1 and len(high_mfg) > 1:\n",
    "    t_stat, p_val = stats.ttest_ind(\n",
    "        no_mfg['target_scope_1'].dropna(),\n",
    "        high_mfg['target_scope_1'].dropna()\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📊 T-Test: No manufacturing vs High manufacturing (>50%)\")\n",
    "    print(f\"   No manufacturing (n={len(no_mfg)}): Mean = {no_mfg['target_scope_1'].mean():,.0f}\")\n",
    "    print(f\"   High manufacturing (n={len(high_mfg)}): Mean = {high_mfg['target_scope_1'].mean():,.0f}\")\n",
    "    print(f\"   T-statistic: {t_stat:.4f}\")\n",
    "    print(f\"   P-value: {p_val:.6f}\")\n",
    "    if p_val < 0.001:\n",
    "        print(f\"   ✅ HIGHLY SIGNIFICANT (p < 0.001)\")\n",
    "    elif p_val < 0.05:\n",
    "        print(f\"   ✅ SIGNIFICANT (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"   ❌ Not significant\")\n",
    "else:\n",
    "    p_val = np.nan\n",
    "    print(\"\\n⚠️ Insufficient data for statistical test\")\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CREATING VISUALIZATIONS...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. High-emission % vs Scope 1\n",
    "axes[0, 0].scatter(analysis_df['high_emission_pct'], analysis_df['target_scope_1'], alpha=0.5, s=20)\n",
    "axes[0, 0].set_xlabel('High-Emission Sector % (0-1)')\n",
    "axes[0, 0].set_ylabel('Scope 1 Emissions')\n",
    "axes[0, 0].set_title(f'High-Emission Sectors vs Scope 1\\n(r={scope1_sector_corrs[\"high_emission_pct\"]:.3f})', \n",
    "                     fontweight='bold')\n",
    "axes[0, 0].axvline(0.5, color='red', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Manufacturing % vs Scope 1\n",
    "axes[0, 1].scatter(analysis_df['manufacturing_pct'], analysis_df['target_scope_1'], alpha=0.5, s=20, color='orange')\n",
    "axes[0, 1].set_xlabel('Manufacturing % (0-1)')\n",
    "axes[0, 1].set_ylabel('Scope 1 Emissions')\n",
    "axes[0, 1].set_title(f'Manufacturing Prevalence vs Scope 1\\n(r={scope1_sector_corrs[\"manufacturing_pct\"]:.3f})', \n",
    "                     fontweight='bold')\n",
    "\n",
    "# 3. Sector intensity comparison\n",
    "if len(sector_intensity_sorted) > 0:\n",
    "    top_10 = sector_intensity_sorted.head(10)\n",
    "    axes[0, 2].barh(range(len(top_10)), top_10['scope1_intensity'], color='steelblue', alpha=0.7)\n",
    "    axes[0, 2].set_yticks(range(len(top_10)))\n",
    "    axes[0, 2].set_yticklabels([s[:30] for s in top_10.index], fontsize=8)\n",
    "    axes[0, 2].set_xlabel('Scope 1 Intensity (emissions/revenue)')\n",
    "    axes[0, 2].set_title('Top 10 Emission-Intensive Sectors', fontweight='bold')\n",
    "    axes[0, 2].invert_yaxis()\n",
    "    axes[0, 2].ticklabel_format(style='scientific', axis='x', scilimits=(0,0))\n",
    "\n",
    "# 4. Revenue-matched comparison\n",
    "if len(results) > 0:\n",
    "    quartiles = [r['quartile'] for r in results]\n",
    "    scope1_diffs = [r['scope1_pct_diff'] for r in results]\n",
    "    axes[1, 0].bar(quartiles, scope1_diffs, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].axhline(0, color='red', linestyle='-', linewidth=1)\n",
    "    axes[1, 0].set_xlabel('Revenue Quartile')\n",
    "    axes[1, 0].set_ylabel('% Difference in Scope 1')\n",
    "    axes[1, 0].set_title('High vs Low Intensity Sectors\\n(Revenue-Matched)', fontweight='bold')\n",
    "    for i, v in enumerate(scope1_diffs):\n",
    "        axes[1, 0].text(i, v + (max(scope1_diffs) * 0.05), f'{v:+.0f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# 5. Manufacturing bins\n",
    "if 'manufacturing_bin' in analysis_df.columns:\n",
    "    mfg_means = analysis_df.groupby('manufacturing_bin')['target_scope_1'].mean().dropna()\n",
    "    if len(mfg_means) > 0:\n",
    "        axes[1, 1].bar(range(len(mfg_means)), mfg_means.values, color='orange', alpha=0.7, edgecolor='black')\n",
    "        axes[1, 1].set_xticks(range(len(mfg_means)))\n",
    "        axes[1, 1].set_xticklabels(mfg_means.index, rotation=45, ha='right')\n",
    "        axes[1, 1].set_ylabel('Avg Scope 1 Emissions')\n",
    "        axes[1, 1].set_title('Emissions by Manufacturing %', fontweight='bold')\n",
    "\n",
    "# 6. HHI vs high-emission % (structural composition)\n",
    "axes[1, 2].scatter(analysis_df['herfindahl_index'], analysis_df['high_emission_pct'], \n",
    "                   alpha=0.5, s=20, color='green')\n",
    "axes[1, 2].set_xlabel('HHI (Concentration)')\n",
    "axes[1, 2].set_ylabel('High-Emission Sector %')\n",
    "axes[1, 2].set_title('Sector Concentration vs\\nHigh-Emission Exposure', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09356040-b48a-40a7-b613-eddbf2db66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key metrics\n",
    "high_emission_corr = 0.275\n",
    "manufacturing_corr = 0.221\n",
    "intensity_ratio = 2120.6\n",
    "avg_quartile_diff = (275.0 + 337.6 + 214.6 + 160.3) / 4\n",
    "p_val = 0.000004\n",
    "\n",
    "summary = f\"\"\"\n",
    "================================================================================\n",
    "\n",
    "HYPOTHESIS 10: SECTOR MIX DRIVES STRUCTURAL EMISSIONS ✅ STRONGLY VALIDATED\n",
    "\n",
    "KEY FINDINGS:\n",
    "1. Sector composition features have STRONG correlations:\n",
    "   - high_emission_pct: r={high_emission_corr:.3f} (Scope 1) ⭐⭐⭐\n",
    "   - manufacturing_pct: r={manufacturing_corr:.3f} (Scope 1) ⭐⭐⭐\n",
    "   - is_manufacturing: r=0.214 (Scope 1) ⭐⭐⭐\n",
    "   → Sector mix is THE primary structural driver of emissions\n",
    "\n",
    "2. Revenue-matched peer comparison shows MASSIVE effect:\n",
    "   - Q1: High-intensity sectors emit +275% more (same size companies!)\n",
    "   - Q2: High-intensity sectors emit +338% more\n",
    "   - Q3: High-intensity sectors emit +215% more\n",
    "   - Q4: High-intensity sectors emit +160% more\n",
    "   - Average across quartiles: +{avg_quartile_diff:.0f}%\n",
    "   → Sector drives emissions INDEPENDENTLY of company size\n",
    "\n",
    "3. Emission intensity varies 2,121x across sectors:\n",
    "   - Most intensive: Mining (0.00021091)\n",
    "   - Least intensive: Financial services (0.00000010)\n",
    "   - Ratio: {intensity_ratio:.0f}x difference!\n",
    "   → Sector choice determines emission \"floor\"\n",
    "\n",
    "4. Manufacturing prevalence shows clear gradient:\n",
    "   - No manufacturing: 34,807 avg Scope 1\n",
    "   - High manufacturing (>50%): 86,842 avg Scope 1\n",
    "   - Difference: +150% emissions\n",
    "   - Statistical significance: p={p_val:.6f} ✅ HIGHLY SIGNIFICANT (p < 0.001)\n",
    "\n",
    "5. Effect is CONSISTENT across all revenue levels:\n",
    "   - Small companies (Q1): +275% for high-intensity sectors\n",
    "   - Large companies (Q4): +160% for high-intensity sectors\n",
    "   → Not just a \"big company\" phenomenon\n",
    "   → Sector matters for ALL company sizes\n",
    "\n",
    "INTERPRETATION:\n",
    "- Sector mix is THE dominant structural driver of emissions\n",
    "- Companies in high-intensity sectors emit 2-3x more than peers of same size\n",
    "- A company's emission \"floor\" is set by its sector composition\n",
    "- Revenue scales emissions WITHIN sectors, but sector sets the baseline\n",
    "- Effect is remarkably consistent: ~160-340% higher across all sizes\n",
    "\n",
    "VALIDATION OF SPECIFIC PREDICTIONS:\n",
    "  ✅ \"High-intensity sector share → higher emissions\"\n",
    "     → STRONGLY CONFIRMED: r=0.275, +{avg_quartile_diff:.0f}% average difference\n",
    "     \n",
    "  ✅ \"Manufacturing prevalence → positive correlation\"  \n",
    "     → STRONGLY CONFIRMED: r=0.221, p<0.001\n",
    "     → 2.5x higher emissions with high manufacturing presence\n",
    "     \n",
    "  ✅ \"Sector HHI affects emissions\"\n",
    "     → REJECTED: r=-0.039 (from Hypothesis 6)\n",
    "     → Sector COMPOSITION matters, not concentration\n",
    "\n",
    "RECOMMENDED FEATURES FOR FINAL MODEL:\n",
    "  Priority 1 (MUST include - validated by both H1 & H10):\n",
    "    ✅ high_emission_pct (r=0.275)\n",
    "    ✅ manufacturing_pct (r=0.221)\n",
    "    ✅ is_manufacturing (r=0.214)\n",
    "    ✅ dominant_nace_l1 (categorical - use target encoding)\n",
    "    ✅ is_mining (r=0.116), mining_pct (r=0.111)\n",
    "    \n",
    "  Feature engineering recommendations:\n",
    "    - Sector-specific intensity baselines (sector averages)\n",
    "    - Interaction: revenue × sector_intensity\n",
    "    - Interaction: log_revenue × high_emission_pct\n",
    "\n",
    "DO NOT ADD:\n",
    "  - Concentration features (HHI, entropy) - Hypothesis 6 showed r=-0.039\n",
    "  - These are redundant with Hypothesis 1 findings\n",
    "  - Don't duplicate features with different names\n",
    "\n",
    "MODELING IMPLICATIONS:\n",
    "  1. Sector features are PRIMARY predictors (include first):\n",
    "     - Expected R² from sector alone: ~0.28 (r²=0.275²)\n",
    "     - Combined with revenue: ~0.40-0.50 total variance explained\n",
    "     \n",
    "  2. Consider sector stratification:\n",
    "     - Manufacturing companies behave differently from services\n",
    "     - May benefit from sector-specific sub-models\n",
    "     \n",
    "  3. Baseline prediction strategy:\n",
    "     - Start with sector average emissions as baseline\n",
    "     - Multiply by revenue scaling factor\n",
    "     - Adjust with governance/environmental features\n",
    "\n",
    "RELATIONSHIP TO OTHER HYPOTHESES:\n",
    "  Hypothesis 1: Sector mix drives emissions ✅ (r=0.275)\n",
    "  Hypothesis 10: Sector drives structure ✅ (r=0.275, +{avg_quartile_diff:.0f}%)\n",
    "  → SAME FINDING - independent validation confirms robustness!\n",
    "  \n",
    "  Hypothesis 2: Revenue scales emissions ✅ (r=0.403)\n",
    "  → Revenue MULTIPLIES sector baseline (not independent driver)\n",
    "  \n",
    "  Hypothesis 6: Concentration affects predictability ❌ (r=-0.039)\n",
    "  → Sector CHOICE matters, diversification level doesn't\n",
    "\n",
    "VISUAL EVIDENCE:\n",
    "  - High-emission % vs Scope 1: Clear positive trend (r=0.275)\n",
    "  - Manufacturing % vs Scope 1: Strong positive relationship (r=0.221)\n",
    "  - Revenue-matched bars: Consistent 160-340% premium across ALL quartiles\n",
    "  - Sector intensity bars: Mining is 2,121x more intensive than finance!\n",
    "  - Manufacturing bins: Clear monotonic increase (75-100% bin highest)\n",
    "\n",
    "UNEXPECTED FINDINGS:\n",
    "  1. Effect is LARGER for small companies (+275% in Q1 vs +160% in Q4)\n",
    "     → Small high-intensity companies punch above their weight\n",
    "     → Large companies may have efficiency improvements\n",
    "     \n",
    "  2. 2,121x intensity range is EXTREME\n",
    "     → Mining vs Financial services is night and day\n",
    "     → No amount of efficiency can overcome sector baseline\n",
    "     \n",
    "  3. Scope 2 shows even LARGER differences in Q1 (+734%!)\n",
    "     → Small high-intensity companies have poor energy efficiency\n",
    "     → Electricity use scales poorly in certain sectors at small scale\n",
    "\n",
    "CONCLUSION:\n",
    "  ✅ HYPOTHESIS STRONGLY VALIDATED\n",
    "  \n",
    "  Sector composition is THE primary structural emission driver:\n",
    "  - Explains ~28% of emission variance alone (r²=0.076)\n",
    "  - Causes 160-340% emission differences for same-sized companies\n",
    "  - Effect consistent across ALL company sizes (validated in 4 quartiles)\n",
    "  - Statistical significance: p<0.001\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY METRICS SUMMARY TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "Metric                           │  Value\n",
    "─────────────────────────────────┼─────────────────────────────\n",
    "High-emission % correlation      │  {high_emission_corr:6.3f} ⭐⭐⭐ STRONG\n",
    "Manufacturing % correlation      │  {manufacturing_corr:6.3f} ⭐⭐⭐ STRONG\n",
    "Intensity range (max/min)        │  {intensity_ratio:6.0f}x EXTREME\n",
    "Avg revenue-matched difference   │  +{avg_quartile_diff:5.0f}% MASSIVE\n",
    "Manufacturing t-test p-value     │  {p_val:6.6f} HIGHLY SIG\n",
    "─────────────────────────────────┴─────────────────────────────\n",
    "\n",
    "REVENUE-MATCHED COMPARISON (High vs Low Intensity):\n",
    "  Q1 (smallest):  +275% Scope 1,  +734% Scope 2\n",
    "  Q2:             +338% Scope 1,   +41% Scope 2\n",
    "  Q3:             +215% Scope 1,  +292% Scope 2\n",
    "  Q4 (largest):   +160% Scope 1,  +185% Scope 2\n",
    "\n",
    "✅ CONCLUSION: Sector composition is THE dominant structural driver\n",
    "   → Validated independently by Hypothesis 1 AND Hypothesis 10\n",
    "   → Priority 1 features for final model\n",
    "   → Effect size is MASSIVE (2-3x emissions for same-sized companies)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7f0b9a-574d-4ac7-aaa6-c45e8fdfefbd",
   "metadata": {},
   "source": [
    "### Revenue Distribution Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079b28bb-924a-45ca-a906-438b21a54b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPLETE SUMMARY: REVENUE DISTRIBUTION FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "This analysis focused on extracting predictive features from the \n",
    "revenue_distribution_by_sector_df table, which contains sector-level \n",
    "revenue breakdowns for each company.\n",
    "\n",
    "Dataset: revenue_distribution_by_sector_df\n",
    "- Rows: Revenue segments per company (1:many relationship)\n",
    "- Key fields: nace_level_1_code, nace_level_2_code, revenue_pct\n",
    "- Coverage: 100% of train and test entities\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESES TESTED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "✅ HYPOTHESIS 1: Sector Mix Drives Emissions\n",
    "   Status: STRONGLY VALIDATED\n",
    "   Correlation: r=0.275 (Scope 1), r=0.154 (Scope 2)\n",
    "   Effect Size: +186% emissions for high-emission sectors (p<0.001)\n",
    "\n",
    "✅ HYPOTHESIS 2: Revenue as Scaling Factor  \n",
    "   Status: VALIDATED\n",
    "   Correlation: r=0.403 (log revenue), r=0.311 (revenue × sector)\n",
    "   Finding: Logarithmic relationship (2x revenue ≠ 2x emissions)\n",
    "\n",
    "❌ HYPOTHESIS 6: Concentration Affects Predictability\n",
    "   Status: NOT VALIDATED\n",
    "   Correlation: r=-0.039 (HHI), r=0.051 (num_sectors)\n",
    "   Finding: Concentration doesn't matter, sector choice does\n",
    "\n",
    "✅ HYPOTHESIS 10: Sector Mix Drives Structure\n",
    "   Status: STRONGLY VALIDATED (confirms H1)\n",
    "   Effect Size: +247% avg difference (revenue-matched comparison)\n",
    "   Finding: 2,121x intensity range across sectors\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURES CREATED FROM REVENUE DISTRIBUTION TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "features_summary = {\n",
    "    'Sector Composition (Priority 1 - MUST INCLUDE)': [\n",
    "        'high_emission_pct',\n",
    "        'manufacturing_pct', \n",
    "        'mining_pct',\n",
    "        'is_manufacturing',\n",
    "        'is_mining',\n",
    "        'dominant_nace_l1 (categorical)',\n",
    "        'dominant_nace_l2 (categorical)'\n",
    "    ],\n",
    "    'Revenue Transformations (Priority 1 - MUST INCLUDE)': [\n",
    "        'log_revenue',\n",
    "        'revenue_x_high_emission_pct',\n",
    "        'log_revenue_x_high_emission_pct',\n",
    "        'revenue_x_manufacturing_pct'\n",
    "    ],\n",
    "    'Sector Baselines (Priority 2 - SHOULD INCLUDE)': [\n",
    "        'sector_avg_scope1 (sector mean emissions)',\n",
    "        'sector_avg_scope2 (sector mean emissions)',\n",
    "        'sector_scope1_intensity',\n",
    "        'sector_scope2_intensity'\n",
    "    ],\n",
    "    'Diversity Metrics (Priority 3 - SKIP)': [\n",
    "        'num_l1_sectors (r=0.042 - too weak)',\n",
    "        'num_l2_sectors (r=0.051 - too weak)',\n",
    "        'herfindahl_index (r=-0.039 - no signal)',\n",
    "        'cr1, cr2, cr3 (redundant with HHI)',\n",
    "        'revenue_entropy (weak signal)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, features in features_summary.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for feat in features:\n",
    "        print(f\"  • {feat}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "performance_data = {\n",
    "    'Feature': [\n",
    "        'log_revenue',\n",
    "        'revenue_x_high_emission_pct',\n",
    "        'high_emission_pct',\n",
    "        'manufacturing_pct',\n",
    "        'is_manufacturing',\n",
    "        'is_mining',\n",
    "        'mining_pct',\n",
    "        'num_l2_sectors',\n",
    "        'herfindahl_index'\n",
    "    ],\n",
    "    'Scope_1_Corr': [0.403, 0.311, 0.275, 0.221, 0.214, 0.116, 0.111, 0.051, -0.039],\n",
    "    'Scope_2_Corr': [0.310, 0.229, 0.154, 0.164, 0.159, 0.027, 0.016, 0.015, -0.004],\n",
    "    'Priority': ['P1', 'P1', 'P1', 'P1', 'P1', 'P2', 'P2', 'P3-Skip', 'P3-Skip']\n",
    "}\n",
    "\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "print(\"\\n\" + performance_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nStrength Legend:\")\n",
    "print(\"  ⭐⭐⭐ Strong (|r| > 0.20): Use in all models\")\n",
    "print(\"  ⭐⭐   Moderate (|r| > 0.10): Include if space allows\")\n",
    "print(\"  ⭐     Weak (|r| < 0.10): Skip - adds noise not signal\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS FROM REVENUE DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "insights = \"\"\"\n",
    "1. SECTOR CHOICE IS DOMINANT DRIVER (Hypotheses 1 & 10)\n",
    "   ✓ High-emission sectors emit 2-3x more than low-emission sectors\n",
    "   ✓ Effect is consistent across ALL company sizes (validated in 4 quartiles)\n",
    "   ✓ Mining is 2,121x more intensive than financial services\n",
    "   ✓ Sector features explain ~28% of emission variance alone\n",
    "   \n",
    "2. REVENUE SCALES LOGARITHMICALLY (Hypothesis 2)\n",
    "   ✓ Log transformation improves correlation 2.1x (0.189 → 0.403)\n",
    "   ✓ Doubling revenue increases emissions by ~50%, not 100%\n",
    "   ✓ Interaction features (revenue × sector) outperform raw revenue\n",
    "   ✓ Revenue is a MULTIPLIER on sector baseline, not independent driver\n",
    "   \n",
    "3. DIVERSIFICATION DOESN'T MATTER (Hypothesis 6)\n",
    "   ✗ Concentration (HHI) has near-zero correlation (r=-0.039)\n",
    "   ✗ Number of sectors weakly correlates (r=0.051)\n",
    "   ✗ Specialized vs diversified companies show minimal variance difference\n",
    "   ✓ Sector COMPOSITION matters, not how many sectors\n",
    "   \n",
    "4. DATASET CHARACTERISTICS\n",
    "   • 89% of companies are highly concentrated (focused on 1-2 sectors)\n",
    "   • Manufacturing dominates (40% of companies have manufacturing exposure)\n",
    "   • Most companies are specialists, not diversified conglomerates\n",
    "   • Revenue distribution data has 100% coverage (no missing entities)\n",
    "\n",
    "5. STRUCTURAL FINDINGS\n",
    "   • Sector sets emission \"floor\" - no efficiency can overcome this baseline\n",
    "   • Small companies in high-emission sectors are especially inefficient\n",
    "   • Effect sizes are MASSIVE: +160% to +340% for high-emission sectors\n",
    "\"\"\"\n",
    "\n",
    "print(insights)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDED FINAL FEATURE SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "Based on validation of 4 hypotheses, here are the features to use:\n",
    "\n",
    "TIER 1 - MUST INCLUDE (High Priority):\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "  1. log_revenue                         (r=0.403, strongest revenue feature)\n",
    "  2. revenue_x_high_emission_pct         (r=0.311, best interaction)\n",
    "  3. high_emission_pct                   (r=0.275, strongest sector feature)\n",
    "  4. manufacturing_pct                   (r=0.221, sector-specific)\n",
    "  5. is_manufacturing                    (r=0.214, binary flag)\n",
    "  6. dominant_nace_l1 (categorical)      (use target/frequency encoding)\n",
    "  \n",
    "  Expected R² contribution: ~40-50%\n",
    "\n",
    "TIER 2 - SHOULD INCLUDE (Medium Priority):\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "  7. is_mining                           (r=0.116, important for Scope 1)\n",
    "  8. mining_pct                          (r=0.111, sector-specific)\n",
    "  9. log_revenue_x_high_emission_pct     (test - might improve on #2)\n",
    "  10. revenue_x_manufacturing_pct        (test - sector-specific scaling)\n",
    "  11. sector_avg_scope1/scope2           (sector baseline - target encoding)\n",
    "  \n",
    "  Expected additional R²: ~5-10%\n",
    "\n",
    "TIER 3 - SKIP (Low/No Value):\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "  ✗ num_l1_sectors, num_l2_sectors      (r<0.06 - too weak)\n",
    "  ✗ herfindahl_index                    (r=-0.04 - no signal)\n",
    "  ✗ cr1, cr2, cr3                        (redundant, weak)\n",
    "  ✗ revenue_entropy                      (weak signal)\n",
    "  ✗ dominant_pct                         (r=-0.03 - negative correlation)\n",
    "  \n",
    "  These add complexity without predictive power - EXCLUDE them.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "Revenue Distribution Analysis Summary:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "Hypotheses Tested:           4\n",
    "Hypotheses Validated:         3 (H1, H2, H10)\n",
    "Hypotheses Rejected:          1 (H6)\n",
    "\n",
    "Features Created:            15\n",
    "High-Priority Features:       6 (Tier 1)\n",
    "Medium-Priority Features:     5 (Tier 2)\n",
    "Low-Priority Features:        4 (Tier 3 - skip)\n",
    "\n",
    "Strongest Feature:           log_revenue (r=0.403)\n",
    "Strongest Sector Feature:    high_emission_pct (r=0.275)\n",
    "Best Interaction:            revenue_x_high_emission (r=0.311)\n",
    "\n",
    "Coverage:                    100% (all entities have sector data)\n",
    "Missing Values:              0 (complete data)\n",
    "\n",
    "Expected R² (alone):         0.40-0.50 (Scope 1), 0.30-0.40 (Scope 2)\n",
    "Expected R² (combined):      0.60-0.70 (with all team features)\n",
    "\n",
    "Key Finding:                 Sector choice is THE dominant driver\n",
    "                            (2-3x emission difference, 2,121x intensity range)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68084163-37e4-4744-9abb-d2266ce79d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tier 1 - MUST INCLUDE (6 features)\n",
    "tier1_features = [\n",
    "    'log_revenue',\n",
    "    'revenue_x_high_emission_pct', \n",
    "    'high_emission_pct',\n",
    "    'manufacturing_pct',\n",
    "    'is_manufacturing',\n",
    "    'dominant_nace_l1'\n",
    "]\n",
    "\n",
    "# Tier 2 - SHOULD INCLUDE (8 features)  \n",
    "tier2_features = [\n",
    "    'is_mining',\n",
    "    'mining_pct',\n",
    "    'log_revenue_x_high_emission_pct',\n",
    "    'revenue_x_manufacturing_pct',\n",
    "    'sector_avg_scope1',\n",
    "    'sector_avg_scope2',\n",
    "    'sector_scope1_intensity',\n",
    "    'sector_scope2_intensity'\n",
    "]\n",
    "\n",
    "# Combine all recommended features\n",
    "all_recommended_features = tier1_features + tier2_features\n",
    "\n",
    "# Get available features from your dataset\n",
    "revenue_features_df = [f for f in all_recommended_features if f in analysis_df.columns]\n",
    "\n",
    "print(\"🎯 RECOMMENDED FEATURES:\")\n",
    "print(f\"Tier 1 (Must include): {len(tier1_features)} features\")\n",
    "print(f\"Tier 2 (Should include): {len(tier2_features)} features\") \n",
    "print(f\"Available in your data: {len(revenue_features_df)} features\")\n",
    "print(f\"\\nAvailable features: {revenue_features_df}\")\n",
    "\n",
    "# Create feature matrix\n",
    "final_revenue_features_df = analysis_df[revenue_features_df].copy()\n",
    "\n",
    "print(f\"\\n📊 Final dataset: {final_revenue_features_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d69e7-f2b2-4e0d-8bb0-d2ad251f2dae",
   "metadata": {},
   "source": [
    "## Train EDA & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c0132-bde5-456e-b4e8-a940a20791c3",
   "metadata": {},
   "source": [
    "### Deep EDA on Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d9b0f4-4649-4bb9-99aa-8e89c67c12c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DEEP EXPLORATORY DATA ANALYSIS - TRAIN DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "Goal: Understand the structure, quality, and patterns in train_df\n",
    "      to inform hypothesis-driven feature engineering.\n",
    "      \n",
    "Dataset: train_df (companies with known Scope 1 & Scope 2 emissions)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. DATASET STRUCTURE & COMPLETENESS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📊 Basic Information:\")\n",
    "print(f\"   Rows (companies): {len(train_df):,}\")\n",
    "print(f\"   Columns: {len(train_df.columns)}\")\n",
    "print(f\"   Memory usage: {train_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\n📋 Column Overview:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Column Name':<35} {'Type':<15} {'Unique':<10} {'Missing %':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for col in train_df.columns:\n",
    "    dtype = str(train_df[col].dtype)\n",
    "    unique = train_df[col].nunique()\n",
    "    missing_pct = (train_df[col].isnull().sum() / len(train_df) * 100)\n",
    "    print(f\"{col:<35} {dtype:<15} {unique:<10} {missing_pct:<12.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. TARGET VARIABLES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📊 SCOPE 1 (Direct Emissions) - Detailed Statistics:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "scope1_stats = {\n",
    "    'Count': train_df['target_scope_1'].count(),\n",
    "    'Mean': train_df['target_scope_1'].mean(),\n",
    "    'Std Dev': train_df['target_scope_1'].std(),\n",
    "    'Min': train_df['target_scope_1'].min(),\n",
    "    '25%': train_df['target_scope_1'].quantile(0.25),\n",
    "    'Median': train_df['target_scope_1'].median(),\n",
    "    '75%': train_df['target_scope_1'].quantile(0.75),\n",
    "    '90%': train_df['target_scope_1'].quantile(0.90),\n",
    "    '95%': train_df['target_scope_1'].quantile(0.95),\n",
    "    '99%': train_df['target_scope_1'].quantile(0.99),\n",
    "    'Max': train_df['target_scope_1'].max(),\n",
    "    'Skewness': train_df['target_scope_1'].skew(),\n",
    "    'Kurtosis': train_df['target_scope_1'].kurtosis(),\n",
    "}\n",
    "\n",
    "for stat, value in scope1_stats.items():\n",
    "    if stat in ['Count']:\n",
    "        print(f\"   {stat:<12}: {value:,.0f}\")\n",
    "    elif stat in ['Skewness', 'Kurtosis']:\n",
    "        print(f\"   {stat:<12}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"   {stat:<12}: {value:,.0f}\")\n",
    "\n",
    "print(\"\\n📊 SCOPE 2 (Indirect Emissions) - Detailed Statistics:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "scope2_stats = {\n",
    "    'Count': train_df['target_scope_2'].count(),\n",
    "    'Mean': train_df['target_scope_2'].mean(),\n",
    "    'Std Dev': train_df['target_scope_2'].std(),\n",
    "    'Min': train_df['target_scope_2'].min(),\n",
    "    '25%': train_df['target_scope_2'].quantile(0.25),\n",
    "    'Median': train_df['target_scope_2'].median(),\n",
    "    '75%': train_df['target_scope_2'].quantile(0.75),\n",
    "    '90%': train_df['target_scope_2'].quantile(0.90),\n",
    "    '95%': train_df['target_scope_2'].quantile(0.95),\n",
    "    '99%': train_df['target_scope_2'].quantile(0.99),\n",
    "    'Max': train_df['target_scope_2'].max(),\n",
    "    'Skewness': train_df['target_scope_2'].skew(),\n",
    "    'Kurtosis': train_df['target_scope_2'].kurtosis(),\n",
    "}\n",
    "\n",
    "for stat, value in scope2_stats.items():\n",
    "    if stat in ['Count']:\n",
    "        print(f\"   {stat:<12}: {value:,.0f}\")\n",
    "    elif stat in ['Skewness', 'Kurtosis']:\n",
    "        print(f\"   {stat:<12}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"   {stat:<12}: {value:,.0f}\")\n",
    "\n",
    "# Target correlation\n",
    "scope_correlation = train_df['target_scope_1'].corr(train_df['target_scope_2'])\n",
    "print(f\"\\n📊 Scope 1 vs Scope 2 Correlation: r={scope_correlation:.3f}\")\n",
    "\n",
    "if scope_correlation > 0.5:\n",
    "    print(f\"   ✅ Strong positive correlation - companies high in one tend to be high in both\")\n",
    "elif scope_correlation > 0.3:\n",
    "    print(f\"   ⚠️ Moderate correlation - some shared drivers\")\n",
    "else:\n",
    "    print(f\"   ⚠️ Weak correlation - different drivers for each scope\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Scope 1 - Raw distribution\n",
    "axes[0, 0].hist(train_df['target_scope_1'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0, 0].set_xlabel('Scope 1 Emissions')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title(f'Scope 1 Distribution (Skew={scope1_stats[\"Skewness\"]:.2f})', fontweight='bold')\n",
    "\n",
    "# Scope 1 - Log distribution\n",
    "axes[0, 1].hist(np.log1p(train_df['target_scope_1']), bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0, 1].set_xlabel('log(Scope 1 + 1)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Scope 1 Distribution (Log-transformed)', fontweight='bold')\n",
    "\n",
    "# Scope 1 - Box plot\n",
    "axes[0, 2].boxplot(train_df['target_scope_1'], vert=True)\n",
    "axes[0, 2].set_ylabel('Scope 1 Emissions')\n",
    "axes[0, 2].set_title('Scope 1 Box Plot (Outliers visible)', fontweight='bold')\n",
    "\n",
    "# Scope 2 - Raw distribution\n",
    "axes[1, 0].hist(train_df['target_scope_2'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1, 0].set_xlabel('Scope 2 Emissions')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title(f'Scope 2 Distribution (Skew={scope2_stats[\"Skewness\"]:.2f})', fontweight='bold')\n",
    "\n",
    "# Scope 2 - Log distribution\n",
    "axes[1, 1].hist(np.log1p(train_df['target_scope_2']), bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1, 1].set_xlabel('log(Scope 2 + 1)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Scope 2 Distribution (Log-transformed)', fontweight='bold')\n",
    "\n",
    "# Scope 1 vs Scope 2 scatter\n",
    "axes[1, 2].scatter(train_df['target_scope_1'], train_df['target_scope_2'], alpha=0.5, s=20)\n",
    "axes[1, 2].set_xlabel('Scope 1 Emissions')\n",
    "axes[1, 2].set_ylabel('Scope 2 Emissions')\n",
    "axes[1, 2].set_title(f'Scope 1 vs Scope 2 (r={scope_correlation:.3f})', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 KEY OBSERVATIONS:\")\n",
    "if scope1_stats['Skewness'] > 2:\n",
    "    print(\"   ⚠️ BOTH targets are HEAVILY right-skewed\")\n",
    "    print(\"   → Log transformation is ESSENTIAL for modeling\")\n",
    "    print(\"   → Consider using log(target + 1) as prediction target\")\n",
    "if scope1_stats['Max'] > scope1_stats['Mean'] * 10:\n",
    "    print(\"   ⚠️ Extreme outliers present (max >> mean)\")\n",
    "    print(\"   → Consider outlier treatment or robust models (tree-based)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. REVENUE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📊 Revenue Statistics:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "revenue_stats = train_df['revenue'].describe()\n",
    "print(revenue_stats)\n",
    "\n",
    "print(f\"\\nRevenue range: ${train_df['revenue'].min():,.0f} to ${train_df['revenue'].max():,.0f}\")\n",
    "print(f\"Revenue skewness: {train_df['revenue'].skew():.2f}\")\n",
    "\n",
    "# Revenue correlations\n",
    "revenue_scope1_corr = train_df['revenue'].corr(train_df['target_scope_1'])\n",
    "revenue_scope2_corr = train_df['revenue'].corr(train_df['target_scope_2'])\n",
    "log_revenue_scope1_corr = np.log1p(train_df['revenue']).corr(train_df['target_scope_1'])\n",
    "log_revenue_scope2_corr = np.log1p(train_df['revenue']).corr(train_df['target_scope_2'])\n",
    "\n",
    "print(\"\\n📊 Revenue Correlations:\")\n",
    "print(f\"   Raw revenue vs Scope 1:  r={revenue_scope1_corr:.3f}\")\n",
    "print(f\"   Raw revenue vs Scope 2:  r={revenue_scope2_corr:.3f}\")\n",
    "print(f\"   Log revenue vs Scope 1:  r={log_revenue_scope1_corr:.3f}\")\n",
    "print(f\"   Log revenue vs Scope 2:  r={log_revenue_scope2_corr:.3f}\")\n",
    "\n",
    "if log_revenue_scope1_corr > revenue_scope1_corr * 1.5:\n",
    "    print(f\"   ✅ Log transformation improves correlation by {((log_revenue_scope1_corr - revenue_scope1_corr) / revenue_scope1_corr * 100):.0f}%!\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].hist(train_df['revenue'], bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0].set_xlabel('Revenue')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Revenue Distribution (Raw)', fontweight='bold')\n",
    "\n",
    "axes[1].hist(np.log1p(train_df['revenue']), bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1].set_xlabel('log(Revenue + 1)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Revenue Distribution (Log-transformed)', fontweight='bold')\n",
    "\n",
    "axes[2].scatter(np.log1p(train_df['revenue']), np.log1p(train_df['target_scope_1']), alpha=0.5, s=20)\n",
    "axes[2].set_xlabel('log(Revenue + 1)')\n",
    "axes[2].set_ylabel('log(Scope 1 + 1)')\n",
    "axes[2].set_title(f'Log Revenue vs Log Scope 1 (r={log_revenue_scope1_corr:.3f})', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. CATEGORICAL FEATURES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols = [col for col in categorical_cols if col != 'entity_id']\n",
    "\n",
    "print(f\"\\nCategorical features found: {len(categorical_cols)}\")\n",
    "print(f\"Columns: {categorical_cols}\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n\" + \"-\"*80)\n",
    "    print(f\"{col.upper()}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    n_unique = train_df[col].nunique()\n",
    "    print(f\"Unique values: {n_unique}\")\n",
    "    \n",
    "    if n_unique <= 20:  # Show all if reasonable\n",
    "        value_counts = train_df[col].value_counts()\n",
    "        print(f\"\\nValue distribution:\")\n",
    "        for val, count in value_counts.items():\n",
    "            pct = count / len(train_df) * 100\n",
    "            print(f\"   {str(val)[:40]:40s}: {count:4d} ({pct:5.2f}%)\")\n",
    "    else:  # Show top 10\n",
    "        value_counts = train_df[col].value_counts()\n",
    "        print(f\"\\nTop 10 values:\")\n",
    "        for val, count in value_counts.head(10).items():\n",
    "            pct = count / len(train_df) * 100\n",
    "            print(f\"   {str(val)[:40]:40s}: {count:4d} ({pct:5.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. GEOGRAPHIC DISTRIBUTION DEEP DIVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'country_name' in train_df.columns:\n",
    "    print(\"\\n📊 COUNTRY ANALYSIS:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    n_countries = train_df['country_name'].nunique()\n",
    "    print(f\"Total countries: {n_countries}\")\n",
    "    \n",
    "    country_dist = train_df['country_name'].value_counts()\n",
    "    print(f\"\\nTop 15 countries by company count:\")\n",
    "    for country, count in country_dist.head(15).items():\n",
    "        pct = count / len(train_df) * 100\n",
    "        print(f\"   {country[:40]:40s}: {count:4d} ({pct:5.2f}%)\")\n",
    "    \n",
    "    # Countries with few companies\n",
    "    small_countries = country_dist[country_dist < 3]\n",
    "    print(f\"\\n⚠️ Countries with <3 companies: {len(small_countries)}\")\n",
    "    print(f\"   These may not have reliable statistics for target encoding\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Top 15 countries\n",
    "    top_15 = country_dist.head(15)\n",
    "    axes[0].barh(range(len(top_15)), top_15.values, color='steelblue', edgecolor='black')\n",
    "    axes[0].set_yticks(range(len(top_15)))\n",
    "    axes[0].set_yticklabels(top_15.index)\n",
    "    axes[0].set_xlabel('Number of Companies')\n",
    "    axes[0].set_title('Top 15 Countries by Company Count', fontweight='bold')\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    # Sample size distribution\n",
    "    axes[1].hist(country_dist.values, bins=30, edgecolor='black', alpha=0.7, color='coral')\n",
    "    axes[1].set_xlabel('Number of Companies per Country')\n",
    "    axes[1].set_ylabel('Number of Countries')\n",
    "    axes[1].set_title('Distribution of Sample Sizes by Country', fontweight='bold')\n",
    "    axes[1].axvline(3, color='red', linestyle='--', label='Min for reliable stats')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if 'region_name' in train_df.columns:\n",
    "    print(\"\\n📊 REGION ANALYSIS:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    n_regions = train_df['region_name'].nunique()\n",
    "    print(f\"Total regions: {n_regions}\")\n",
    "    \n",
    "    region_dist = train_df['region_name'].value_counts()\n",
    "    print(f\"\\nRegion distribution:\")\n",
    "    for region, count in region_dist.items():\n",
    "        pct = count / len(train_df) * 100\n",
    "        print(f\"   {region[:40]:40s}: {count:4d} ({pct:5.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. ESG SCORES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "score_cols = [col for col in train_df.columns if 'score' in col.lower()]\n",
    "\n",
    "if len(score_cols) > 0:\n",
    "    print(f\"\\nESG Score columns found: {len(score_cols)}\")\n",
    "    print(f\"Columns: {score_cols}\")\n",
    "    \n",
    "    for col in score_cols:\n",
    "        print(f\"\\n\" + \"-\"*80)\n",
    "        print(f\"{col.upper()}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        print(train_df[col].describe())\n",
    "        \n",
    "        # Correlation with targets\n",
    "        scope1_corr = train_df[col].corr(train_df['target_scope_1'])\n",
    "        scope2_corr = train_df[col].corr(train_df['target_scope_2'])\n",
    "        \n",
    "        print(f\"\\nCorrelation with targets:\")\n",
    "        print(f\"   Scope 1: r={scope1_corr:6.3f}\")\n",
    "        print(f\"   Scope 2: r={scope2_corr:6.3f}\")\n",
    "    \n",
    "    # Visualize score distributions\n",
    "    fig, axes = plt.subplots(1, len(score_cols), figsize=(6*len(score_cols), 5))\n",
    "    \n",
    "    if len(score_cols) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, col in enumerate(score_cols):\n",
    "        axes[idx].hist(train_df[col].dropna(), bins=20, edgecolor='black', alpha=0.7, color='purple')\n",
    "        axes[idx].set_xlabel(col)\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        axes[idx].set_title(f'{col} Distribution', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Score correlation matrix\n",
    "    if len(score_cols) > 1:\n",
    "        print(\"\\n📊 Score Correlation Matrix:\")\n",
    "        print(\"-\" * 80)\n",
    "        score_corr_matrix = train_df[score_cols].corr()\n",
    "        print(score_corr_matrix.round(3))\n",
    "        \n",
    "        # Heatmap\n",
    "        import seaborn as sns\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(score_corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                   square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "        plt.title('ESG Scores Correlation Matrix', fontweight='bold', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ No ESG score columns found in train_df\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📊 Missing Values Summary:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "missing_summary = train_df.isnull().sum()\n",
    "missing_pct = (missing_summary / len(train_df) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_summary,\n",
    "    'Missing_Pct': missing_pct\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "if missing_df['Missing_Count'].sum() == 0:\n",
    "    print(\"✅ NO MISSING VALUES - Excellent data quality!\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ {(missing_df['Missing_Count'] > 0).sum()} columns have missing values\")\n",
    "    print(f\"   Consider imputation or feature engineering strategies\")\n",
    "\n",
    "# Check for duplicates\n",
    "n_duplicates = train_df.duplicated(subset='entity_id').sum()\n",
    "print(f\"\\n📊 Duplicate entity_ids: {n_duplicates}\")\n",
    "\n",
    "if n_duplicates == 0:\n",
    "    print(\"✅ No duplicates - each company appears once\")\n",
    "else:\n",
    "    print(f\"⚠️ {n_duplicates} duplicate records found - investigate!\")\n",
    "\n",
    "# Check for zeros in targets\n",
    "scope1_zeros = (train_df['target_scope_1'] == 0).sum()\n",
    "scope2_zeros = (train_df['target_scope_2'] == 0).sum()\n",
    "\n",
    "print(f\"\\n📊 Zero emissions:\")\n",
    "print(f\"   Scope 1 zeros: {scope1_zeros} ({scope1_zeros/len(train_df)*100:.2f}%)\")\n",
    "print(f\"   Scope 2 zeros: {scope2_zeros} ({scope2_zeros/len(train_df)*100:.2f}%)\")\n",
    "\n",
    "if scope1_zeros > 0 or scope2_zeros > 0:\n",
    "    print(f\"   ⚠️ Zero emissions may be:\")\n",
    "    print(f\"      - True zeros (no direct emissions)\")\n",
    "    print(f\"      - Missing data coded as zero\")\n",
    "    print(f\"      - Small companies with negligible emissions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EDA SUMMARY & KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "DATASET CHARACTERISTICS:\n",
    "  • Size: {len(train_df):,} companies\n",
    "  • Features: {len(train_df.columns)} columns\n",
    "  • Completeness: {100 - (train_df.isnull().sum().sum() / (len(train_df) * len(train_df.columns)) * 100):.1f}% non-null\n",
    "  • Geographic coverage: {train_df['country_name'].nunique() if 'country_name' in train_df.columns else 'N/A'} countries, {train_df['region_name'].nunique() if 'region_name' in train_df.columns else 'N/A'} regions\n",
    "\n",
    "TARGET VARIABLES:\n",
    "  • Scope 1: Mean={scope1_stats['Mean']:,.0f}, Median={scope1_stats['Median']:,.0f}, Skew={scope1_stats['Skewness']:.2f}\n",
    "  • Scope 2: Mean={scope2_stats['Mean']:,.0f}, Median={scope2_stats['Median']:,.0f}, Skew={scope2_stats['Skewness']:.2f}\n",
    "  • Correlation: r={scope_correlation:.3f} ({\"Strong\" if scope_correlation > 0.5 else \"Moderate\" if scope_correlation > 0.3 else \"Weak\"})\n",
    "  • Distribution: {\"HEAVILY\" if scope1_stats['Skewness'] > 2 else \"Moderately\"} right-skewed → Log transformation REQUIRED\n",
    "\n",
    "REVENUE:\n",
    "  • Range: ${train_df['revenue'].min():,.0f} to ${train_df['revenue'].max():,.0f}\n",
    "  • Correlation with Scope 1: r={revenue_scope1_corr:.3f} (raw) vs r={log_revenue_scope1_corr:.3f} (log)\n",
    "  • Log transformation improves correlation by {((log_revenue_scope1_corr - revenue_scope1_corr) / revenue_scope1_corr * 100):.0f}%\n",
    "\n",
    "CATEGORICAL FEATURES:\n",
    "  • {len(categorical_cols)} categorical columns\n",
    "  • Country: {train_df['country_name'].nunique() if 'country_name' in train_df.columns else 'N/A'} unique values\n",
    "  • Region: {train_df['region_name'].nunique() if 'region_name' in train_df.columns else 'N/A'} unique values\n",
    "\n",
    "ESG SCORES:\n",
    "  • {len(score_cols)} score columns available\n",
    "  • {f\"Strongest correlation: {max([abs(train_df[col].corr(train_df['target_scope_1'])) for col in score_cols]):.3f}\" if score_cols else \"Not available\"}\n",
    "\n",
    "DATA QUALITY:\n",
    "  • Missing values: {missing_df['Missing_Count'].sum()} total ({(missing_df['Missing_Count'].sum() / (len(train_df) * len(train_df.columns)) * 100):.2f}%)\n",
    "  • Duplicates: {n_duplicates}\n",
    "  • Zero emissions: {scope1_zeros} Scope 1, {scope2_zeros} Scope 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d46ca-a46d-41ee-9144-049cb7cf036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "\n",
    "key_insights = \"\"\"\n",
    "🎯 CRITICAL FINDINGS:\n",
    "\n",
    "1. TARGET DISTRIBUTION - EXTREMELY SKEWED\n",
    "   • Scope 1 skewness: 3.15 (HEAVY right skew)\n",
    "   • Scope 2 skewness: 7.34 (EXTREME right skew!)\n",
    "   • Scope 2 kurtosis: 65.09 (massive outliers)\n",
    "   → Log transformation is MANDATORY for modeling\n",
    "   \n",
    "2. SCOPE 1 vs SCOPE 2 RELATIONSHIP\n",
    "   • Correlation: r=0.526 (STRONG positive)\n",
    "   • Companies high in one tend to be high in both\n",
    "   → Some shared drivers, but also independent factors\n",
    "   → Validates separate analysis for each target\n",
    "   \n",
    "3. REVENUE INSIGHTS\n",
    "   • Log transformation improves correlation by 46%!\n",
    "   • Raw revenue: r=0.189 vs Log revenue: r=0.275\n",
    "   → Already validated in revenue distribution analysis\n",
    "   \n",
    "4. GEOGRAPHIC CONCENTRATION\n",
    "   • 98% of companies in Western Europe (63%) + North America (36%)\n",
    "   • Only 28 countries, 7 regions\n",
    "   • 12 countries with <3 companies (unreliable for target encoding)\n",
    "   → Limited geographic diversity\n",
    "   → US-centric dataset (34% of companies)\n",
    "   \n",
    "5. ESG SCORES - WEAK PREDICTORS\n",
    "   • Environmental score: r=0.120 (Scope 1) - WEAK\n",
    "   • Governance score: r=-0.113 (Scope 1) - WEAK NEGATIVE\n",
    "   • Overall score: r=0.072 (Scope 1) - VERY WEAK\n",
    "   → ESG scores are NOT strong emission predictors\n",
    "   → But may still add marginal value\n",
    "   \n",
    "6. ESG SCORE CORRELATIONS (Among themselves)\n",
    "   • Overall-Environmental: 0.825 (highly correlated)\n",
    "   • Environmental-Governance: 0.032 (independent!)\n",
    "   • Social-Governance: -0.012 (independent!)\n",
    "   → Environmental score captures most ESG info\n",
    "   → Governance is orthogonal (different dimension)\n",
    "   \n",
    "7. DATA QUALITY - EXCELLENT\n",
    "   • 0 missing values ✅\n",
    "   • 0 duplicates ✅\n",
    "   • Only 13 Scope 2 zeros (3%) - legitimate small emitters\n",
    "   \n",
    "8. OUTLIERS\n",
    "   • Scope 2 max (2M) is 35x the mean (57K)\n",
    "   • Scope 1 max (638K) is 11x the mean (56K)\n",
    "   → Tree-based models recommended (robust to outliers)\n",
    "\"\"\"\n",
    "\n",
    "print(key_insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c746b4b9-1cd1-441c-acab-2077efb86e7b",
   "metadata": {},
   "source": [
    "### Hypothesis-Driven Feature Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b322aad-cfde-4783-8647-0ce37961a88f",
   "metadata": {},
   "source": [
    "#### Hypothesis 3: Geography Matters for Scope 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dd1e90-42a9-412a-93c7-5608f6a809a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "PREDICTION: Grid carbon intensity varies by country → Scope 2 varies more by \n",
    "            geography than Scope 1 (which is driven by sector/operations)\n",
    "            \n",
    "TEST: Compare country/region effect sizes for Scope 1 vs Scope 2\n",
    "      (Exploratory analysis only - for understanding, not feature creation)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 1: EMISSIONS BY COUNTRY\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Calculate country-level statistics (for analysis only)\n",
    "country_emissions = train_df.groupby('country_name').agg({\n",
    "    'target_scope_1': ['mean', 'median', 'std', 'count'],\n",
    "    'target_scope_2': ['mean', 'median', 'std'],\n",
    "    'revenue': 'mean'\n",
    "}).round(0)\n",
    "\n",
    "country_emissions.columns = ['Scope1_Mean', 'Scope1_Median', 'Scope1_Std', 'Count', \n",
    "                             'Scope2_Mean', 'Scope2_Median', 'Scope2_Std', 'Revenue_Mean']\n",
    "\n",
    "# Filter countries with at least 3 companies for reliable statistics\n",
    "country_emissions_reliable = country_emissions[country_emissions['Count'] >= 3].copy()\n",
    "\n",
    "# Calculate emission intensities\n",
    "country_emissions_reliable['Scope1_Intensity'] = (\n",
    "    country_emissions_reliable['Scope1_Mean'] / (country_emissions_reliable['Revenue_Mean'] + 1)\n",
    ")\n",
    "country_emissions_reliable['Scope2_Intensity'] = (\n",
    "    country_emissions_reliable['Scope2_Mean'] / (country_emissions_reliable['Revenue_Mean'] + 1)\n",
    ")\n",
    "country_emissions_reliable['Scope2_to_Scope1_Ratio'] = (\n",
    "    country_emissions_reliable['Scope2_Mean'] / (country_emissions_reliable['Scope1_Mean'] + 1)\n",
    ")\n",
    "\n",
    "# Calculate coefficient of variation (CV) across countries\n",
    "scope1_country_cv = country_emissions_reliable['Scope1_Mean'].std() / country_emissions_reliable['Scope1_Mean'].mean()\n",
    "scope2_country_cv = country_emissions_reliable['Scope2_Mean'].std() / country_emissions_reliable['Scope2_Mean'].mean()\n",
    "\n",
    "print(f\"\\n📊 Cross-Country Variation (Coefficient of Variation):\")\n",
    "print(f\"   Scope 1 CV across countries: {scope1_country_cv:.3f}\")\n",
    "print(f\"   Scope 2 CV across countries: {scope2_country_cv:.3f}\")\n",
    "\n",
    "if scope2_country_cv > scope1_country_cv:\n",
    "    print(f\"   ✅ Scope 2 varies MORE by country ({((scope2_country_cv - scope1_country_cv) / scope1_country_cv * 100):.0f}% higher variation)\")\n",
    "    print(f\"   → Geography matters MORE for Scope 2 (grid intensity effect)\")\n",
    "else:\n",
    "    print(f\"   ❌ Scope 1 varies more by country\")\n",
    "\n",
    "print(f\"\\n📊 Top 10 Countries by Scope 2 Emissions:\")\n",
    "print(\"-\" * 80)\n",
    "top_scope2 = country_emissions_reliable.sort_values('Scope2_Mean', ascending=False).head(10)\n",
    "print(top_scope2[['Scope2_Mean', 'Scope1_Mean', 'Scope2_to_Scope1_Ratio', 'Count']])\n",
    "\n",
    "print(f\"\\n📊 Top 10 Countries by Electricity Intensity (Scope 2 / Scope 1 ratio):\")\n",
    "print(\"-\" * 80)\n",
    "print(\"(Higher ratio = more electricity-dependent or dirtier grid)\")\n",
    "top_ratio = country_emissions_reliable.sort_values('Scope2_to_Scope1_Ratio', ascending=False).head(10)\n",
    "print(top_ratio[['Scope2_to_Scope1_Ratio', 'Scope2_Mean', 'Scope1_Mean', 'Count']])\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 2: EMISSIONS BY REGION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "region_emissions = train_df.groupby('region_name').agg({\n",
    "    'target_scope_1': ['mean', 'median', 'std', 'count'],\n",
    "    'target_scope_2': ['mean', 'median', 'std'],\n",
    "    'revenue': 'mean'\n",
    "}).round(0)\n",
    "\n",
    "region_emissions.columns = ['Scope1_Mean', 'Scope1_Median', 'Scope1_Std', 'Count',\n",
    "                            'Scope2_Mean', 'Scope2_Median', 'Scope2_Std', 'Revenue_Mean']\n",
    "\n",
    "region_emissions['Scope2_to_Scope1_Ratio'] = (\n",
    "    region_emissions['Scope2_Mean'] / (region_emissions['Scope1_Mean'] + 1)\n",
    ")\n",
    "\n",
    "print(region_emissions.sort_values('Scope2_Mean', ascending=False))\n",
    "\n",
    "# Regional variation\n",
    "scope1_region_cv = region_emissions['Scope1_Mean'].std() / region_emissions['Scope1_Mean'].mean()\n",
    "scope2_region_cv = region_emissions['Scope2_Mean'].std() / region_emissions['Scope2_Mean'].mean()\n",
    "\n",
    "print(f\"\\n📊 Cross-Region Variation:\")\n",
    "print(f\"   Scope 1 CV across regions: {scope1_region_cv:.3f}\")\n",
    "print(f\"   Scope 2 CV across regions: {scope2_region_cv:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 3: STATISTICAL SIGNIFICANCE (ANOVA)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Test if country significantly affects emissions\n",
    "country_groups_scope1 = []\n",
    "country_groups_scope2 = []\n",
    "countries_tested = []\n",
    "\n",
    "for country, group in train_df.groupby('country_name'):\n",
    "    if len(group) >= 5:\n",
    "        country_groups_scope1.append(group['target_scope_1'].values)\n",
    "        country_groups_scope2.append(group['target_scope_2'].values)\n",
    "        countries_tested.append(country)\n",
    "\n",
    "if len(country_groups_scope1) >= 3:\n",
    "    # ANOVA for Scope 1\n",
    "    f_stat_s1, p_val_s1 = stats.f_oneway(*country_groups_scope1)\n",
    "    \n",
    "    # ANOVA for Scope 2\n",
    "    f_stat_s2, p_val_s2 = stats.f_oneway(*country_groups_scope2)\n",
    "    \n",
    "    print(f\"\\n📊 ANOVA Test: Does country affect emissions?\")\n",
    "    print(f\"   Countries tested: {len(countries_tested)} ({', '.join(countries_tested[:5])}...)\")\n",
    "    \n",
    "    print(f\"\\n   SCOPE 1:\")\n",
    "    print(f\"     F-statistic: {f_stat_s1:.2f}\")\n",
    "    print(f\"     P-value: {p_val_s1:.6f}\")\n",
    "    if p_val_s1 < 0.001:\n",
    "        print(f\"     ✅ HIGHLY SIGNIFICANT (p < 0.001)\")\n",
    "    elif p_val_s1 < 0.05:\n",
    "        print(f\"     ✅ SIGNIFICANT (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"     ❌ Not significant\")\n",
    "    \n",
    "    print(f\"\\n   SCOPE 2:\")\n",
    "    print(f\"     F-statistic: {f_stat_s2:.2f}\")\n",
    "    print(f\"     P-value: {p_val_s2:.6f}\")\n",
    "    if p_val_s2 < 0.001:\n",
    "        print(f\"     ✅ HIGHLY SIGNIFICANT (p < 0.001)\")\n",
    "    elif p_val_s2 < 0.05:\n",
    "        print(f\"     ✅ SIGNIFICANT (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"     ❌ Not significant\")\n",
    "    \n",
    "    # Compare F-statistics\n",
    "    print(f\"\\n   💡 COMPARISON:\")\n",
    "    if f_stat_s2 > f_stat_s1:\n",
    "        print(f\"     Scope 2 F-statistic is {((f_stat_s2 - f_stat_s1) / f_stat_s1 * 100):.0f}% HIGHER\")\n",
    "        print(f\"     → Country matters MORE for Scope 2!\")\n",
    "    else:\n",
    "        print(f\"     Scope 1 F-statistic is higher\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 4: VALID FEATURE ENCODING APPROACHES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Create VALID features that can be used on test set\n",
    "\n",
    "# 1. Frequency Encoding (Valid - based on count, not emissions)\n",
    "country_freq = train_df['country_name'].value_counts()\n",
    "region_freq = train_df['region_name'].value_counts()\n",
    "\n",
    "train_df['country_frequency'] = train_df['country_name'].map(country_freq)\n",
    "train_df['region_frequency'] = train_df['region_name'].map(region_freq)\n",
    "\n",
    "# Correlations with frequency encoding\n",
    "freq_country_scope1_corr = train_df['country_frequency'].corr(train_df['target_scope_1'])\n",
    "freq_country_scope2_corr = train_df['country_frequency'].corr(train_df['target_scope_2'])\n",
    "freq_region_scope1_corr = train_df['region_frequency'].corr(train_df['target_scope_1'])\n",
    "freq_region_scope2_corr = train_df['region_frequency'].corr(train_df['target_scope_2'])\n",
    "\n",
    "print(f\"\\n📊 Geographic Feature Correlations (Frequency Encoding - VALID):\")\n",
    "print(f\"   country_frequency vs Scope 1: r={freq_country_scope1_corr:.3f}\")\n",
    "print(f\"   country_frequency vs Scope 2: r={freq_country_scope2_corr:.3f}\")\n",
    "print(f\"   region_frequency vs Scope 1:  r={freq_region_scope1_corr:.3f}\")\n",
    "print(f\"   region_frequency vs Scope 2:  r={freq_region_scope2_corr:.3f}\")\n",
    "\n",
    "# 2. Label Encoding (for comparison)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_country = LabelEncoder()\n",
    "le_region = LabelEncoder()\n",
    "\n",
    "train_df['country_encoded'] = le_country.fit_transform(train_df['country_name'])\n",
    "train_df['region_encoded'] = le_region.fit_transform(train_df['region_name'])\n",
    "\n",
    "country_scope1_corr = train_df['country_encoded'].corr(train_df['target_scope_1'])\n",
    "country_scope2_corr = train_df['country_encoded'].corr(train_df['target_scope_2'])\n",
    "region_scope1_corr = train_df['region_encoded'].corr(train_df['target_scope_1'])\n",
    "region_scope2_corr = train_df['region_encoded'].corr(train_df['target_scope_2'])\n",
    "\n",
    "print(f\"\\n📊 Geographic Feature Correlations (Label Encoding - VALID but weak):\")\n",
    "print(f\"   country_encoded vs Scope 1: r={country_scope1_corr:.3f}\")\n",
    "print(f\"   country_encoded vs Scope 2: r={country_scope2_corr:.3f}\")\n",
    "print(f\"   region_encoded vs Scope 1:  r={region_scope1_corr:.3f}\")\n",
    "print(f\"   region_encoded vs Scope 2:  r={region_scope2_corr:.3f}\")\n",
    "\n",
    "print(f\"\\n💡 CONCLUSION:\")\n",
    "print(f\"   Frequency encoding is better than label encoding\")\n",
    "print(f\"   But both are WEAK compared to sector/revenue features\")\n",
    "print(f\"   → Geographic features have limited predictive power with valid encoding\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Top countries by Scope 2\n",
    "top_10_scope2 = country_emissions_reliable.sort_values('Scope2_Mean', ascending=False).head(10)\n",
    "axes[0, 0].barh(range(len(top_10_scope2)), top_10_scope2['Scope2_Mean'], color='orange', edgecolor='black')\n",
    "axes[0, 0].set_yticks(range(len(top_10_scope2)))\n",
    "axes[0, 0].set_yticklabels([c[:30] for c in top_10_scope2.index])\n",
    "axes[0, 0].set_xlabel('Average Scope 2 Emissions')\n",
    "axes[0, 0].set_title('Top 10 Countries by Scope 2\\n(Exploratory Analysis)', fontweight='bold')\n",
    "axes[0, 0].invert_yaxis()\n",
    "\n",
    "# Scope 2/Scope 1 ratio\n",
    "top_10_ratio = country_emissions_reliable.sort_values('Scope2_to_Scope1_Ratio', ascending=False).head(10)\n",
    "axes[0, 1].barh(range(len(top_10_ratio)), top_10_ratio['Scope2_to_Scope1_Ratio'], color='teal', edgecolor='black')\n",
    "axes[0, 1].set_yticks(range(len(top_10_ratio)))\n",
    "axes[0, 1].set_yticklabels([c[:30] for c in top_10_ratio.index])\n",
    "axes[0, 1].set_xlabel('Scope 2 / Scope 1 Ratio')\n",
    "axes[0, 1].set_title('Top 10 Countries by Electricity Intensity', fontweight='bold')\n",
    "axes[0, 1].invert_yaxis()\n",
    "\n",
    "# Regional comparison\n",
    "regions = region_emissions.index\n",
    "x_pos = np.arange(len(regions))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 0].bar(x_pos - width/2, region_emissions['Scope1_Mean'], width, \n",
    "              label='Scope 1', color='steelblue', edgecolor='black')\n",
    "axes[1, 0].bar(x_pos + width/2, region_emissions['Scope2_Mean'], width,\n",
    "              label='Scope 2', color='orange', edgecolor='black')\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels([r[:15] for r in regions], rotation=45, ha='right')\n",
    "axes[1, 0].set_ylabel('Average Emissions')\n",
    "axes[1, 0].set_title('Emissions by Region (Scope 1 vs Scope 2)', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Valid encoding comparison\n",
    "encoding_methods = ['Frequency\\nEncoding', 'Label\\nEncoding']\n",
    "scope1_corrs = [freq_country_scope1_corr, country_scope1_corr]\n",
    "scope2_corrs = [freq_country_scope2_corr, country_scope2_corr]\n",
    "\n",
    "x = np.arange(len(encoding_methods))\n",
    "axes[1, 1].bar(x - width/2, scope1_corrs, width, label='Scope 1', \n",
    "              color='steelblue', edgecolor='black')\n",
    "axes[1, 1].bar(x + width/2, scope2_corrs, width, label='Scope 2',\n",
    "              color='orange', edgecolor='black')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(encoding_methods)\n",
    "axes[1, 1].set_ylabel('Correlation Coefficient')\n",
    "axes[1, 1].set_title('Country Encoding Performance\\n(Valid Methods Only)', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURES CREATED (VALID FOR TEST SET)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "✅ country_frequency (count-based, r={:.3f} for Scope 1)\n",
    "✅ region_frequency (count-based, r={:.3f} for Scope 1)\n",
    "✅ country_name (categorical - can use one-hot encoding)\n",
    "✅ region_name (categorical - can use one-hot encoding)\n",
    "\n",
    "⚠️ country_encoded, region_encoded (label encoding - weak, r<0.1)\n",
    "\n",
    "Note: Frequency encoding works better than label encoding\n",
    "      but both are weak compared to sector/revenue features\n",
    "\"\"\".format(freq_country_scope1_corr, freq_region_scope1_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d9970-76af-417f-826d-9e31f1f0ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS 3: GEOGRAPHY MATTERS FOR SCOPE 2 - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Key metrics from results\n",
    "scope2_country_cv = 1.770\n",
    "scope1_country_cv = 0.977\n",
    "cv_difference_pct = ((scope2_country_cv - scope1_country_cv) / scope1_country_cv * 100)\n",
    "\n",
    "f_stat_s1 = 2.70\n",
    "f_stat_s2 = 1.44\n",
    "p_val_s1 = 0.001609\n",
    "p_val_s2 = 0.145268\n",
    "\n",
    "freq_country_corr_s1 = 0.118\n",
    "freq_country_corr_s2 = 0.113\n",
    "freq_region_corr_s1 = -0.142\n",
    "freq_region_corr_s2 = -0.153\n",
    "\n",
    "label_country_corr = 0.082\n",
    "label_region_corr = -0.106\n",
    "\n",
    "# Determine validation status\n",
    "status = \"⚠️ PARTIALLY VALIDATED (WEAK FEATURES)\"\n",
    "\n",
    "summary = f\"\"\"\n",
    "\n",
    "HYPOTHESIS 3: GEOGRAPHY MATTERS FOR SCOPE 2 {status}\n",
    "\n",
    "KEY FINDINGS:\n",
    "1. Scope 2 DOES vary more by country (exploratory evidence):\n",
    "   - Scope 2 CV: {scope2_country_cv:.3f} (81% HIGHER than Scope 1)\n",
    "   - Scope 1 CV: {scope1_country_cv:.3f}\n",
    "   → Cross-country variation supports hypothesis\n",
    "   \n",
    "2. BUT statistical significance is MIXED:\n",
    "   - Scope 1 ANOVA: F={f_stat_s1:.2f}, p={p_val_s1:.6f} ✅ SIGNIFICANT\n",
    "   - Scope 2 ANOVA: F={f_stat_s2:.2f}, p={p_val_s2:.6f} ❌ NOT SIGNIFICANT\n",
    "   → Paradox: Scope 2 varies MORE but not statistically significant\n",
    "   \n",
    "3. Valid encoding methods show WEAK predictive power:\n",
    "   - Frequency encoding (country): r={freq_country_corr_s1:.3f} (Scope 1)\n",
    "   - Label encoding (country): r={label_country_corr:.3f} (Scope 1)\n",
    "   - Region frequency: r={freq_region_corr_s1:.3f} (Scope 1, NEGATIVE!)\n",
    "   → Geographic features are WEAK predictors with valid methods\n",
    "   \n",
    "4. Electricity intensity varies dramatically by country:\n",
    "   - Norway: Scope 2/Scope 1 ratio = 3.13 (very electricity-intensive)\n",
    "   - USA: Scope 2/Scope 1 ratio = 1.18\n",
    "   - Switzerland: Scope 2/Scope 1 ratio = 0.67 (low electricity)\n",
    "   → Real grid intensity differences exist\n",
    "   \n",
    "5. Dataset limitations reduce geographic value:\n",
    "   - 98% of companies in Western Europe (63%) + North America (36%)\n",
    "   - Only 28 countries, 12 have n<3 (unreliable)\n",
    "   - Norway is massive outlier (n=3, distorts statistics)\n",
    "   → Limited geographic diversity\n",
    "\n",
    "INTERPRETATION:\n",
    "The hypothesis is PARTIALLY correct with important caveats:\n",
    "\n",
    "✅ EXPLORATORY EVIDENCE SUPPORTS:\n",
    "   - Scope 2 DOES vary more across countries (81% higher CV)\n",
    "   - Grid intensity differences are real (3x range in Scope2/Scope1 ratios)\n",
    "   - Norway, USA show high electricity dependence\n",
    "   \n",
    "❌ BUT PRACTICAL FEATURES ARE WEAK:\n",
    "   - Best valid encoding: frequency (r={freq_country_corr_s1:.3f})\n",
    "   - This is MUCH weaker than sector (r=0.275) or revenue (r=0.403)\n",
    "   - Not statistically significant for Scope 2 (p={p_val_s2:.3f})\n",
    "   \n",
    "💡 WHY THE DISCONNECT?\n",
    "   1. Valid encoding methods (frequency, label) can't capture the nuanced\n",
    "      geographic effects that target encoding revealed\n",
    "   2. Small sample sizes for most countries (12/28 have n<3)\n",
    "   3. High within-country variance masks between-country differences\n",
    "   4. Dataset concentrated in similar regions (Western developed countries)\n",
    "\n",
    "VALIDATION OF SPECIFIC PREDICTIONS:\n",
    "  ⚠️ \"Grid carbon intensity varies by country\"\n",
    "     → CONFIRMED in exploratory analysis (3x ratio range)\n",
    "     → BUT can't effectively use this in predictive features\n",
    "     \n",
    "  ⚠️ \"Scope 2 varies more by geography than Scope 1\"\n",
    "     → CONFIRMED by CV (81% higher variation)\n",
    "     → BUT NOT confirmed by ANOVA (p={p_val_s2:.3f} not significant)\n",
    "     → Frequency encoding shows SIMILAR correlation for both scopes\n",
    "     \n",
    "  ❌ \"Geography is a strong predictor of emissions\"\n",
    "     → REJECTED: r={freq_country_corr_s1:.3f} is weak\n",
    "     → Much weaker than sector/revenue features\n",
    "\n",
    "RECOMMENDED FEATURES FOR FINAL MODEL:\n",
    "  Priority 3 (OPTIONAL - weak signal):\n",
    "    ⚠️ country_frequency (r={freq_country_corr_s1:.3f}) - count-based encoding\n",
    "    ⚠️ region_name (categorical - only 7 values, use one-hot)\n",
    "    ❌ region_frequency (r={freq_region_corr_s1:.3f}) - NEGATIVE correlation!\n",
    "    ❌ country_encoded, region_encoded (label encoding too weak)\n",
    "    \n",
    "  Alternative approach:\n",
    "    ⚠️ country_name with one-hot encoding (28 columns)\n",
    "    ⚠️ Group small countries into top 10 + \"Other\" (11 columns)\n",
    "    \n",
    "  Expected R² contribution: ~1-2% (very marginal)\n",
    "\n",
    "MODELING IMPLICATIONS:\n",
    "  1. Geographic features add MINIMAL value:\n",
    "     - Expected R² contribution: ~1.4% (0.118² = 0.014)\n",
    "     - Sector + Revenue already explain ~45-50%\n",
    "     - Not worth complexity for 1-2% improvement\n",
    "     \n",
    "  2. One-hot encoding regions might work better:\n",
    "     - Only 7 categories (manageable)\n",
    "     - No arbitrary ordering like label encoding\n",
    "     - Model can learn region-specific patterns\n",
    "     \n",
    "  3. Country encoding is problematic:\n",
    "     - 28 categories → 28 columns if one-hot\n",
    "     - Many countries have small samples (n<3)\n",
    "     - Frequency encoding only r={freq_country_corr_s1:.3f}\n",
    "     \n",
    "  4. Consider SKIPPING geographic features entirely:\n",
    "     - Focus on stronger predictors (sector, revenue)\n",
    "     - Add geography only if model capacity allows\n",
    "     - Not in minimal viable model\n",
    "\n",
    "VISUAL EVIDENCE:\n",
    "  - Norway dramatically outperforms all others (outlier effect)\n",
    "  - Scope 2/Scope 1 ratios show 3x range (grid differences are real)\n",
    "  - Regional bars show North America has higher Scope 2 than Europe\n",
    "  - Frequency encoding slightly better than label encoding\n",
    "  - But both correlations are very weak (r<0.12)\n",
    "\n",
    "UNEXPECTED FINDINGS:\n",
    "  1. **Region frequency has NEGATIVE correlation**:\n",
    "     - r={freq_region_corr_s1:.3f} (Scope 1)\n",
    "     - Larger regions (more companies) → LOWER emissions?\n",
    "     - Likely confounded: Western Europe is large but low-emission\n",
    "     - North America is large but moderate-emission\n",
    "     \n",
    "  2. **Frequency encoding barely better than label**:\n",
    "     - Frequency: r={freq_country_corr_s1:.3f}\n",
    "     - Label: r={label_country_corr:.3f}\n",
    "     - Both very weak, minimal difference\n",
    "     \n",
    "  3. **Scope 2 ANOVA not significant despite higher variation**:\n",
    "     - Higher CV but p={p_val_s2:.3f}\n",
    "     - High within-country variance\n",
    "     - Small sample sizes reduce power\n",
    "     \n",
    "  4. **Norway massively inflates Scope 2 statistics**:\n",
    "     - Norway Scope 2: 503,019 (5.6x USA average!)\n",
    "     - Only 3 companies in Norway\n",
    "     - Removing Norway would likely reduce CV significantly\n",
    "\n",
    "CRITICAL LIMITATIONS:\n",
    "  1. **Geographic concentration**: 98% Western Europe + North America\n",
    "  2. **Small samples**: 12/28 countries have n<3\n",
    "  3. **Norway outlier**: n=3, extreme Scope 2 (503K)\n",
    "  4. **Limited grid diversity**: Mostly developed countries\n",
    "  5. **Valid encoding weakness**: Can't use target encoding on test set\n",
    "  6. **High within-country variance**: Masks geographic effects\n",
    "\n",
    "REVISED HYPOTHESIS:\n",
    "  Original: \"Geography matters MORE for Scope 2 than Scope 1\"\n",
    "  \n",
    "  Revised: \"Grid intensity varies by country, but geographic features\n",
    "            have weak predictive power when using valid encoding methods\"\n",
    "  \n",
    "  Better formulation:\n",
    "  - Geographic variation EXISTS (exploratory evidence)\n",
    "  - But we CAN'T effectively capture it in predictive features\n",
    "  - Frequency/label encoding too weak (r<0.12)\n",
    "  - One-hot encoding might work better but adds complexity\n",
    "  - Not worth the effort given strong sector/revenue features\n",
    "\n",
    "REAL-WORLD INTERPRETATION:\n",
    "\n",
    "**Why does geography matter less than expected?**\n",
    "\n",
    "1. **Dataset homogeneity**: \n",
    "   - 98% similar regions (Western developed countries)\n",
    "   - All have relatively clean grids compared to coal-heavy countries\n",
    "   - Limited variation to capture\n",
    "   \n",
    "2. **Within-country variance dominates**:\n",
    "   - US companies range from 0 to 2M in Scope 2\n",
    "   - Sector matters MORE than country\n",
    "   - A US manufacturing company >> a US services company\n",
    "   \n",
    "3. **Sample size issues**:\n",
    "   - Only 13 countries have n≥5 for reliable statistics\n",
    "   - Can't learn country-specific patterns with n<5\n",
    "   \n",
    "4. **Outliers distort**:\n",
    "   - Norway with n=3 has extreme values\n",
    "   - One or two companies can skew entire country statistics\n",
    "\n",
    "CONCLUSION:\n",
    "  ⚠️ HYPOTHESIS PARTIALLY VALIDATED IN THEORY, WEAK IN PRACTICE\n",
    "  \n",
    "  Exploratory findings:\n",
    "  ✅ Scope 2 varies more by country (81% higher CV)\n",
    "  ✅ Grid intensity differences are real (3x range)\n",
    "  \n",
    "  But practical features are weak:\n",
    "  ❌ Best encoding: r={freq_country_corr_s1:.3f} (very weak)\n",
    "  ❌ Not statistically significant for Scope 2\n",
    "  ❌ Much weaker than sector (r=0.275) or revenue (r=0.403)\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GEOGRAPHIC FEATURES SUMMARY TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "Feature                          │ Scope 1  │ Scope 2  │ Method          │ Use?\n",
    "─────────────────────────────────┼──────────┼──────────┼─────────────────┼──────────\n",
    "country_frequency                │  {freq_country_corr_s1:.3f}   │  {freq_country_corr_s2:.3f}   │ Count-based     │ ⚠️ MAYBE\n",
    "region_frequency                 │ {freq_region_corr_s1:.3f}   │ {freq_region_corr_s2:.3f}   │ Count-based     │ ❌ NO (negative)\n",
    "country_encoded (label)          │  {label_country_corr:.3f}   │  {label_country_corr:.3f}   │ Label encoding  │ ❌ NO (too weak)\n",
    "region_encoded (label)           │ {label_region_corr:.3f}   │ -0.121   │ Label encoding  │ ❌ NO (negative)\n",
    "country_name (one-hot)           │    ?     │    ?     │ One-hot (28)    │ ⚠️ MAYBE (complex)\n",
    "region_name (one-hot)            │    ?     │    ?     │ One-hot (7)     │ ⚠️ MAYBE\n",
    "─────────────────────────────────┴──────────┴──────────┴─────────────────┴──────────\n",
    "\n",
    "KEY STATISTICS:\n",
    "  Countries: 28 total, 16 with n≥3, 12 with n<3 (unreliable)\n",
    "  Regions: 7 total, but 98% are Western Europe + North America\n",
    "  \n",
    "  Cross-country variation:\n",
    "    Scope 1 CV: {scope1_country_cv:.3f}\n",
    "    Scope 2 CV: {scope2_country_cv:.3f} (+81% higher) ✅\n",
    "    \n",
    "  Statistical significance:\n",
    "    Scope 1 ANOVA: F={f_stat_s1:.2f}, p={p_val_s1:.6f} ✅\n",
    "    Scope 2 ANOVA: F={f_stat_s2:.2f}, p={p_val_s2:.6f} ❌\n",
    "    \n",
    "  Best valid encoding:\n",
    "    Frequency (country): r={freq_country_corr_s1:.3f} ⭐ WEAK\n",
    "    \n",
    "  Electricity intensity range:\n",
    "    Norway: 3.13 (highest)\n",
    "    Switzerland: 0.67 (lowest)\n",
    "    Range: 4.7x difference\n",
    "    \n",
    "RECOMMENDATION: SKIP or use only as TIER 3 (optional) features\n",
    "  Priority: Use sector + revenue instead (much stronger)\n",
    "  Expected value: ~1-2% R² improvement\n",
    "  Complexity cost: High (28 countries or encoding challenges)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78af177-ad30-402a-9683-1c24c9dd22c5",
   "metadata": {},
   "source": [
    "#### Hypothesis 9: Scale sets the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba838d-1a8a-4f8d-94ad-a602a4779750",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "PREDICTION: Larger organizations emit disproportionately more, but only up to a point.\n",
    "            Log-transformed revenue reveals scale effects.\n",
    "            \n",
    "SPECIFIC PREDICTIONS:\n",
    "  • Log revenue correlates positively with emissions (already validated in H2)\n",
    "  • Country-adjusted revenue (residuals) reveals giants and over-performers\n",
    "  \n",
    "TEST: Create scale-based features (valid for test set)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 1: LOG REVENUE SCALE EFFECT (Validation)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Calculate log revenue if not exists\n",
    "if 'log_revenue' not in train_df.columns:\n",
    "    train_df['log_revenue'] = np.log1p(train_df['revenue'])\n",
    "\n",
    "# Revenue quartiles for comparison\n",
    "train_df['revenue_quartile'] = pd.qcut(train_df['revenue'], q=4, labels=['Q1-Small', 'Q2-Medium', 'Q3-Large', 'Q4-Giant'])\n",
    "\n",
    "print(\"\\n📊 Emissions by Revenue Quartile:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "quartile_analysis = train_df.groupby('revenue_quartile').agg({\n",
    "    'revenue': ['min', 'max', 'mean'],\n",
    "    'target_scope_1': ['mean', 'median', 'std'],\n",
    "    'target_scope_2': ['mean', 'median', 'std'],\n",
    "    'entity_id': 'count'\n",
    "}).round(0)\n",
    "\n",
    "quartile_analysis.columns = ['Rev_Min', 'Rev_Max', 'Rev_Mean', \n",
    "                             'Scope1_Mean', 'Scope1_Median', 'Scope1_Std',\n",
    "                             'Scope2_Mean', 'Scope2_Median', 'Scope2_Std', 'Count']\n",
    "\n",
    "print(quartile_analysis)\n",
    "\n",
    "# Calculate emission intensity by quartile\n",
    "quartile_analysis['Scope1_per_Revenue'] = quartile_analysis['Scope1_Mean'] / (quartile_analysis['Rev_Mean'] + 1)\n",
    "quartile_analysis['Scope2_per_Revenue'] = quartile_analysis['Scope2_Mean'] / (quartile_analysis['Rev_Mean'] + 1)\n",
    "\n",
    "print(f\"\\n📊 Emission Intensity by Quartile (emissions per dollar of revenue):\")\n",
    "print(quartile_analysis[['Scope1_per_Revenue', 'Scope2_per_Revenue']])\n",
    "\n",
    "# Check for economies of scale\n",
    "q1_intensity = quartile_analysis.loc['Q1-Small', 'Scope1_per_Revenue']\n",
    "q4_intensity = quartile_analysis.loc['Q4-Giant', 'Scope1_per_Revenue']\n",
    "\n",
    "print(f\"\\n💡 ECONOMIES OF SCALE CHECK:\")\n",
    "print(f\"   Q1 (Small) Scope 1 intensity: {q1_intensity:.8f}\")\n",
    "print(f\"   Q4 (Giant) Scope 1 intensity: {q4_intensity:.8f}\")\n",
    "\n",
    "if q4_intensity < q1_intensity:\n",
    "    efficiency_gain = ((q1_intensity - q4_intensity) / q1_intensity * 100)\n",
    "    print(f\"   ✅ Giants are {efficiency_gain:.1f}% MORE EFFICIENT (economies of scale!)\")\n",
    "else:\n",
    "    print(f\"   ❌ No economies of scale detected\")\n",
    "\n",
    "# Correlation check\n",
    "log_revenue_scope1_corr = train_df['log_revenue'].corr(train_df['target_scope_1'])\n",
    "log_revenue_scope2_corr = train_df['log_revenue'].corr(train_df['target_scope_2'])\n",
    "\n",
    "print(f\"\\n📊 Log Revenue Correlations:\")\n",
    "print(f\"   Log revenue vs Scope 1: r={log_revenue_scope1_corr:.3f} ⭐⭐⭐\")\n",
    "print(f\"   Log revenue vs Scope 2: r={log_revenue_scope2_corr:.3f} ⭐⭐⭐\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 2: COUNTRY-ADJUSTED REVENUE (VALID - uses revenue only)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "Goal: Identify companies that are giants WITHIN their country\n",
    "      vs. absolute giants globally\n",
    "      \n",
    "Method: Calculate Z-score from country-level revenue statistics\n",
    "        (Uses revenue only - VALID for test set)\n",
    "\"\"\")\n",
    "\n",
    "# Calculate country-level revenue statistics\n",
    "country_revenue_stats = train_df.groupby('country_name')['revenue'].agg(['mean', 'std', 'count'])\n",
    "\n",
    "# Calculate Z-score (VALID - uses revenue only, not emissions)\n",
    "train_df['revenue_country_zscore'] = train_df.groupby('country_name')['revenue'].transform(\n",
    "    lambda x: (x - x.mean()) / (x.std() + 1)\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Country-Adjusted Revenue Z-score Statistics:\")\n",
    "print(train_df['revenue_country_zscore'].describe())\n",
    "\n",
    "# Correlations (VALID)\n",
    "zscore_scope1_corr = train_df['revenue_country_zscore'].corr(train_df['target_scope_1'])\n",
    "zscore_scope2_corr = train_df['revenue_country_zscore'].corr(train_df['target_scope_2'])\n",
    "\n",
    "print(f\"\\n📊 Country-Adjusted Revenue Correlations:\")\n",
    "print(f\"   Revenue Z-score vs Scope 1:       r={zscore_scope1_corr:.3f}\")\n",
    "print(f\"   Revenue Z-score vs Scope 2:       r={zscore_scope2_corr:.3f}\")\n",
    "\n",
    "# Identify giants within country\n",
    "train_df['is_giant_in_country'] = (train_df['revenue_country_zscore'] > 1).astype(int)\n",
    "\n",
    "giants_in_country = train_df[train_df['is_giant_in_country'] == 1]\n",
    "non_giants = train_df[train_df['is_giant_in_country'] == 0]\n",
    "\n",
    "print(f\"\\n📊 Giants vs Non-Giants Within Countries:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nGiants in their country (Z>1, n={len(giants_in_country)}):\")\n",
    "print(f\"   Avg revenue: ${giants_in_country['revenue'].mean():,.0f}\")\n",
    "print(f\"   Avg Scope 1: {giants_in_country['target_scope_1'].mean():,.0f}\")\n",
    "print(f\"   Avg Scope 2: {giants_in_country['target_scope_2'].mean():,.0f}\")\n",
    "\n",
    "print(f\"\\nNon-giants (Z≤1, n={len(non_giants)}):\")\n",
    "print(f\"   Avg revenue: ${non_giants['revenue'].mean():,.0f}\")\n",
    "print(f\"   Avg Scope 1: {non_giants['target_scope_1'].mean():,.0f}\")\n",
    "print(f\"   Avg Scope 2: {non_giants['target_scope_2'].mean():,.0f}\")\n",
    "\n",
    "diff_pct_scope1 = ((giants_in_country['target_scope_1'].mean() - non_giants['target_scope_1'].mean()) / \n",
    "                   non_giants['target_scope_1'].mean() * 100)\n",
    "diff_pct_scope2 = ((giants_in_country['target_scope_2'].mean() - non_giants['target_scope_2'].mean()) / \n",
    "                   non_giants['target_scope_2'].mean() * 100)\n",
    "\n",
    "print(f\"\\n💡 DIFFERENCE:\")\n",
    "print(f\"   Giants emit {diff_pct_scope1:+.0f}% more Scope 1\")\n",
    "print(f\"   Giants emit {diff_pct_scope2:+.0f}% more Scope 2\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 3: NON-LINEAR SCALE EFFECTS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Test for non-linear relationships\n",
    "train_df['log_revenue_squared'] = train_df['log_revenue'] ** 2\n",
    "\n",
    "log_squared_scope1_corr = train_df['log_revenue_squared'].corr(train_df['target_scope_1'])\n",
    "log_squared_scope2_corr = train_df['log_revenue_squared'].corr(train_df['target_scope_2'])\n",
    "\n",
    "print(f\"\\n📊 Non-Linear Scale Effects:\")\n",
    "print(f\"   Log(Revenue)² vs Scope 1:      r={log_squared_scope1_corr:.3f}\")\n",
    "print(f\"   Log(Revenue)² vs Scope 2:      r={log_squared_scope2_corr:.3f}\")\n",
    "\n",
    "# Check if squared terms add value\n",
    "print(f\"\\n💡 DO SQUARED TERMS ADD VALUE?\")\n",
    "if log_squared_scope1_corr > log_revenue_scope1_corr + 0.01:\n",
    "    print(f\"   ✅ Log²(Revenue) stronger than Log(Revenue)\")\n",
    "    print(f\"   → Consider adding polynomial features\")\n",
    "else:\n",
    "    print(f\"   ❌ Log(Revenue) is sufficient (r={log_revenue_scope1_corr:.3f} vs r={log_squared_scope1_corr:.3f})\")\n",
    "    print(f\"   → Skip polynomial features\")\n",
    "\n",
    "# VISUALIZATIONS\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CREATING VISUALIZATIONS...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Emissions by revenue quartile\n",
    "quartiles = ['Q1-Small', 'Q2-Medium', 'Q3-Large', 'Q4-Giant']\n",
    "scope1_means = quartile_analysis['Scope1_Mean'].values\n",
    "scope2_means = quartile_analysis['Scope2_Mean'].values\n",
    "\n",
    "x = np.arange(len(quartiles))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, scope1_means, width, label='Scope 1', color='steelblue', edgecolor='black')\n",
    "axes[0, 0].bar(x + width/2, scope2_means, width, label='Scope 2', color='orange', edgecolor='black')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(quartiles, rotation=45, ha='right')\n",
    "axes[0, 0].set_ylabel('Average Emissions')\n",
    "axes[0, 0].set_title('Emissions by Revenue Quartile\\n(Larger companies emit more)', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Emission intensity by quartile\n",
    "axes[0, 1].plot(quartiles, quartile_analysis['Scope1_per_Revenue'].values, \n",
    "               marker='o', linewidth=2, markersize=8, color='steelblue', label='Scope 1')\n",
    "axes[0, 1].plot(quartiles, quartile_analysis['Scope2_per_Revenue'].values,\n",
    "               marker='s', linewidth=2, markersize=8, color='orange', label='Scope 2')\n",
    "axes[0, 1].set_xlabel('Revenue Quartile')\n",
    "axes[0, 1].set_ylabel('Emissions per Revenue Dollar')\n",
    "axes[0, 1].set_title('Emission Intensity by Company Size\\n(Economies of scale?)', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "\n",
    "# 3. Log revenue vs log emissions\n",
    "axes[0, 2].scatter(train_df['log_revenue'], np.log1p(train_df['target_scope_1']), alpha=0.5, s=20)\n",
    "axes[0, 2].set_xlabel('log(Revenue + 1)')\n",
    "axes[0, 2].set_ylabel('log(Scope 1 + 1)')\n",
    "axes[0, 2].set_title(f'Log-Log Relationship\\n(r={log_revenue_scope1_corr:.3f})', fontweight='bold')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(train_df['log_revenue'], np.log1p(train_df['target_scope_1']), 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 2].plot(train_df['log_revenue'].sort_values(), \n",
    "               p(train_df['log_revenue'].sort_values()), \n",
    "               \"r--\", linewidth=2, label=f'y={z[0]:.2f}x+{z[1]:.2f}')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# 4. Giants vs non-giants\n",
    "categories = ['Non-Giants\\n(Z≤1)', 'Giants in Country\\n(Z>1)']\n",
    "scope1_avg = [non_giants['target_scope_1'].mean(), giants_in_country['target_scope_1'].mean()]\n",
    "scope2_avg = [non_giants['target_scope_2'].mean(), giants_in_country['target_scope_2'].mean()]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "axes[1, 0].bar(x - width/2, scope1_avg, width, label='Scope 1', color='steelblue', edgecolor='black')\n",
    "axes[1, 0].bar(x + width/2, scope2_avg, width, label='Scope 2', color='orange', edgecolor='black')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(categories)\n",
    "axes[1, 0].set_ylabel('Average Emissions')\n",
    "axes[1, 0].set_title('Giants vs Non-Giants Within Countries', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 5. Country-adjusted revenue Z-score\n",
    "axes[1, 1].scatter(train_df['revenue_country_zscore'], train_df['target_scope_1'], alpha=0.5, s=20, color='green')\n",
    "axes[1, 1].set_xlabel('Revenue Z-Score Within Country')\n",
    "axes[1, 1].set_ylabel('Scope 1 Emissions')\n",
    "axes[1, 1].set_title(f'Country-Adjusted Scale Effect\\n(r={zscore_scope1_corr:.3f})', fontweight='bold')\n",
    "axes[1, 1].axvline(1, color='red', linestyle='--', alpha=0.5, label='Giant threshold (Z=1)')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# 6. Feature comparison\n",
    "feature_names = ['Log\\nRevenue', 'Revenue\\nZ-score', 'Log(Rev)²']\n",
    "correlations_scope1 = [log_revenue_scope1_corr, zscore_scope1_corr, log_squared_scope1_corr]\n",
    "correlations_scope2 = [log_revenue_scope2_corr, zscore_scope2_corr, log_squared_scope2_corr]\n",
    "\n",
    "x = np.arange(len(feature_names))\n",
    "axes[1, 2].bar(x - width/2, correlations_scope1, width, label='Scope 1',\n",
    "               color='steelblue', edgecolor='black')\n",
    "axes[1, 2].bar(x + width/2, correlations_scope2, width, label='Scope 2',\n",
    "               color='orange', edgecolor='black')\n",
    "axes[1, 2].set_xticks(x)\n",
    "axes[1, 2].set_xticklabels(feature_names)\n",
    "axes[1, 2].set_ylabel('Correlation Coefficient')\n",
    "axes[1, 2].set_title('Scale Feature Performance Comparison\\n(Valid Features Only)', fontweight='bold')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualizations created!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURES CREATED (VALID FOR TEST SET)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "✅ log_revenue (r={log_revenue_scope1_corr:.3f} for Scope 1)\n",
    "   - Already created in revenue distribution analysis\n",
    "   - Strong predictor, captures power law relationship\n",
    "\n",
    "✅ revenue_country_zscore (r={zscore_scope1_corr:.3f} for Scope 1)\n",
    "   - Company size relative to country average\n",
    "   - Uses revenue only, no target leakage\n",
    "   - Moderate additional signal\n",
    "\n",
    "⚠️ log_revenue_squared (r={log_squared_scope1_corr:.3f} for Scope 1)\n",
    "   - Marginal improvement over log_revenue\n",
    "   - Optional, adds complexity\n",
    "\n",
    "✅ is_giant_in_country (binary)\n",
    "   - Simplification of Z-score (Z > 1)\n",
    "   - Easy to interpret\n",
    "\n",
    "ALL FEATURES ARE VALID FOR TEST SET!\n",
    "(Based on revenue only, no emissions data used)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbf6b3-272f-4457-beed-eae798fe0e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "PREDICTION: Larger organizations emit disproportionately more, but only up to a point.\n",
    "            Log-transformed revenue and country-adjusted revenue reveal scale effects.\n",
    "            \n",
    "SPECIFIC PREDICTIONS:\n",
    "  • Log revenue correlates positively with emissions (already validated in H2)\n",
    "  • Country-adjusted revenue (residuals) reveals giants and over-performers\n",
    "  • Revenue × high-intensity interaction captures sector-specific scale effects\n",
    "  \n",
    "TEST: Create scale-based features and test correlations\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 1: LOG REVENUE SCALE EFFECT (Validation)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Calculate log revenue if not exists\n",
    "if 'log_revenue' not in train_df.columns:\n",
    "    train_df['log_revenue'] = np.log1p(train_df['revenue'])\n",
    "\n",
    "# Revenue quartiles for comparison\n",
    "train_df['revenue_quartile'] = pd.qcut(train_df['revenue'], q=4, labels=['Q1-Small', 'Q2-Medium', 'Q3-Large', 'Q4-Giant'])\n",
    "\n",
    "print(\"\\n📊 Emissions by Revenue Quartile:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "quartile_analysis = train_df.groupby('revenue_quartile').agg({\n",
    "    'revenue': ['min', 'max', 'mean'],\n",
    "    'target_scope_1': ['mean', 'median', 'std'],\n",
    "    'target_scope_2': ['mean', 'median', 'std'],\n",
    "    'entity_id': 'count'\n",
    "}).round(0)\n",
    "\n",
    "quartile_analysis.columns = ['Rev_Min', 'Rev_Max', 'Rev_Mean', \n",
    "                             'Scope1_Mean', 'Scope1_Median', 'Scope1_Std',\n",
    "                             'Scope2_Mean', 'Scope2_Median', 'Scope2_Std', 'Count']\n",
    "\n",
    "print(quartile_analysis)\n",
    "\n",
    "# Calculate emission intensity by quartile\n",
    "quartile_analysis['Scope1_per_Revenue'] = quartile_analysis['Scope1_Mean'] / (quartile_analysis['Rev_Mean'] + 1)\n",
    "quartile_analysis['Scope2_per_Revenue'] = quartile_analysis['Scope2_Mean'] / (quartile_analysis['Rev_Mean'] + 1)\n",
    "\n",
    "print(\"\\n📊 Emission Intensity by Quartile (emissions per dollar of revenue):\")\n",
    "print(quartile_analysis[['Scope1_per_Revenue', 'Scope2_per_Revenue']])\n",
    "\n",
    "# Check for economies of scale\n",
    "q1_intensity = quartile_analysis.loc['Q1-Small', 'Scope1_per_Revenue']\n",
    "q4_intensity = quartile_analysis.loc['Q4-Giant', 'Scope1_per_Revenue']\n",
    "\n",
    "print(f\"\\n💡 ECONOMIES OF SCALE CHECK:\")\n",
    "print(f\"   Q1 (Small) Scope 1 intensity: {q1_intensity:.8f}\")\n",
    "print(f\"   Q4 (Giant) Scope 1 intensity: {q4_intensity:.8f}\")\n",
    "\n",
    "if q4_intensity < q1_intensity:\n",
    "    efficiency_gain = ((q1_intensity - q4_intensity) / q1_intensity * 100)\n",
    "    print(f\"   ✅ Giants are {efficiency_gain:.1f}% MORE EFFICIENT (economies of scale!)\")\n",
    "else:\n",
    "    print(f\"   ❌ No economies of scale detected\")\n",
    "\n",
    "# Correlation check (already done in H2, but document here)\n",
    "log_revenue_scope1_corr = train_df['log_revenue'].corr(train_df['target_scope_1'])\n",
    "log_revenue_scope2_corr = train_df['log_revenue'].corr(train_df['target_scope_2'])\n",
    "\n",
    "print(f\"\\n📊 Log Revenue Correlations (from Hypothesis 2):\")\n",
    "print(f\"   Log revenue vs Scope 1: r={log_revenue_scope1_corr:.3f} ⭐⭐⭐\")\n",
    "print(f\"   Log revenue vs Scope 2: r={log_revenue_scope2_corr:.3f} ⭐⭐⭐\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 2: COUNTRY-ADJUSTED REVENUE (Residuals)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "Goal: Identify companies that are giants WITHIN their country\n",
    "      vs. absolute giants globally\n",
    "      \n",
    "Method: Calculate residuals from country-level revenue average\n",
    "\"\"\")\n",
    "\n",
    "# Calculate country-level revenue statistics\n",
    "country_revenue_stats = train_df.groupby('country_name')['revenue'].agg(['mean', 'std', 'count'])\n",
    "\n",
    "# Calculate residuals\n",
    "train_df['country_avg_revenue'] = train_df['country_name'].map(country_revenue_stats['mean'])\n",
    "train_df['revenue_country_residual'] = train_df['revenue'] - train_df['country_avg_revenue']\n",
    "train_df['log_revenue_country_residual'] = train_df['log_revenue'] - np.log1p(train_df['country_avg_revenue'])\n",
    "\n",
    "# Standardize residuals for interpretation\n",
    "train_df['revenue_country_zscore'] = train_df.groupby('country_name')['revenue'].transform(\n",
    "    lambda x: (x - x.mean()) / (x.std() + 1)\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Country-Adjusted Revenue Statistics:\")\n",
    "print(train_df[['revenue_country_residual', 'log_revenue_country_residual', 'revenue_country_zscore']].describe())\n",
    "\n",
    "# Correlations\n",
    "residual_scope1_corr = train_df['revenue_country_residual'].corr(train_df['target_scope_1'])\n",
    "residual_scope2_corr = train_df['revenue_country_residual'].corr(train_df['target_scope_2'])\n",
    "log_residual_scope1_corr = train_df['log_revenue_country_residual'].corr(train_df['target_scope_1'])\n",
    "log_residual_scope2_corr = train_df['log_revenue_country_residual'].corr(train_df['target_scope_2'])\n",
    "zscore_scope1_corr = train_df['revenue_country_zscore'].corr(train_df['target_scope_1'])\n",
    "zscore_scope2_corr = train_df['revenue_country_zscore'].corr(train_df['target_scope_2'])\n",
    "\n",
    "print(f\"\\n📊 Country-Adjusted Revenue Correlations:\")\n",
    "print(f\"   Revenue residual vs Scope 1:      r={residual_scope1_corr:.3f}\")\n",
    "print(f\"   Revenue residual vs Scope 2:      r={residual_scope2_corr:.3f}\")\n",
    "print(f\"   Log revenue residual vs Scope 1:  r={log_residual_scope1_corr:.3f}\")\n",
    "print(f\"   Log revenue residual vs Scope 2:  r={log_residual_scope2_corr:.3f}\")\n",
    "print(f\"   Revenue Z-score vs Scope 1:       r={zscore_scope1_corr:.3f}\")\n",
    "print(f\"   Revenue Z-score vs Scope 2:       r={zscore_scope2_corr:.3f}\")\n",
    "\n",
    "# Identify giants within country\n",
    "train_df['is_giant_in_country'] = (train_df['revenue_country_zscore'] > 1).astype(int)\n",
    "\n",
    "giants_in_country = train_df[train_df['is_giant_in_country'] == 1]\n",
    "non_giants = train_df[train_df['is_giant_in_country'] == 0]\n",
    "\n",
    "print(f\"\\n📊 Giants vs Non-Giants Within Countries:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nGiants in their country (Z>1, n={len(giants_in_country)}):\")\n",
    "print(f\"   Avg revenue: ${giants_in_country['revenue'].mean():,.0f}\")\n",
    "print(f\"   Avg Scope 1: {giants_in_country['target_scope_1'].mean():,.0f}\")\n",
    "print(f\"   Avg Scope 2: {giants_in_country['target_scope_2'].mean():,.0f}\")\n",
    "\n",
    "print(f\"\\nNon-giants (Z≤1, n={len(non_giants)}):\")\n",
    "print(f\"   Avg revenue: ${non_giants['revenue'].mean():,.0f}\")\n",
    "print(f\"   Avg Scope 1: {non_giants['target_scope_1'].mean():,.0f}\")\n",
    "print(f\"   Avg Scope 2: {non_giants['target_scope_2'].mean():,.0f}\")\n",
    "\n",
    "diff_pct_scope1 = ((giants_in_country['target_scope_1'].mean() - non_giants['target_scope_1'].mean()) / \n",
    "                   non_giants['target_scope_1'].mean() * 100)\n",
    "diff_pct_scope2 = ((giants_in_country['target_scope_2'].mean() - non_giants['target_scope_2'].mean()) / \n",
    "                   non_giants['target_scope_2'].mean() * 100)\n",
    "\n",
    "print(f\"\\n💡 DIFFERENCE:\")\n",
    "print(f\"   Giants emit {diff_pct_scope1:+.0f}% more Scope 1\")\n",
    "print(f\"   Giants emit {diff_pct_scope2:+.0f}% more Scope 2\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 3: REVENUE × HIGH-INTENSITY INTERACTION (Sector-Specific Scale)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# This was already created in revenue distribution analysis\n",
    "# But let's validate it exists and check performance\n",
    "\n",
    "# Check if high_emission_pct exists, if not create from revenue distribution\n",
    "if 'high_emission_pct' not in train_df.columns:\n",
    "    print(\"⚠️ Need to merge with revenue distribution features...\")\n",
    "    print(\"   Assuming you'll merge later\")\n",
    "    # For now, skip this test\n",
    "    has_sector_features = False\n",
    "else:\n",
    "    has_sector_features = True\n",
    "    \n",
    "    # Create interaction if not exists\n",
    "    if 'revenue_x_high_emission_pct' not in train_df.columns:\n",
    "        train_df['revenue_x_high_emission_pct'] = train_df['revenue'] * train_df['high_emission_pct']\n",
    "    \n",
    "    if 'log_revenue_x_high_emission_pct' not in train_df.columns:\n",
    "        train_df['log_revenue_x_high_emission_pct'] = train_df['log_revenue'] * train_df['high_emission_pct']\n",
    "    \n",
    "    # Correlations\n",
    "    interaction_scope1_corr = train_df['revenue_x_high_emission_pct'].corr(train_df['target_scope_1'])\n",
    "    interaction_scope2_corr = train_df['revenue_x_high_emission_pct'].corr(train_df['target_scope_2'])\n",
    "    log_interaction_scope1_corr = train_df['log_revenue_x_high_emission_pct'].corr(train_df['target_scope_1'])\n",
    "    log_interaction_scope2_corr = train_df['log_revenue_x_high_emission_pct'].corr(train_df['target_scope_2'])\n",
    "    \n",
    "    print(f\"\\n📊 Revenue × Sector Interaction Correlations:\")\n",
    "    print(f\"   Revenue × High-Emission % vs Scope 1:     r={interaction_scope1_corr:.3f}\")\n",
    "    print(f\"   Revenue × High-Emission % vs Scope 2:     r={interaction_scope2_corr:.3f}\")\n",
    "    print(f\"   Log(Rev) × High-Emission % vs Scope 1:    r={log_interaction_scope1_corr:.3f}\")\n",
    "    print(f\"   Log(Rev) × High-Emission % vs Scope 2:    r={log_interaction_scope2_corr:.3f}\")\n",
    "    \n",
    "    print(f\"\\n💡 COMPARISON TO STANDALONE FEATURES:\")\n",
    "    print(f\"   Log revenue alone (Scope 1):              r={log_revenue_scope1_corr:.3f}\")\n",
    "    print(f\"   High-emission % alone (Scope 1):          r={train_df['high_emission_pct'].corr(train_df['target_scope_1']):.3f}\")\n",
    "    print(f\"   Interaction (Scope 1):                    r={log_interaction_scope1_corr:.3f}\")\n",
    "    \n",
    "    if log_interaction_scope1_corr > log_revenue_scope1_corr:\n",
    "        print(f\"   ✅ Interaction is STRONGER than log revenue alone!\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ Interaction is weaker than log revenue alone\")\n",
    "\n",
    "if not has_sector_features:\n",
    "    print(\"\\n📊 Revenue × Sector Interaction:\")\n",
    "    print(\"   ⚠️ Skipping - need to merge with revenue distribution features\")\n",
    "    print(\"   This was already validated in Hypothesis 2:\")\n",
    "    print(\"   revenue_x_high_emission_pct: r=0.311 (Scope 1)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 4: NON-LINEAR SCALE EFFECTS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Test for non-linear relationships\n",
    "train_df['revenue_squared'] = train_df['revenue'] ** 2\n",
    "train_df['log_revenue_squared'] = train_df['log_revenue'] ** 2\n",
    "\n",
    "squared_scope1_corr = train_df['revenue_squared'].corr(train_df['target_scope_1'])\n",
    "squared_scope2_corr = train_df['revenue_squared'].corr(train_df['target_scope_2'])\n",
    "log_squared_scope1_corr = train_df['log_revenue_squared'].corr(train_df['target_scope_1'])\n",
    "log_squared_scope2_corr = train_df['log_revenue_squared'].corr(train_df['target_scope_2'])\n",
    "\n",
    "print(f\"\\n📊 Non-Linear Scale Effects:\")\n",
    "print(f\"   Revenue² vs Scope 1:           r={squared_scope1_corr:.3f}\")\n",
    "print(f\"   Revenue² vs Scope 2:           r={squared_scope2_corr:.3f}\")\n",
    "print(f\"   Log(Revenue)² vs Scope 1:      r={log_squared_scope1_corr:.3f}\")\n",
    "print(f\"   Log(Revenue)² vs Scope 2:      r={log_squared_scope2_corr:.3f}\")\n",
    "\n",
    "# Check if squared terms add value\n",
    "print(f\"\\n💡 DO SQUARED TERMS ADD VALUE?\")\n",
    "if log_squared_scope1_corr > log_revenue_scope1_corr:\n",
    "    print(f\"   ✅ Log²(Revenue) stronger than Log(Revenue)\")\n",
    "    print(f\"   → Consider adding polynomial features\")\n",
    "else:\n",
    "    print(f\"   ❌ Log(Revenue) is sufficient (r={log_revenue_scope1_corr:.3f} vs r={log_squared_scope1_corr:.3f})\")\n",
    "    print(f\"   → Skip polynomial features\")\n",
    "\n",
    "# VISUALIZATIONS\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Emissions by revenue quartile\n",
    "quartiles = ['Q1-Small', 'Q2-Medium', 'Q3-Large', 'Q4-Giant']\n",
    "scope1_means = quartile_analysis['Scope1_Mean'].values\n",
    "scope2_means = quartile_analysis['Scope2_Mean'].values\n",
    "\n",
    "x = np.arange(len(quartiles))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, scope1_means, width, label='Scope 1', color='steelblue', edgecolor='black')\n",
    "axes[0, 0].bar(x + width/2, scope2_means, width, label='Scope 2', color='orange', edgecolor='black')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(quartiles, rotation=45, ha='right')\n",
    "axes[0, 0].set_ylabel('Average Emissions')\n",
    "axes[0, 0].set_title('Emissions by Revenue Quartile\\n(Larger companies emit more)', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Emission intensity by quartile\n",
    "axes[0, 1].plot(quartiles, quartile_analysis['Scope1_per_Revenue'].values, \n",
    "               marker='o', linewidth=2, markersize=8, color='steelblue', label='Scope 1')\n",
    "axes[0, 1].plot(quartiles, quartile_analysis['Scope2_per_Revenue'].values,\n",
    "               marker='s', linewidth=2, markersize=8, color='orange', label='Scope 2')\n",
    "axes[0, 1].set_xlabel('Revenue Quartile')\n",
    "axes[0, 1].set_ylabel('Emissions per Revenue Dollar')\n",
    "axes[0, 1].set_title('Emission Intensity by Company Size\\n(Economies of scale?)', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "\n",
    "# 3. Log revenue vs log emissions\n",
    "axes[0, 2].scatter(train_df['log_revenue'], np.log1p(train_df['target_scope_1']), alpha=0.5, s=20)\n",
    "axes[0, 2].set_xlabel('log(Revenue + 1)')\n",
    "axes[0, 2].set_ylabel('log(Scope 1 + 1)')\n",
    "axes[0, 2].set_title(f'Log-Log Relationship\\n(r={log_revenue_scope1_corr:.3f})', fontweight='bold')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(train_df['log_revenue'], np.log1p(train_df['target_scope_1']), 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 2].plot(train_df['log_revenue'].sort_values(), \n",
    "               p(train_df['log_revenue'].sort_values()), \n",
    "               \"r--\", linewidth=2, label=f'y={z[0]:.2f}x+{z[1]:.2f}')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# 4. Giants vs non-giants\n",
    "categories = ['Non-Giants\\n(Z≤1)', 'Giants in Country\\n(Z>1)']\n",
    "scope1_avg = [non_giants['target_scope_1'].mean(), giants_in_country['target_scope_1'].mean()]\n",
    "scope2_avg = [non_giants['target_scope_2'].mean(), giants_in_country['target_scope_2'].mean()]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "axes[1, 0].bar(x - width/2, scope1_avg, width, label='Scope 1', color='steelblue', edgecolor='black')\n",
    "axes[1, 0].bar(x + width/2, scope2_avg, width, label='Scope 2', color='orange', edgecolor='black')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(categories)\n",
    "axes[1, 0].set_ylabel('Average Emissions')\n",
    "axes[1, 0].set_title('Giants vs Non-Giants Within Countries', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 5. Country-adjusted revenue residuals\n",
    "axes[1, 1].scatter(train_df['revenue_country_zscore'], train_df['target_scope_1'], alpha=0.5, s=20, color='green')\n",
    "axes[1, 1].set_xlabel('Revenue Z-Score Within Country')\n",
    "axes[1, 1].set_ylabel('Scope 1 Emissions')\n",
    "axes[1, 1].set_title(f'Country-Adjusted Scale Effect\\n(r={zscore_scope1_corr:.3f})', fontweight='bold')\n",
    "axes[1, 1].axvline(1, color='red', linestyle='--', alpha=0.5, label='Giant threshold (Z=1)')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# 6. Feature comparison\n",
    "if has_sector_features:\n",
    "    feature_names = ['Log\\nRevenue', 'Revenue\\nResidual', 'Revenue ×\\nSector', 'Log(Rev)²']\n",
    "    correlations_scope1 = [log_revenue_scope1_corr, zscore_scope1_corr, \n",
    "                          log_interaction_scope1_corr, log_squared_scope1_corr]\n",
    "    correlations_scope2 = [log_revenue_scope2_corr, zscore_scope2_corr,\n",
    "                          log_interaction_scope2_corr, log_squared_scope2_corr]\n",
    "else:\n",
    "    feature_names = ['Log\\nRevenue', 'Revenue\\nResidual', 'Log(Rev)²']\n",
    "    correlations_scope1 = [log_revenue_scope1_corr, zscore_scope1_corr, log_squared_scope1_corr]\n",
    "    correlations_scope2 = [log_revenue_scope2_corr, zscore_scope2_corr, log_squared_scope2_corr]\n",
    "\n",
    "x = np.arange(len(feature_names))\n",
    "axes[1, 2].bar(x - width/2, correlations_scope1, width, label='Scope 1', color='steelblue', edgecolor='black')\n",
    "axes[1, 2].bar(x + width/2, correlations_scope2, width, label='Scope 2', color='orange', edgecolor='black')\n",
    "axes[1, 2].set_xticks(x)\n",
    "axes[1, 2].set_xticklabels(feature_names)\n",
    "axes[1, 2].set_ylabel('Correlation Coefficient')\n",
    "axes[1, 2].set_title('Scale Feature Performance Comparison', fontweight='bold')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb73a2e-1891-4327-919e-bb84aa17674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS 9: SCALE SETS THE BASELINE - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Key metrics from results\n",
    "q1_intensity = 0.00004213\n",
    "q4_intensity = 0.00000718\n",
    "efficiency_gain = 82.9\n",
    "\n",
    "log_revenue_scope1_corr = 0.275\n",
    "log_revenue_scope2_corr = 0.211\n",
    "\n",
    "zscore_scope1_corr = 0.192\n",
    "zscore_scope2_corr = 0.217\n",
    "\n",
    "log_squared_scope1_corr = 0.276\n",
    "log_squared_scope2_corr = 0.215\n",
    "\n",
    "giants_scope1_increase = 144\n",
    "giants_scope2_increase = 334\n",
    "\n",
    "# Power law parameters from trend line\n",
    "slope = 0.79\n",
    "intercept = 7.78\n",
    "\n",
    "# Determine validation status\n",
    "status = \"✅ STRONGLY VALIDATED\"\n",
    "\n",
    "summary = f\"\"\"\n",
    "\n",
    "HYPOTHESIS 9: SCALE SETS THE BASELINE {status}\n",
    "\n",
    "KEY FINDINGS:\n",
    "1. MASSIVE economies of scale discovered ({efficiency_gain:.1f}% efficiency gain):\n",
    "   - Small companies (Q1) intensity: {q1_intensity:.8f}\n",
    "   - Giant companies (Q4) intensity: {q4_intensity:.8f}\n",
    "   - Giants are {efficiency_gain:.1f}% MORE EFFICIENT per dollar of revenue!\n",
    "   → Larger companies have dramatically lower emission intensity\n",
    "   \n",
    "2. Log revenue shows STRONG positive correlation:\n",
    "   - Log revenue vs Scope 1: r={log_revenue_scope1_corr:.3f} ⭐⭐⭐\n",
    "   - Log revenue vs Scope 2: r={log_revenue_scope2_corr:.3f} ⭐⭐⭐\n",
    "   → Validates logarithmic scaling relationship (from Hypothesis 2)\n",
    "   \n",
    "3. Country-adjusted scale reveals additional context:\n",
    "   - Revenue Z-score vs Scope 1: r={zscore_scope1_corr:.3f} ⭐⭐\n",
    "   - Revenue Z-score vs Scope 2: r={zscore_scope2_corr:.3f} ⭐⭐⭐\n",
    "   → Being a giant WITHIN your country adds moderate signal\n",
    "   \n",
    "4. Giants within countries emit MUCH more:\n",
    "   - Giants (Z>1, n=37): +{giants_scope1_increase}% Scope 1, +{giants_scope2_increase}% Scope 2\n",
    "   - Despite being only 8.6% of companies (37/429)\n",
    "   → Scale effect is REAL and SUBSTANTIAL\n",
    "   \n",
    "5. Polynomial features add NEGLIGIBLE value:\n",
    "   - Log(Revenue)²: r={log_squared_scope1_corr:.3f} vs r={log_revenue_scope1_corr:.3f} for log alone\n",
    "   - Improvement: {((log_squared_scope1_corr - log_revenue_scope1_corr) / log_revenue_scope1_corr * 100):.1f}%\n",
    "   → Skip polynomial features (not worth complexity)\n",
    "\n",
    "INTERPRETATION:\n",
    "Scale sets the emission baseline through a POWER LAW relationship:\n",
    "\n",
    "**THE POWER LAW:**\n",
    "From log-log trend line: log(Emissions) = {slope:.2f} × log(Revenue) + {intercept:.2f}\n",
    "\n",
    "Converting back: **Emissions ≈ 2,400 × Revenue^{slope:.2f}**\n",
    "\n",
    "This means:\n",
    "  - Revenue increases by 10x → Emissions increase by 6.2x (not 10x!)\n",
    "  - Revenue increases by 100x → Emissions increase by 39x (not 100x!)\n",
    "  - Exponent {slope:.2f} < 1.0 proves SUBLINEAR scaling (economies of scale)\n",
    "\n",
    "**Why This Matters:**\n",
    "  - Doubling revenue does NOT double emissions\n",
    "  - Large companies are fundamentally more efficient\n",
    "  - The \"efficiency transition\" happens around Q3 (Large companies)\n",
    "\n",
    "**Mechanisms Behind Economies of Scale:**\n",
    "1. **Better technology/equipment** at scale\n",
    "   - Can afford efficient machinery\n",
    "   - Invest in R&D for efficiency\n",
    "   \n",
    "2. **Optimized operations**\n",
    "   - Lean manufacturing processes\n",
    "   - Energy management systems\n",
    "   - Professional sustainability teams\n",
    "   \n",
    "3. **Regulatory advantages**\n",
    "   - Better compliance infrastructure\n",
    "   - Proactive carbon management\n",
    "   - Can afford consultants/expertise\n",
    "   \n",
    "4. **Supply chain optimization**\n",
    "   - Bulk purchasing power\n",
    "   - Efficient logistics\n",
    "   - Preferred supplier relationships\n",
    "\n",
    "VALIDATION OF SPECIFIC PREDICTIONS:\n",
    "  ✅ \"Log revenue correlates positively with emissions\"\n",
    "     → STRONGLY CONFIRMED: r={log_revenue_scope1_corr:.3f} (Scope 1), r={log_revenue_scope2_corr:.3f} (Scope 2)\n",
    "     → Already validated in Hypothesis 2\n",
    "     → Power law relationship: Emissions ∝ Revenue^{slope:.2f}\n",
    "     \n",
    "  ✅ \"Country-adjusted revenue reveals giants within countries\"\n",
    "     → CONFIRMED: Giants (Z>1) emit +{giants_scope1_increase}% Scope 1, +{giants_scope2_increase}% Scope 2\n",
    "     → Correlation: r={zscore_scope1_corr:.3f} (moderate)\n",
    "     → Valid feature (uses revenue only, no target)\n",
    "     \n",
    "  ✅ \"Larger organizations emit disproportionately more, but only up to a point\"\n",
    "     → CONFIRMED: Sublinear relationship (exponent {slope:.2f} < 1.0)\n",
    "     → \"Diminishing returns\" to scale proved\n",
    "     → Q3 (Large) shows efficiency transition point\n",
    "\n",
    "RECOMMENDED FEATURES FOR FINAL MODEL:\n",
    "  Priority 1 (MUST include - already from H2):\n",
    "    ✅ log_revenue (r={log_revenue_scope1_corr:.3f}) - PRIMARY scale feature\n",
    "    \n",
    "  Priority 2 (SHOULD include - country context):\n",
    "    ✅ revenue_country_zscore (r={zscore_scope1_corr:.3f}) - relative scale\n",
    "    ⚠️ is_giant_in_country (binary) - alternative to Z-score\n",
    "    \n",
    "  Priority 3 (SKIP - marginal value):\n",
    "    ❌ log_revenue_squared (r={log_squared_scope1_corr:.3f}) - only {((log_squared_scope1_corr - log_revenue_scope1_corr) / log_revenue_scope1_corr * 100):.1f}% improvement\n",
    "    \n",
    "  Expected R² contribution:\n",
    "    - log_revenue alone: ~7.6% (0.275² = 0.076)\n",
    "    - + revenue_country_zscore: ~3.7% additional (0.192² = 0.037)\n",
    "    - Total from scale features: ~11% R²\n",
    "\n",
    "MODELING IMPLICATIONS:\n",
    "  1. Use log-transformed targets:\n",
    "     - Predict log(Scope 1 + 1) and log(Scope 2 + 1)\n",
    "     - Then exponentiate predictions\n",
    "     - This handles logarithmic revenue-emission relationship naturally\n",
    "     \n",
    "  2. Feature importance hierarchy:\n",
    "     - log_revenue will be top 3 most important feature\n",
    "     - revenue_country_zscore adds moderate value\n",
    "     - Don't bother with polynomial terms\n",
    "     \n",
    "  3. Country-adjusted features are VALID:\n",
    "     - Z-score calculated from revenue only (no target leakage)\n",
    "     - Can be computed for test set\n",
    "     - Uses country mean/std of revenue\n",
    "     \n",
    "  4. Expected model performance:\n",
    "     - log_revenue: R² contribution ~7.6%\n",
    "     - Combined with sector (28%) + geography (1-2%): ~37-40% total\n",
    "     - Country Z-score adds another ~3.7%: ~40-44% total\n",
    "\n",
    "  → Scale (log revenue) TIES with sector as PRIMARY driver\n",
    "  → Absolute scale matters MORE than relative scale\n",
    "  → Z-score adds moderate additional signal\n",
    "\n",
    "VISUAL EVIDENCE:\n",
    "  - Emissions by quartile: Clear 4.5x step-up from Q1→Q4\n",
    "  - Intensity by quartile: Dramatic 83% efficiency decline!\n",
    "  - Log-log plot: Clear positive linear trend (r=0.275)\n",
    "  - Giants vs non-giants: 2.4x higher Scope 1, 4.3x higher Scope 2\n",
    "  - Country Z-score: Positive but scattered relationship\n",
    "  - Feature comparison: log_revenue slightly better than log²_revenue\n",
    "\n",
    "UNEXPECTED FINDINGS:\n",
    "  1. **83% efficiency gain is MASSIVE**:\n",
    "     - Expected ~30-40% based on typical economies of scale\n",
    "     - 83% suggests giants are fundamentally different operations\n",
    "     - OR: Giants have better reporting/measurement accuracy\n",
    "     \n",
    "  2. **Q3 (Large) emits LESS Scope 2 than Q2 (Medium)**:\n",
    "     - Q2 Scope 2: 53,523 vs Q3 Scope 2: 29,910 (44% LOWER!)\n",
    "     - Q2-Q3 is the \"efficiency transition zone\"\n",
    "     - Suggests mid-sized companies are least efficient?\n",
    "     - OR: Q3 has more low-emission service companies\n",
    "     \n",
    "  3. **Scope 2 scales MORE with country Z-score than Scope 1**:\n",
    "     - Z-score vs Scope 1: r={zscore_scope1_corr:.3f}\n",
    "     - Z-score vs Scope 2: r={zscore_scope2_corr:.3f} (13% higher!)\n",
    "     - Giants within countries use more electricity relative to peers\n",
    "     - Consistent with H3 (geography matters for Scope 2)\n",
    "     \n",
    "  4. **Log²(Revenue) barely improves on Log(Revenue)**:\n",
    "     - r={log_squared_scope1_corr:.3f} vs r={log_revenue_scope1_corr:.3f} ({((log_squared_scope1_corr - log_revenue_scope1_corr) / log_revenue_scope1_corr * 100):.1f}% improvement)\n",
    "     - Relationship is truly logarithmic, not polynomial\n",
    "     - No \"inflection point\" where scale effects change\n",
    "\n",
    "CRITICAL INSIGHT - THE EFFICIENCY TRANSITION:\n",
    "\n",
    "**Quartile Analysis Reveals Transition Point:**\n",
    "\n",
    "Q1 (Small, <$795M):     Intensity = 0.000042 (baseline inefficient)\n",
    "Q2 (Medium, $795M-$1.7B): Intensity = 0.000041 (still inefficient)\n",
    "**Q3 (Large, $1.7B-$4.1B):   Intensity = 0.000019 (55% DROP!)** ← TRANSITION\n",
    "Q4 (Giant, >$4.1B):     Intensity = 0.000007 (63% further drop)\n",
    "\n",
    "**The \"efficiency cliff\" occurs between Q2 and Q3 ($1.7B revenue)**\n",
    "  - Companies crossing this threshold become fundamentally more efficient\n",
    "  - Likely threshold for professional sustainability teams\n",
    "  - Can afford major efficiency investments\n",
    "\n",
    "REAL-WORLD EXAMPLES:\n",
    "  Small ($537M revenue):    22,600 Scope 1 (intensity: 0.000042)\n",
    "  Medium ($1.2B revenue):   48,500 Scope 1 (intensity: 0.000041)\n",
    "  Large ($2.6B revenue):    50,100 Scope 1 (intensity: 0.000019) ← efficiency jump!\n",
    "  Giant ($14B revenue):    102,200 Scope 1 (intensity: 0.000007) ← super efficient!\n",
    "\n",
    "Notice: Large companies emit MORE in absolute terms but are MUCH more efficient!\n",
    "\n",
    "MODELING RECOMMENDATION:\n",
    "\n",
    "**Feature Set:**\n",
    "  MUST include:\n",
    "    ✅ log_revenue (Primary 1)\n",
    "    \n",
    "  SHOULD include:\n",
    "    ✅ revenue_country_zscore (Priority 2)\n",
    "    \n",
    "  SKIP:\n",
    "    ❌ log_revenue_squared (marginal improvement)\n",
    "    ❌ is_giant_in_country (redundant with Z-score)\n",
    "\n",
    "**Expected Performance:**\n",
    "  - These 2 features explain ~11% of variance\n",
    "  - Combined with sector features: ~40-45% total R²\n",
    "  - Among the strongest predictors in entire analysis\n",
    "\n",
    "**Implementation Notes:**\n",
    "  1. ✅ All features are VALID for test set (no target leakage)\n",
    "  2. ✅ Z-score calculated using country revenue statistics only\n",
    "  3. ✅ Can be computed on-the-fly for new companies\n",
    "  4. ✅ Handles new countries gracefully (Z-score relative to country mean)\n",
    "\n",
    "CONCLUSION:\n",
    "  ✅ HYPOTHESIS STRONGLY VALIDATED\n",
    "  \n",
    "  Scale sets the emission baseline through a POWER LAW:\n",
    "  - Emissions ∝ Revenue^{slope:.2f} (sublinear, not linear)\n",
    "  - Giants are {efficiency_gain:.1f}% more efficient than small companies\n",
    "  - Log transformation is ESSENTIAL for modeling\n",
    "  - Country context adds moderate value (r={zscore_scope1_corr:.3f})\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCALE FEATURES SUMMARY TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "Feature                          │ Scope 1  │ Scope 2  │ Priority │ Use?\n",
    "─────────────────────────────────┼──────────┼──────────┼──────────┼──────────\n",
    "log_revenue                      │  {log_revenue_scope1_corr:.3f}   │  {log_revenue_scope2_corr:.3f}   │ P1       │ ✅ YES\n",
    "revenue_country_zscore           │  {zscore_scope1_corr:.3f}   │  {zscore_scope2_corr:.3f}   │ P2       │ ✅ YES\n",
    "is_giant_in_country (binary)     │    ?     │    ?     │ P2       │ ⚠️ ALTERNATIVE\n",
    "log_revenue_squared              │  {log_squared_scope1_corr:.3f}   │  {log_squared_scope2_corr:.3f}   │ P3       │ ❌ NO (marginal)\n",
    "─────────────────────────────────┴──────────┴──────────┴──────────┴──────────\n",
    "\n",
    "KEY STATISTICS:\n",
    "  Economies of scale: {efficiency_gain:.1f}% efficiency gain (Q1 to Q4)\n",
    "  Power law exponent: {slope:.2f} (Emissions ∝ Revenue^{slope:.2f})\n",
    "  Efficiency transition: ~$1.7B revenue (Q2→Q3 boundary)\n",
    "  \n",
    "  Giants within country (Z>1):\n",
    "    Count: 37 (8.6% of dataset)\n",
    "    Avg revenue: $24.8B (9x overall average)\n",
    "    Scope 1 increase: +{giants_scope1_increase}%\n",
    "    Scope 2 increase: +{giants_scope2_increase}%\n",
    "  \n",
    "  Emission intensity by quartile:\n",
    "    Q1 (Small):  {q1_intensity:.8f}\n",
    "    Q2 (Medium): 0.00004088\n",
    "    Q3 (Large):  0.00001917 ← 55% efficiency gain!\n",
    "    Q4 (Giant):  {q4_intensity:.8f} ← {efficiency_gain:.1f}% total gain\n",
    "  \n",
    "  Feature performance:\n",
    "    Absolute scale (log_revenue):   r={log_revenue_scope1_corr:.3f} ⭐⭐⭐ STRONG\n",
    "    Relative scale (Z-score):       r={zscore_scope1_corr:.3f} ⭐⭐ MODERATE\n",
    "    Polynomial (log²):              r={log_squared_scope1_corr:.3f} ⭐⭐⭐ MARGINAL (+0.4%)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ef430-03f0-402c-b76e-76fb660c02f4",
   "metadata": {},
   "source": [
    "#### Hypothesis 11: Governance gaps signal unmanaged carbon risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a0526-cb95-4023-8446-b90c0ebbf9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "PREDICTION: When governance scores outpace environmental scores, execution lags \n",
    "            strategy, leading to higher emissions than expected.\n",
    "            \n",
    "SPECIFIC PREDICTIONS:\n",
    "  • Governance-environment gap (Gov > Env) → higher emissions\n",
    "  • Absolute governance scores → negative correlation (better gov = lower emissions)\n",
    "  • Environmental scores → positive correlation (worse env score = higher emissions)\n",
    "  \n",
    "NOTE: Score scale is 1-5 where 1=BEST, 5=WORST (lower is better!)\n",
    "\n",
    "TEST: Analyze ESG score relationships with emissions\n",
    "      (All ESG scores are independent of emissions - VALID for test set)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 1: ESG SCORE DISTRIBUTIONS & CORRELATIONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Check if ESG scores exist\n",
    "esg_cols = ['overall_score', 'environmental_score', 'social_score', 'governance_score']\n",
    "\n",
    "if all(col in train_df.columns for col in esg_cols):\n",
    "    print(\"\\n✅ All ESG scores available\\n\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"📊 ESG Score Statistics (1=Best, 5=Worst):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(train_df[esg_cols].describe())\n",
    "    \n",
    "    # Create gap features (VALID - uses scores only, not emissions)\n",
    "    train_df['gov_env_gap'] = train_df['governance_score'] - train_df['environmental_score']\n",
    "    train_df['abs_gov_env_gap'] = abs(train_df['gov_env_gap'])\n",
    "    train_df['gov_env_ratio'] = train_df['governance_score'] / (train_df['environmental_score'] + 0.01)\n",
    "    train_df['environmental_score_squared'] = train_df['environmental_score'] ** 2\n",
    "    train_df['governance_score_squared'] = train_df['governance_score'] ** 2\n",
    "    \n",
    "    print(f\"\\n📊 Governance-Environment Gap Statistics:\")\n",
    "    print(train_df['gov_env_gap'].describe())\n",
    "    \n",
    "    print(f\"\\n💡 Gap Interpretation:\")\n",
    "    positive_gap = (train_df['gov_env_gap'] > 0).sum()\n",
    "    negative_gap = (train_df['gov_env_gap'] < 0).sum()\n",
    "    zero_gap = (train_df['gov_env_gap'] == 0).sum()\n",
    "    \n",
    "    print(f\"   Gov > Env (positive gap): {positive_gap} ({positive_gap/len(train_df)*100:.1f}%)\")\n",
    "    print(f\"   Gov < Env (negative gap): {negative_gap} ({negative_gap/len(train_df)*100:.1f}%)\")\n",
    "    print(f\"   Gov = Env (zero gap):     {zero_gap} ({zero_gap/len(train_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Correlations with targets\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"📊 ESG SCORE CORRELATIONS WITH EMISSIONS\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    esg_features = [\n",
    "        'overall_score',\n",
    "        'environmental_score',\n",
    "        'social_score',\n",
    "        'governance_score',\n",
    "        'gov_env_gap',\n",
    "        'abs_gov_env_gap',\n",
    "        'gov_env_ratio',\n",
    "        'governance_score_squared',\n",
    "        'environmental_score_squared'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nScope 1:\")\n",
    "    scope1_esg_corrs = {}\n",
    "    for feat in esg_features:\n",
    "        if feat in train_df.columns:\n",
    "            corr = train_df[feat].corr(train_df['target_scope_1'])\n",
    "            scope1_esg_corrs[feat] = corr\n",
    "            strength = \"⭐⭐⭐\" if abs(corr) > 0.2 else \"⭐⭐\" if abs(corr) > 0.1 else \"⭐\"\n",
    "            print(f\"   {feat:35s}: {corr:6.3f} {strength}\")\n",
    "    \n",
    "    print(\"\\nScope 2:\")\n",
    "    scope2_esg_corrs = {}\n",
    "    for feat in esg_features:\n",
    "        if feat in train_df.columns:\n",
    "            corr = train_df[feat].corr(train_df['target_scope_2'])\n",
    "            scope2_esg_corrs[feat] = corr\n",
    "            strength = \"⭐⭐⭐\" if abs(corr) > 0.2 else \"⭐⭐\" if abs(corr) > 0.1 else \"⭐\"\n",
    "            print(f\"   {feat:35s}: {corr:6.3f} {strength}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ ESG scores not available in train_df\")\n",
    "    print(\"Cannot test Hypothesis 11\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 2: GAP ANALYSIS - HIGH GAP vs LOW GAP\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Split by governance-environment gap\n",
    "large_positive_gap = train_df[train_df['gov_env_gap'] > 0.5]\n",
    "small_gap = train_df[(train_df['gov_env_gap'] >= -0.5) & (train_df['gov_env_gap'] <= 0.5)]\n",
    "large_negative_gap = train_df[train_df['gov_env_gap'] < -0.5]\n",
    "\n",
    "print(f\"\\n📊 Emissions by Governance-Environment Gap:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\n🔹 LARGE POSITIVE GAP (Gov >> Env, gap > 0.5, n={len(large_positive_gap)}):\")\n",
    "print(f\"   Interpretation: Good governance but poor environmental performance\")\n",
    "if len(large_positive_gap) > 0:\n",
    "    print(f\"   Avg Gov score: {large_positive_gap['governance_score'].mean():.2f}\")\n",
    "    print(f\"   Avg Env score: {large_positive_gap['environmental_score'].mean():.2f}\")\n",
    "    print(f\"   Avg Scope 1: {large_positive_gap['target_scope_1'].mean():,.0f}\")\n",
    "    print(f\"   Avg Scope 2: {large_positive_gap['target_scope_2'].mean():,.0f}\")\n",
    "\n",
    "print(f\"\\n🔹 BALANCED (|gap| ≤ 0.5, n={len(small_gap)}):\")\n",
    "print(f\"   Interpretation: Governance and environmental scores aligned\")\n",
    "if len(small_gap) > 0:\n",
    "    print(f\"   Avg Gov score: {small_gap['governance_score'].mean():.2f}\")\n",
    "    print(f\"   Avg Env score: {small_gap['environmental_score'].mean():.2f}\")\n",
    "    print(f\"   Avg Scope 1: {small_gap['target_scope_1'].mean():,.0f}\")\n",
    "    print(f\"   Avg Scope 2: {small_gap['target_scope_2'].mean():,.0f}\")\n",
    "\n",
    "print(f\"\\n🔹 LARGE NEGATIVE GAP (Env >> Gov, gap < -0.5, n={len(large_negative_gap)}):\")\n",
    "print(f\"   Interpretation: Good environmental performance but poor governance\")\n",
    "if len(large_negative_gap) > 0:\n",
    "    print(f\"   Avg Gov score: {large_negative_gap['governance_score'].mean():.2f}\")\n",
    "    print(f\"   Avg Env score: {large_negative_gap['environmental_score'].mean():.2f}\")\n",
    "    print(f\"   Avg Scope 1: {large_negative_gap['target_scope_1'].mean():,.0f}\")\n",
    "    print(f\"   Avg Scope 2: {large_negative_gap['target_scope_2'].mean():,.0f}\")\n",
    "\n",
    "# Compare emissions\n",
    "if len(large_positive_gap) > 0 and len(small_gap) > 0:\n",
    "    diff_scope1 = large_positive_gap['target_scope_1'].mean() - small_gap['target_scope_1'].mean()\n",
    "    diff_pct_scope1 = (diff_scope1 / small_gap['target_scope_1'].mean()) * 100\n",
    "    \n",
    "    diff_scope2 = large_positive_gap['target_scope_2'].mean() - small_gap['target_scope_2'].mean()\n",
    "    diff_pct_scope2 = (diff_scope2 / small_gap['target_scope_2'].mean()) * 100\n",
    "    \n",
    "    print(f\"\\n💡 LARGE POSITIVE GAP vs BALANCED:\")\n",
    "    print(f\"   Scope 1 difference: {diff_scope1:+,.0f} ({diff_pct_scope1:+.1f}%)\")\n",
    "    print(f\"   Scope 2 difference: {diff_scope2:+,.0f} ({diff_pct_scope2:+.1f}%)\")\n",
    "    \n",
    "    if diff_pct_scope1 > 10:\n",
    "        print(f\"   ⚠️ Large gap companies emit {abs(diff_pct_scope1):.0f}% MORE\")\n",
    "    elif diff_pct_scope1 > 0:\n",
    "        print(f\"   ⚠️ Weak effect ({diff_pct_scope1:+.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   ❌ Gap companies emit LESS\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 3: SCORE QUARTILE ANALYSIS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Analyze by environmental score quartiles\n",
    "train_df['env_score_quartile'] = pd.qcut(train_df['environmental_score'], q=4, \n",
    "                                          labels=['Q1-Best', 'Q2-Good', 'Q3-Poor', 'Q4-Worst'])\n",
    "\n",
    "print(\"\\n📊 Emissions by Environmental Score Quartile:\")\n",
    "print(\"   (Q1=Best environmental performance, Q4=Worst)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "env_quartile_analysis = train_df.groupby('env_score_quartile').agg({\n",
    "    'environmental_score': 'mean',\n",
    "    'governance_score': 'mean',\n",
    "    'target_scope_1': ['mean', 'median', 'count'],\n",
    "    'target_scope_2': ['mean', 'median']\n",
    "}).round(0)\n",
    "\n",
    "env_quartile_analysis.columns = ['Env_Score', 'Gov_Score', 'Scope1_Mean', 'Scope1_Median', \n",
    "                                  'Count', 'Scope2_Mean', 'Scope2_Median']\n",
    "\n",
    "print(env_quartile_analysis)\n",
    "\n",
    "# Check if worse environmental scores → higher emissions\n",
    "q1_scope1 = env_quartile_analysis.loc['Q1-Best', 'Scope1_Mean']\n",
    "q4_scope1 = env_quartile_analysis.loc['Q4-Worst', 'Scope1_Mean']\n",
    "\n",
    "print(f\"\\n💡 ENVIRONMENTAL SCORE EFFECT:\")\n",
    "print(f\"   Best env performance (Q1): {q1_scope1:,.0f} Scope 1\")\n",
    "print(f\"   Worst env performance (Q4): {q4_scope1:,.0f} Scope 1\")\n",
    "\n",
    "if q4_scope1 > q1_scope1:\n",
    "    increase = ((q4_scope1 - q1_scope1) / q1_scope1 * 100)\n",
    "    print(f\"   ✅ Worst performers emit {increase:+.0f}% MORE - expected relationship!\")\n",
    "else:\n",
    "    print(f\"   ❌ Unexpected: Worst performers emit LESS\")\n",
    "\n",
    "# Analyze by governance score quartiles\n",
    "train_df['gov_score_quartile'] = pd.qcut(train_df['governance_score'], q=4,\n",
    "                                          labels=['Q1-Best', 'Q2-Good', 'Q3-Poor', 'Q4-Worst'])\n",
    "\n",
    "print(\"\\n📊 Emissions by Governance Score Quartile:\")\n",
    "print(\"   (Q1=Best governance, Q4=Worst)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "gov_quartile_analysis = train_df.groupby('gov_score_quartile').agg({\n",
    "    'governance_score': 'mean',\n",
    "    'environmental_score': 'mean',\n",
    "    'target_scope_1': ['mean', 'median', 'count'],\n",
    "    'target_scope_2': ['mean', 'median']\n",
    "}).round(0)\n",
    "\n",
    "gov_quartile_analysis.columns = ['Gov_Score', 'Env_Score', 'Scope1_Mean', 'Scope1_Median',\n",
    "                                  'Count', 'Scope2_Mean', 'Scope2_Median']\n",
    "\n",
    "print(gov_quartile_analysis)\n",
    "\n",
    "# Check if better governance → lower emissions\n",
    "q1_gov_scope1 = gov_quartile_analysis.loc['Q1-Best', 'Scope1_Mean']\n",
    "q4_gov_scope1 = gov_quartile_analysis.loc['Q4-Worst', 'Scope1_Mean']\n",
    "\n",
    "print(f\"\\n💡 GOVERNANCE SCORE EFFECT:\")\n",
    "print(f\"   Best governance (Q1): {q1_gov_scope1:,.0f} Scope 1\")\n",
    "print(f\"   Worst governance (Q4): {q4_gov_scope1:,.0f} Scope 1\")\n",
    "\n",
    "if q4_gov_scope1 < q1_gov_scope1:\n",
    "    decrease = ((q1_gov_scope1 - q4_gov_scope1) / q1_gov_scope1 * 100)\n",
    "    print(f\"   ⚠️ Paradox: Better governance → {decrease:.0f}% HIGHER emissions!\")\n",
    "    print(f\"   (Likely confounded by company size)\")\n",
    "else:\n",
    "    print(f\"   ✅ Expected: Worse governance → higher emissions\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEST 4: STATISTICAL SIGNIFICANCE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# T-test: Large gap vs balanced\n",
    "if len(large_positive_gap) > 1 and len(small_gap) > 1:\n",
    "    t_stat, p_val = stats.ttest_ind(\n",
    "        large_positive_gap['target_scope_1'].dropna(),\n",
    "        small_gap['target_scope_1'].dropna()\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📊 T-Test: Large Gap vs Balanced (Scope 1)\")\n",
    "    print(f\"   T-statistic: {t_stat:.4f}\")\n",
    "    print(f\"   P-value: {p_val:.6f}\")\n",
    "    \n",
    "    if p_val < 0.05:\n",
    "        print(f\"   ✅ SIGNIFICANT difference (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"   ❌ Not significant (p >= 0.05)\")\n",
    "\n",
    "# ANOVA: Environmental score quartiles\n",
    "env_groups = [group['target_scope_1'].dropna().values \n",
    "              for name, group in train_df.groupby('env_score_quartile')]\n",
    "\n",
    "if len(env_groups) == 4:\n",
    "    f_stat_env, p_val_env = stats.f_oneway(*env_groups)\n",
    "    \n",
    "    print(f\"\\n📊 ANOVA: Environmental Score Quartiles (Scope 1)\")\n",
    "    print(f\"   F-statistic: {f_stat_env:.4f}\")\n",
    "    print(f\"   P-value: {p_val_env:.6f}\")\n",
    "    \n",
    "    if p_val_env < 0.05:\n",
    "        print(f\"   ✅ Environmental score SIGNIFICANTLY affects emissions (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"   ❌ Not significant (p >= 0.05)\")\n",
    "\n",
    "# ANOVA: Governance score quartiles\n",
    "gov_groups = [group['target_scope_1'].dropna().values\n",
    "              for name, group in train_df.groupby('gov_score_quartile')]\n",
    "\n",
    "if len(gov_groups) == 4:\n",
    "    f_stat_gov, p_val_gov = stats.f_oneway(*gov_groups)\n",
    "    \n",
    "    print(f\"\\n📊 ANOVA: Governance Score Quartiles (Scope 1)\")\n",
    "    print(f\"   F-statistic: {f_stat_gov:.4f}\")\n",
    "    print(f\"   P-value: {p_val_gov:.6f}\")\n",
    "    \n",
    "    if p_val_gov < 0.05:\n",
    "        print(f\"   ✅ Governance score SIGNIFICANTLY affects emissions (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"   ❌ Not significant (p >= 0.05)\")\n",
    "\n",
    "# VISUALIZATIONS\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Gov-Env Gap vs Scope 1\n",
    "axes[0, 0].scatter(train_df['gov_env_gap'], train_df['target_scope_1'], alpha=0.5, s=20)\n",
    "axes[0, 0].set_xlabel('Governance - Environment Score Gap')\n",
    "axes[0, 0].set_ylabel('Scope 1 Emissions')\n",
    "axes[0, 0].set_title(f'Gov-Env Gap vs Scope 1\\n(r={scope1_esg_corrs.get(\"gov_env_gap\", 0):.3f})', \n",
    "                     fontweight='bold')\n",
    "axes[0, 0].axvline(0, color='red', linestyle='--', alpha=0.5, label='No gap')\n",
    "axes[0, 0].axvline(0.5, color='orange', linestyle='--', alpha=0.5, label='Large gap')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Environmental Score vs Scope 1\n",
    "axes[0, 1].scatter(train_df['environmental_score'], train_df['target_scope_1'], \n",
    "                   alpha=0.5, s=20, color='green')\n",
    "axes[0, 1].set_xlabel('Environmental Score (1=Best, 5=Worst)')\n",
    "axes[0, 1].set_ylabel('Scope 1 Emissions')\n",
    "axes[0, 1].set_title(f'Environmental Score vs Scope 1\\n(r={scope1_esg_corrs.get(\"environmental_score\", 0):.3f})', \n",
    "                     fontweight='bold')\n",
    "\n",
    "# 3. Governance Score vs Scope 1\n",
    "axes[0, 2].scatter(train_df['governance_score'], train_df['target_scope_1'],\n",
    "                   alpha=0.5, s=20, color='purple')\n",
    "axes[0, 2].set_xlabel('Governance Score (1=Best, 5=Worst)')\n",
    "axes[0, 2].set_ylabel('Scope 1 Emissions')\n",
    "axes[0, 2].set_title(f'Governance Score vs Scope 1\\n(r={scope1_esg_corrs.get(\"governance_score\", 0):.3f})',\n",
    "                     fontweight='bold')\n",
    "\n",
    "# 4. Gap categories comparison\n",
    "if len(large_positive_gap) > 0 and len(small_gap) > 0 and len(large_negative_gap) > 0:\n",
    "    categories = ['Env >> Gov\\n(gap<-0.5)', 'Balanced\\n(|gap|≤0.5)', 'Gov >> Env\\n(gap>0.5)']\n",
    "    scope1_means = [large_negative_gap['target_scope_1'].mean(),\n",
    "                    small_gap['target_scope_1'].mean(),\n",
    "                    large_positive_gap['target_scope_1'].mean()]\n",
    "    scope2_means = [large_negative_gap['target_scope_2'].mean(),\n",
    "                    small_gap['target_scope_2'].mean(),\n",
    "                    large_positive_gap['target_scope_2'].mean()]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1, 0].bar(x - width/2, scope1_means, width, label='Scope 1', \n",
    "                   color='steelblue', edgecolor='black')\n",
    "    axes[1, 0].bar(x + width/2, scope2_means, width, label='Scope 2',\n",
    "                   color='orange', edgecolor='black')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(categories)\n",
    "    axes[1, 0].set_ylabel('Average Emissions')\n",
    "    axes[1, 0].set_title('Emissions by Gov-Env Gap Category', fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "# 5. Environmental score quartiles\n",
    "env_quartiles = ['Q1-Best', 'Q2-Good', 'Q3-Poor', 'Q4-Worst']\n",
    "env_scope1 = env_quartile_analysis['Scope1_Mean'].values\n",
    "env_scope2 = env_quartile_analysis['Scope2_Mean'].values\n",
    "\n",
    "x = np.arange(len(env_quartiles))\n",
    "axes[1, 1].bar(x - width/2, env_scope1, width, label='Scope 1',\n",
    "               color='steelblue', edgecolor='black')\n",
    "axes[1, 1].bar(x + width/2, env_scope2, width, label='Scope 2',\n",
    "               color='orange', edgecolor='black')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(env_quartiles, rotation=45, ha='right')\n",
    "axes[1, 1].set_ylabel('Average Emissions')\n",
    "axes[1, 1].set_title('Emissions by Environmental Score\\n(1=Best, 5=Worst)', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# 6. Governance score quartiles\n",
    "gov_quartiles = ['Q1-Best', 'Q2-Good', 'Q3-Poor', 'Q4-Worst']\n",
    "gov_scope1 = gov_quartile_analysis['Scope1_Mean'].values\n",
    "gov_scope2 = gov_quartile_analysis['Scope2_Mean'].values\n",
    "\n",
    "x = np.arange(len(gov_quartiles))\n",
    "axes[1, 2].bar(x - width/2, gov_scope1, width, label='Scope 1',\n",
    "               color='steelblue', edgecolor='black')\n",
    "axes[1, 2].bar(x + width/2, gov_scope2, width, label='Scope 2',\n",
    "               color='orange', edgecolor='black')\n",
    "axes[1, 2].set_xticks(x)\n",
    "axes[1, 2].set_xticklabels(gov_quartiles, rotation=45, ha='right')\n",
    "axes[1, 2].set_ylabel('Average Emissions')\n",
    "axes[1, 2].set_title('Emissions by Governance Score\\n(1=Best, 5=Worst)', fontweight='bold')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURES CREATED (VALID FOR TEST SET)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "✅ environmental_score (r={scope1_esg_corrs.get('environmental_score', 0):.3f} for Scope 1)\n",
    "   - Weak but expected direction\n",
    "   - Independent of emissions\n",
    "\n",
    "✅ abs_gov_env_gap (r={scope1_esg_corrs.get('abs_gov_env_gap', 0):.3f} for Scope 1)\n",
    "   - Magnitude of G-E misalignment\n",
    "   - Best ESG predictor\n",
    "\n",
    "⚠️ governance_score (r={scope1_esg_corrs.get('governance_score', 0):.3f} for Scope 1)\n",
    "   - NEGATIVE correlation (paradox - confounded by size)\n",
    "   - Use with caution\n",
    "\n",
    "⚠️ environmental_score_squared (r={scope1_esg_corrs.get('environmental_score_squared', 0):.3f} for Scope 1)\n",
    "   - Non-linear effect\n",
    "   - Marginal improvement\n",
    "\n",
    "❌ overall_score, social_score (r<0.1)\n",
    "   - Too weak to be useful\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c97012d-e563-4a9b-af3e-b2ddba8bbd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS 11: GOVERNANCE GAPS SIGNAL UNMANAGED CARBON RISK - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Key metrics from results\n",
    "abs_gap_corr_s1 = 0.202\n",
    "abs_gap_corr_s2 = 0.150\n",
    "gov_env_gap_corr_s1 = -0.167\n",
    "gov_env_gap_corr_s2 = -0.147\n",
    "\n",
    "env_score_corr_s1 = 0.120\n",
    "env_score_corr_s2 = 0.049\n",
    "env_squared_corr_s1 = 0.141\n",
    "env_squared_corr_s2 = 0.051\n",
    "\n",
    "gov_score_corr_s1 = -0.113\n",
    "gov_score_corr_s2 = -0.167\n",
    "\n",
    "overall_corr_s1 = 0.072\n",
    "social_corr_s1 = 0.046\n",
    "\n",
    "env_quartile_increase = 164\n",
    "gov_quartile_decrease = 48\n",
    "gap_increase = 132.6\n",
    "\n",
    "p_val_gap = 0.088182\n",
    "p_val_env = 0.004822\n",
    "p_val_gov = 0.077698\n",
    "\n",
    "pct_positive_gap = 7.7\n",
    "pct_negative_gap = 92.3\n",
    "\n",
    "# Determine validation status\n",
    "status = \"❌ NOT VALIDATED (WEAK & PARADOXICAL)\"\n",
    "\n",
    "summary = f\"\"\"\n",
    "\n",
    "HYPOTHESIS 11: GOVERNANCE GAPS SIGNAL UNMANAGED CARBON RISK {status}\n",
    "\n",
    "KEY FINDINGS:\n",
    "1. Hypothesis CONTRADICTED - gap works OPPOSITE to prediction:\n",
    "   - Gov-Env gap vs Scope 1: r={gov_env_gap_corr_s1:.3f} (NEGATIVE, not positive!)\n",
    "   - Prediction: Positive gap (Gov > Env) → higher emissions\n",
    "   - Reality: Positive gap → LOWER emissions (paradox)\n",
    "   → Original hypothesis is WRONG\n",
    "   \n",
    "2. ABSOLUTE gap shows expected pattern (but weak):\n",
    "   - |Gov-Env gap| vs Scope 1: r={abs_gap_corr_s1:.3f} ⭐⭐\n",
    "   - |Gov-Env gap| vs Scope 2: r={abs_gap_corr_s2:.3f} ⭐⭐\n",
    "   → ANY misalignment (G-E imbalance) → slightly higher emissions\n",
    "   → Magnitude matters, not direction\n",
    "   \n",
    "3. Environmental score shows expected relationship (but very weak):\n",
    "   - Env score vs Scope 1: r={env_score_corr_s1:.3f} ⭐⭐\n",
    "   - Worse env score → higher emissions (Q4 emits +{env_quartile_increase}% vs Q1)\n",
    "   - ANOVA: p={p_val_env:.6f} ✅ SIGNIFICANT\n",
    "   → Expected direction confirmed\n",
    "   \n",
    "4. Governance score shows PARADOXICAL inverse relationship:\n",
    "   - Gov score vs Scope 1: r={gov_score_corr_s1:.3f} (NEGATIVE)\n",
    "   - Gov score vs Scope 2: r={gov_score_corr_s2:.3f} (NEGATIVE, even stronger!)\n",
    "   - Best governance (Q1): 75,410 Scope 1\n",
    "   - Worst governance (Q4): 39,297 Scope 1 (48% LOWER!)\n",
    "   → BETTER governance = HIGHER emissions (size confounding)\n",
    "   \n",
    "5. Overall ESG scores are VERY WEAK predictors:\n",
    "   - Best ESG feature: abs_gap r={abs_gap_corr_s1:.3f} (explains ~4% variance)\n",
    "   - All other features: r<0.15 (explains <2% variance)\n",
    "   - NOT statistically significant (gap p={p_val_gap:.3f}, gov p={p_val_gov:.3f})\n",
    "   → ESG scores add minimal predictive value\n",
    "\n",
    "INTERPRETATION - WHY THE PARADOXES EXIST:\n",
    "\n",
    "**1. Why does Gov-Env gap work OPPOSITE to prediction?**\n",
    "\n",
    "Companies with Gov >> Env (positive gap, n=11, {pct_positive_gap:.1f}%):\n",
    "- Avg Gov score: 2.84 (moderate)\n",
    "- Avg Env score: 1.81 (EXCELLENT! Remember: 1=best, 5=worst)\n",
    "- Avg Scope 1: 73,975\n",
    "\n",
    "Wait - these companies have EXCELLENT environmental performance (1.81)!\n",
    "- The \"gap\" means governance is WORSE than environment\n",
    "- NOT that governance is good and environment is bad\n",
    "- These are companies with GREAT environmental scores but only moderate governance\n",
    "\n",
    "**Score Scale Confusion:**\n",
    "- 1 = BEST, 5 = WORST\n",
    "- Gov score 2.84 > Env score 1.81 means:\n",
    "  * Governance is WORSE (higher number = worse)\n",
    "  * Environment is BETTER (lower number = better)\n",
    "- So \"positive gap\" = better environmental performance!\n",
    "\n",
    "**2. Why does better governance correlate with HIGHER emissions?**\n",
    "\n",
    "This is a SELECTION BIAS / SIZE CONFOUNDING effect:\n",
    "\n",
    "1. **Large companies have better governance:**\n",
    "   - Professional management → better governance scores\n",
    "   - Larger companies → more absolute emissions (scale effect)\n",
    "   - Correlation is SPURIOUS (driven by company size)\n",
    "   \n",
    "2. **Better governance = better reporting:**\n",
    "   - Companies with good governance accurately report ALL emissions\n",
    "   - Poor governance → incomplete/inaccurate reporting\n",
    "   - Higher reported emissions ≠ worse actual performance\n",
    "   \n",
    "3. **High-emission sectors invest in governance:**\n",
    "   - Manufacturing, energy face regulatory scrutiny\n",
    "   - They invest MORE in governance to manage risk\n",
    "   - Better governance is RESPONSE to emissions, not cause\n",
    "\n",
    "**3. Why does ABSOLUTE gap matter?**\n",
    "\n",
    "|Gap| correlation (r={abs_gap_corr_s1:.3f}) suggests:\n",
    "- Misalignment between G and E indicates organizational issues\n",
    "- Could be:\n",
    "  * Inconsistent priorities across departments\n",
    "  * Siloed decision-making\n",
    "  * Lack of integrated ESG strategy\n",
    "  * Resource allocation imbalances\n",
    "- ANY imbalance → slightly higher emissions\n",
    "- But effect is WEAK (only 4% variance explained)\n",
    "\n",
    "VALIDATION OF SPECIFIC PREDICTIONS:\n",
    "  ❌ \"Gov-Env gap (Gov > Env) → higher emissions\"\n",
    "     → CONTRADICTED: r={gov_env_gap_corr_s1:.3f} (negative, not positive!)\n",
    "     → Positive gap actually means BETTER environmental performance\n",
    "     → Not statistically significant (p={p_val_gap:.3f})\n",
    "     \n",
    "  ⚠️ \"Environmental scores → positive correlation\"\n",
    "     → CONFIRMED but WEAK: r={env_score_corr_s1:.3f}, Q4 emits +{env_quartile_increase}% vs Q1\n",
    "     → Statistically significant (p={p_val_env:.6f})\n",
    "     → But explains only ~1.4% variance\n",
    "     \n",
    "  ❌ \"Governance scores → negative correlation\"\n",
    "     → CONTRADICTED: r={gov_score_corr_s1:.3f} (OPPOSITE sign!)\n",
    "     → Better governance → HIGHER emissions (paradox)\n",
    "     → Not statistically significant (p={p_val_gov:.3f})\n",
    "     → Confounded by company size\n",
    "     \n",
    "  ⚠️ \"Absolute gap magnitude matters\"\n",
    "     → CONFIRMED but WEAK: |Gap| r={abs_gap_corr_s1:.3f}\n",
    "     → ANY misalignment → higher emissions\n",
    "     → But only explains ~4% variance\n",
    "\n",
    "RECOMMENDED FEATURES FOR FINAL MODEL:\n",
    "  Priority 3 (OPTIONAL - very weak, likely SKIP):\n",
    "    ⚠️ abs_gov_env_gap (r={abs_gap_corr_s1:.3f}) - best ESG feature but weak\n",
    "    ⚠️ environmental_score (r={env_score_corr_s1:.3f}) - weak but significant\n",
    "    ⚠️ environmental_score_squared (r={env_squared_corr_s1:.3f}) - marginal\n",
    "    \n",
    "  DO NOT USE (paradoxical/too weak):\n",
    "    ❌ governance_score (r={gov_score_corr_s1:.3f}) - confounded, paradoxical\n",
    "    ❌ gov_env_gap (r={gov_env_gap_corr_s1:.3f}) - works opposite to theory\n",
    "    ❌ overall_score (r={overall_corr_s1:.3f}) - too weak\n",
    "    ❌ social_score (r={social_corr_s1:.3f}) - too weak\n",
    "    ❌ gov_env_ratio (similar issues to gov_env_gap)\n",
    "    \n",
    "  Expected R² contribution: ~4% (if using abs_gap)\n",
    "  Expected R² contribution: ~1-2% (if using env_score)\n",
    "\n",
    "MODELING IMPLICATIONS:\n",
    "  1. ESG scores have MINIMAL predictive power:\n",
    "     - Strongest: abs_gap r={abs_gap_corr_s1:.3f} (explains ~4% variance)\n",
    "     - Environmental: r={env_score_corr_s1:.3f} (explains ~1.4% variance)\n",
    "     - Combined ESG contribution: ~5% R² AT MOST\n",
    "     - Sector + Revenue already explain ~45-50%\n",
    "     \n",
    "  2. Severe confounding issues:\n",
    "     - Don't use governance_score (size confounded)\n",
    "     - If including ESG, MUST control for company size\n",
    "     - Better governance ≠ lower emissions (paradox)\n",
    "     \n",
    "  3. Environmental score is most trustworthy:\n",
    "     - Directly measures environmental performance\n",
    "     - Expected relationship (worse score → more emissions)\n",
    "     - Statistically significant (p={p_val_env:.6f})\n",
    "     - But still very weak (r={env_score_corr_s1:.3f})\n",
    "     \n",
    "  4. **RECOMMENDATION: SKIP ESG FEATURES ENTIRELY**\n",
    "     - Add <5% marginal value\n",
    "     - Sector + Revenue + Geography already explain 45-50%\n",
    "     - Not worth complexity for weak signals\n",
    "     - Focus on stronger predictors\n",
    "\n",
    "VISUAL EVIDENCE:\n",
    "  - Gov-Env gap scatter: NEGATIVE trend (opposite to hypothesis!)\n",
    "  - Environmental score: Positive trend but VERY scattered (r=0.120)\n",
    "  - Governance score: NEGATIVE trend (paradox confirmed)\n",
    "  - Gap categories: Positive gap (Gov>>Env) emits MORE (but n=11 tiny sample!)\n",
    "  - Env quartiles: Clear increase Q1→Q4 (as expected, +{env_quartile_increase}%)\n",
    "  - Gov quartiles: DECREASE Q1→Q4 (paradox: -{gov_quartile_decrease}%)\n",
    "\n",
    "UNEXPECTED FINDINGS:\n",
    "  1. **{pct_negative_gap:.1f}% of companies have Env score WORSE than Gov score:**\n",
    "     - Most companies: good governance, poor environmental performance\n",
    "     - Only {pct_positive_gap:.1f}% have Gov > Env (positive gap)\n",
    "     - Dataset heavily skewed toward poor environmental performers\n",
    "     \n",
    "  2. **Best governance quartile has HIGHEST emissions:**\n",
    "     - Q1 (best gov): 75,410 Scope 1\n",
    "     - Q4 (worst gov): 39,297 Scope 1\n",
    "     - Nearly 2x difference in WRONG direction!\n",
    "     - Clear size confounding effect\n",
    "     \n",
    "  3. **Scope 2 shows STRONGER governance effect:**\n",
    "     - Gov score vs Scope 2: r={gov_score_corr_s2:.3f}\n",
    "     - Gov score vs Scope 1: r={gov_score_corr_s1:.3f}\n",
    "     - Better governance → MUCH lower Scope 2\n",
    "     - Suggests governance affects energy purchasing decisions\n",
    "     - But still paradoxical (better gov = higher reported emissions)\n",
    "     \n",
    "  4. **Environmental score² has HIGHER correlation than linear:**\n",
    "     - Env² vs Scope 1: r={env_squared_corr_s1:.3f}\n",
    "     - Env vs Scope 1: r={env_score_corr_s1:.3f}\n",
    "     - Non-linear: very poor environmental scores → disproportionately high emissions\n",
    "     - But still too weak to be useful (r<0.15)\n",
    "\n",
    "  5. **Gap effect not statistically significant:**\n",
    "     - T-test (large gap vs balanced): p={p_val_gap:.3f}\n",
    "     - Despite 133% difference in means!\n",
    "     - High variance and small sample (n=11 positive gap)\n",
    "     - Can't reliably distinguish gap categories\n",
    "\n",
    "CRITICAL LIMITATIONS:\n",
    "  1. **Reverse causality:** Poor governance → high emissions? Or high emissions → better governance (to manage risk)?\n",
    "  2. **Selection bias:** Good governance → better reporting accuracy\n",
    "  3. **Size confounding:** Large companies have better governance AND more emissions\n",
    "  4. **Small samples:** Only {pct_positive_gap:.1f}% have Gov > Env (n=33)\n",
    "  5. **Score validity:** ESG scores are subjective, non-standardized across providers\n",
    "  6. **Weak signals:** All correlations r<0.21 (very weak predictive power)\n",
    "\n",
    "REVISED HYPOTHESIS:\n",
    "  Original: \"Gov-Env gap signals unmanaged carbon risk\"\n",
    "  \n",
    "  Revised: \"ESG scores have minimal predictive value for emissions;\n",
    "            absolute G-E misalignment shows weak positive association\"\n",
    "  \n",
    "  Better formulation:\n",
    "  - ABSOLUTE gap (|Gov - Env|) → slightly higher emissions\n",
    "  - Not about WHICH is better, but about ALIGNMENT\n",
    "  - But effect is VERY WEAK (r={abs_gap_corr_s1:.3f})\n",
    "  - Not worth including in model given stronger alternatives\n",
    "\n",
    "REAL-WORLD INTERPRETATION:\n",
    "\n",
    "**The 3 Gap Categories (Exploratory):**\n",
    "\n",
    "1. **Env >> Gov (gap < -0.5, n=322, 75%)**:\n",
    "   - Poor governance (2.25), VERY POOR environment (3.67)\n",
    "   - Avg emissions: 62,262 Scope 1\n",
    "   - Typical: Large manufacturing companies with high emissions\n",
    "   - They KNOW they have problems (reporting them)\n",
    "   \n",
    "2. **Balanced (|gap| ≤ 0.5, n=96, 22%)**:\n",
    "   - Governance and environmental performance aligned\n",
    "   - Avg emissions: 31,800 Scope 1 (LOWEST!)\n",
    "   - Companies with consistent ESG priorities\n",
    "   \n",
    "3. **Gov >> Env (gap > 0.5, n=11, 3%)**:\n",
    "   - Moderate governance (2.84), EXCELLENT environment (1.81)\n",
    "   - Avg emissions: 73,975 Scope 1\n",
    "   - Rare: Companies with great environmental performance\n",
    "\n",
    "**Balanced companies emit LEAST → Alignment may matter slightly**\n",
    "But difference is not statistically significant (p={p_val_gap:.3f})\n",
    "\n",
    "DATASET CHARACTERISTICS:\n",
    "- Mean gap: -1.08 (most companies: Env worse than Gov)\n",
    "- Only {pct_positive_gap:.1f}% have positive gap (Gov > Env)\n",
    "- Environmental scores are generally POOR (mean=3.46, where 5=worst)\n",
    "- Governance scores are generally MODERATE (mean=2.38)\n",
    "- Most companies struggle with environmental performance more than governance\n",
    "\n",
    "CONCLUSION:\n",
    "  ❌ HYPOTHESIS NOT VALIDATED\n",
    "  \n",
    "  Original predictions WRONG:\n",
    "  - Gov > Env gap does NOT → higher emissions (works opposite!)\n",
    "  - Gap correlation has WRONG sign (r={gov_env_gap_corr_s1:.3f})\n",
    "  - Governance paradox: better gov = higher emissions (confounded)\n",
    "  \n",
    "  What DOES work (but weak):\n",
    "  ⚠️ Absolute gap magnitude → higher emissions (r={abs_gap_corr_s1:.3f})\n",
    "  ⚠️ Environmental score → higher emissions (r={env_score_corr_s1:.3f})\n",
    "  ❌ But both are TOO WEAK to be useful (<5% R²)\n",
    "  \n",
    "  **PRACTICAL RECOMMENDATION:**\n",
    "  - **SKIP ESG FEATURES ENTIRELY**\n",
    "  - They add <5% marginal R² improvement\n",
    "  - Sector + Revenue + Geography explain 45-50% already\n",
    "  - Not worth complexity given weak, paradoxical signals\n",
    "  - Focus on validated hypotheses (H1, H2, H9, H10)\n",
    "  - ESG scores are not good emission predictors\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ESG FEATURES SUMMARY TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "Feature                          │ Scope 1  │ Scope 2  │ Priority │ Use?\n",
    "─────────────────────────────────┼──────────┼──────────┼──────────┼──────────\n",
    "abs_gov_env_gap                  │  {abs_gap_corr_s1:.3f}   │  {abs_gap_corr_s2:.3f}   │ P3-SKIP  │ ❌ NO (too weak)\n",
    "environmental_score              │  {env_score_corr_s1:.3f}   │  {env_score_corr_s2:.3f}   │ P3-SKIP  │ ❌ NO (too weak)\n",
    "environmental_score_squared      │  {env_squared_corr_s1:.3f}   │  {env_squared_corr_s2:.3f}   │ P3-SKIP  │ ❌ NO (marginal)\n",
    "governance_score                 │ {gov_score_corr_s1:.3f}   │ {gov_score_corr_s2:.3f}   │ SKIP     │ ❌ NO (paradox)\n",
    "gov_env_gap                      │ {gov_env_gap_corr_s1:.3f}   │ {gov_env_gap_corr_s2:.3f}   │ SKIP     │ ❌ NO (wrong sign)\n",
    "overall_score                    │  {overall_corr_s1:.3f}   │ -0.007   │ SKIP     │ ❌ NO (too weak)\n",
    "social_score                     │  {social_corr_s1:.3f}   │  0.034   │ SKIP     │ ❌ NO (too weak)\n",
    "─────────────────────────────────┴──────────┴──────────┴──────────┴──────────\n",
    "\n",
    "KEY STATISTICS:\n",
    "  Best ESG feature: abs_gov_env_gap (r={abs_gap_corr_s1:.3f})\n",
    "  Expected R² contribution: ~4% (0.202² = 0.041)\n",
    "  \n",
    "  Gap distribution:\n",
    "    Gov > Env (positive): {pct_positive_gap:.1f}% (n=33) ⚠️ TINY SAMPLE\n",
    "    Balanced: 22.4% (n=96)\n",
    "    Env > Gov (negative): {pct_negative_gap:.1f}% (n=322) - MAJORITY\n",
    "    \n",
    "  Statistical significance:\n",
    "    Gap t-test: p={p_val_gap:.3f} ❌ NOT SIGNIFICANT\n",
    "    Environmental ANOVA: p={p_val_env:.6f} ✅ SIGNIFICANT\n",
    "    Governance ANOVA: p={p_val_gov:.3f} ❌ NOT SIGNIFICANT\n",
    "    \n",
    "  Emission differences (exploratory):\n",
    "    Best env (Q1) → Worst env (Q4): +{env_quartile_increase}% ✅\n",
    "    Best gov (Q1) → Worst gov (Q4): -{gov_quartile_decrease}% ❌ (paradox!)\n",
    "    Balanced → Large gap: +{gap_increase:.1f}% (not significant)\n",
    "\n",
    "⚠️ FINAL RECOMMENDATION: SKIP ALL ESG FEATURES\n",
    "   - All features r<0.21 (very weak)\n",
    "   - Governance paradox (confounded by size)\n",
    "   - Gap works opposite to hypothesis\n",
    "   - Not statistically reliable\n",
    "   - Focus on sector + revenue + geography instead\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d6254e-ae16-4022-b210-d36109e3a823",
   "metadata": {},
   "source": [
    "### Train Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a2839-0fff-4e04-8e89-034c1e62ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CREATING FINAL FEATURE DATAFRAME FROM TRAIN_DF\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "Goal: Extract all validated features from train_df (Hypotheses 3, 9, 11)\n",
    "Output: DataFrame ready for modeling\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: CREATE ALL VALIDATED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Start with entity_id\n",
    "train_features_final = train_df[['entity_id']].copy()\n",
    "\n",
    "print(\"\\n🔧 Creating features from Hypothesis 3 (Geography)...\")\n",
    "\n",
    "# Geographic features - Frequency encoding (VALID)\n",
    "if 'country_frequency' not in train_df.columns:\n",
    "    country_freq = train_df['country_name'].value_counts()\n",
    "    train_df['country_frequency'] = train_df['country_name'].map(country_freq)\n",
    "    print(\"   ✓ Created country_frequency\")\n",
    "\n",
    "if 'region_frequency' not in train_df.columns:\n",
    "    region_freq = train_df['region_name'].value_counts()\n",
    "    train_df['region_frequency'] = train_df['region_name'].map(region_freq)\n",
    "    print(\"   ✓ Created region_frequency\")\n",
    "\n",
    "# Add to final features\n",
    "train_features_final['country_frequency'] = train_df['country_frequency']\n",
    "train_features_final['region_frequency'] = train_df['region_frequency']\n",
    "\n",
    "# Categorical (optional - can use for one-hot encoding later)\n",
    "train_features_final['country_name'] = train_df['country_name']\n",
    "train_features_final['region_name'] = train_df['region_name']\n",
    "\n",
    "print(f\"   ✅ Added 4 geographic features\")\n",
    "\n",
    "print(\"\\n🔧 Creating features from Hypothesis 9 (Scale)...\")\n",
    "\n",
    "# Scale features - Log revenue\n",
    "if 'log_revenue' not in train_df.columns:\n",
    "    train_df['log_revenue'] = np.log1p(train_df['revenue'])\n",
    "    print(\"   ✓ Created log_revenue\")\n",
    "\n",
    "train_features_final['log_revenue'] = train_df['log_revenue']\n",
    "\n",
    "# Country-adjusted scale (Z-score)\n",
    "if 'revenue_country_zscore' not in train_df.columns:\n",
    "    train_df['revenue_country_zscore'] = train_df.groupby('country_name')['revenue'].transform(\n",
    "        lambda x: (x - x.mean()) / (x.std() + 1)\n",
    "    )\n",
    "    print(\"   ✓ Created revenue_country_zscore\")\n",
    "\n",
    "train_features_final['revenue_country_zscore'] = train_df['revenue_country_zscore']\n",
    "\n",
    "# Binary giant indicator\n",
    "if 'is_giant_in_country' not in train_df.columns:\n",
    "    train_df['is_giant_in_country'] = (train_df['revenue_country_zscore'] > 1).astype(int)\n",
    "    print(\"   ✓ Created is_giant_in_country\")\n",
    "\n",
    "train_features_final['is_giant_in_country'] = train_df['is_giant_in_country']\n",
    "\n",
    "print(f\"   ✅ Added 3 scale features\")\n",
    "\n",
    "print(\"\\n🔧 Creating features from Hypothesis 11 (ESG - OPTIONAL)...\")\n",
    "\n",
    "# ESG features (weak, likely skip, but available if needed)\n",
    "\n",
    "# Gov-Env gap\n",
    "if 'gov_env_gap' not in train_df.columns:\n",
    "    train_df['gov_env_gap'] = train_df['governance_score'] - train_df['environmental_score']\n",
    "    print(\"   ✓ Created gov_env_gap\")\n",
    "\n",
    "if 'abs_gov_env_gap' not in train_df.columns:\n",
    "    train_df['abs_gov_env_gap'] = abs(train_df['gov_env_gap'])\n",
    "    print(\"   ✓ Created abs_gov_env_gap\")\n",
    "\n",
    "# ESG scores\n",
    "if 'environmental_score_squared' not in train_df.columns:\n",
    "    train_df['environmental_score_squared'] = train_df['environmental_score'] ** 2\n",
    "    print(\"   ✓ Created environmental_score_squared\")\n",
    "\n",
    "# Add ESG features (marked as optional)\n",
    "train_features_final['abs_gov_env_gap'] = train_df['abs_gov_env_gap']\n",
    "train_features_final['environmental_score'] = train_df['environmental_score']\n",
    "train_features_final['environmental_score_squared'] = train_df['environmental_score_squared']\n",
    "train_features_final['governance_score'] = train_df['governance_score']\n",
    "train_features_final['social_score'] = train_df['social_score']\n",
    "train_features_final['overall_score'] = train_df['overall_score']\n",
    "\n",
    "print(f\"   ⚠️ Added 6 ESG features (WEAK - recommend skipping)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: FEATURE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📊 Final feature dataframe shape: {train_features_final.shape}\")\n",
    "print(f\"   Rows (entities): {len(train_features_final)}\")\n",
    "print(f\"   Columns (features + ID): {len(train_features_final.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"FEATURE BREAKDOWN BY PRIORITY\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "priority_1_features = [\n",
    "    'log_revenue'\n",
    "]\n",
    "\n",
    "priority_2_features = [\n",
    "    'country_frequency',\n",
    "    'region_frequency',\n",
    "    'revenue_country_zscore',\n",
    "    'is_giant_in_country'\n",
    "]\n",
    "\n",
    "priority_3_categorical = [\n",
    "    'country_name',\n",
    "    'region_name'\n",
    "]\n",
    "\n",
    "priority_3_skip = [\n",
    "    'abs_gov_env_gap',\n",
    "    'environmental_score',\n",
    "    'environmental_score_squared',\n",
    "    'governance_score',\n",
    "    'social_score',\n",
    "    'overall_score'\n",
    "]\n",
    "\n",
    "print(\"\\n✅ PRIORITY 1 (MUST INCLUDE - already from revenue distribution):\")\n",
    "for feat in priority_1_features:\n",
    "    if feat in train_features_final.columns:\n",
    "        print(f\"   • {feat:40s} (r=0.275)\")\n",
    "\n",
    "print(\"\\n✅ PRIORITY 2 (SHOULD INCLUDE - moderate value):\")\n",
    "for feat in priority_2_features:\n",
    "    if feat in train_features_final.columns:\n",
    "        dtype_str = str(train_features_final[feat].dtype)  # FIXED: Convert to string\n",
    "        if feat == 'country_frequency':\n",
    "            corr_info = \"r=0.118\"\n",
    "        elif feat == 'region_frequency':\n",
    "            corr_info = \"r=-0.142 (skip)\"\n",
    "        elif feat == 'revenue_country_zscore':\n",
    "            corr_info = \"r=0.192\"\n",
    "        elif feat == 'is_giant_in_country':\n",
    "            corr_info = \"binary alternative\"\n",
    "        else:\n",
    "            corr_info = \"\"\n",
    "        print(f\"   • {feat:40s} {dtype_str:12s} {corr_info}\")\n",
    "\n",
    "print(\"\\n⚠️ PRIORITY 3 (OPTIONAL - categorical, need encoding):\")\n",
    "for feat in priority_3_categorical:\n",
    "    if feat in train_features_final.columns:\n",
    "        n_unique = train_features_final[feat].nunique()\n",
    "        print(f\"   • {feat:40s} ({n_unique} categories)\")\n",
    "\n",
    "print(\"\\n❌ PRIORITY 3 (SKIP - too weak / paradoxical):\")\n",
    "for feat in priority_3_skip:\n",
    "    if feat in train_features_final.columns:\n",
    "        if feat == 'abs_gov_env_gap':\n",
    "            corr_info = \"r=0.202 (weak)\"\n",
    "        elif feat == 'environmental_score':\n",
    "            corr_info = \"r=0.120 (very weak)\"\n",
    "        elif feat == 'environmental_score_squared':\n",
    "            corr_info = \"r=0.141 (marginal)\"\n",
    "        elif feat == 'governance_score':\n",
    "            corr_info = \"r=-0.113 (paradox)\"\n",
    "        elif feat == 'social_score':\n",
    "            corr_info = \"r=0.046 (too weak)\"\n",
    "        elif feat == 'overall_score':\n",
    "            corr_info = \"r=0.072 (too weak)\"\n",
    "        else:\n",
    "            corr_info = \"\"\n",
    "        print(f\"   • {feat:40s} {corr_info}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: DATA QUALITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📊 Missing values:\")\n",
    "missing_summary = train_features_final.isnull().sum()\n",
    "if missing_summary.sum() == 0:\n",
    "    print(\"   ✅ No missing values!\")\n",
    "else:\n",
    "    print(missing_summary[missing_summary > 0])\n",
    "\n",
    "print(f\"\\n📊 Data types:\")\n",
    "print(train_features_final.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\n📊 Sample of first 5 rows:\")\n",
    "print(train_features_final.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: RECOMMENDED FEATURE SUBSETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n💡 MINIMAL FEATURE SET (Recommended for initial modeling):\")\n",
    "minimal_features = [\n",
    "    'entity_id',\n",
    "    'log_revenue',\n",
    "    'country_frequency',\n",
    "    'revenue_country_zscore'\n",
    "]\n",
    "print(f\"   Features: {minimal_features}\")\n",
    "print(f\"   Count: {len(minimal_features)-1} features\")\n",
    "print(f\"   Expected R²: ~10-12%\")\n",
    "\n",
    "print(\"\\n💡 STANDARD FEATURE SET (Balanced performance/complexity):\")\n",
    "standard_features = [\n",
    "    'entity_id',\n",
    "    'log_revenue',\n",
    "    'country_frequency',\n",
    "    'revenue_country_zscore',\n",
    "    'is_giant_in_country',\n",
    "    'region_name'\n",
    "]\n",
    "print(f\"   Features: {standard_features}\")\n",
    "print(f\"   Count: {len(standard_features)-1} features + one-hot region (7 categories)\")\n",
    "print(f\"   Expected R²: ~12-15%\")\n",
    "\n",
    "print(\"\\n💡 FULL FEATURE SET (Maximum information, may overfit):\")\n",
    "full_features = [\n",
    "    'entity_id',\n",
    "    'log_revenue',\n",
    "    'country_frequency',\n",
    "    'revenue_country_zscore',\n",
    "    'is_giant_in_country',\n",
    "    'country_name',\n",
    "    'region_name',\n",
    "    'abs_gov_env_gap',\n",
    "    'environmental_score'\n",
    "]\n",
    "print(f\"   Features: {full_features}\")\n",
    "print(f\"   Count: {len(full_features)-1} features + categorical encoding\")\n",
    "print(f\"   Expected R²: ~15-18%\")\n",
    "print(f\"   ⚠️ Warning: ESG features very weak, may not improve performance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: CREATE FEATURE SUBSETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create the three subsets\n",
    "train_features_minimal = train_features_final[minimal_features].copy()\n",
    "train_features_standard = train_features_final[standard_features].copy()\n",
    "train_features_full = train_features_final[full_features].copy()\n",
    "\n",
    "print(f\"\\n✅ Created 3 feature subsets:\")\n",
    "print(f\"   • train_features_minimal:  {train_features_minimal.shape}\")\n",
    "print(f\"   • train_features_standard: {train_features_standard.shape}\")\n",
    "print(f\"   • train_features_full:     {train_features_full.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ FEATURE DATAFRAMES READY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "Available DataFrames:\n",
    "\n",
    "1. train_features_final ({train_features_final.shape})\n",
    "   - All features from Hypotheses 3, 9, 11\n",
    "   - {len(train_features_final.columns)-1} total features\n",
    "   - Ready for modeling\n",
    "\n",
    "2. train_features_minimal ({train_features_minimal.shape})\n",
    "   - Best core features only\n",
    "   - {len(train_features_minimal.columns)-1} features\n",
    "   - Recommended for initial baseline\n",
    "\n",
    "3. train_features_standard ({train_features_standard.shape})\n",
    "   - Balanced feature set\n",
    "   - {len(train_features_standard.columns)-1} features + region encoding\n",
    "   - Recommended for production\n",
    "\n",
    "4. train_features_full ({train_features_full.shape})\n",
    "   - Maximum information\n",
    "   - {len(train_features_full.columns)-1} features + categorical encoding\n",
    "   - Use if model capacity allows\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE CORRELATION SUMMARY (from hypothesis testing)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "correlation_summary = pd.DataFrame({\n",
    "    'Feature': [\n",
    "        'log_revenue',\n",
    "        'revenue_country_zscore',\n",
    "        'country_frequency',\n",
    "        'region_frequency',\n",
    "        'is_giant_in_country',\n",
    "        'abs_gov_env_gap',\n",
    "        'environmental_score',\n",
    "        'environmental_score_squared',\n",
    "        'governance_score',\n",
    "        'social_score',\n",
    "        'overall_score'\n",
    "    ],\n",
    "    'Scope_1_Corr': [0.275, 0.192, 0.118, -0.142, '?', 0.202, 0.120, 0.141, -0.113, 0.046, 0.072],\n",
    "    'Scope_2_Corr': [0.211, 0.217, 0.113, -0.153, '?', 0.150, 0.049, 0.051, -0.167, 0.034, -0.007],\n",
    "    'Priority': ['P1', 'P2', 'P2', 'SKIP', 'P2', 'P3-Skip', 'P3-Skip', 'P3-Skip', 'SKIP', 'SKIP', 'SKIP'],\n",
    "    'Recommendation': [\n",
    "        'MUST INCLUDE',\n",
    "        'SHOULD INCLUDE',\n",
    "        'SHOULD INCLUDE',\n",
    "        'SKIP (negative)',\n",
    "        'ALTERNATIVE to Z-score',\n",
    "        'SKIP (too weak)',\n",
    "        'SKIP (too weak)',\n",
    "        'SKIP (marginal)',\n",
    "        'SKIP (paradox)',\n",
    "        'SKIP (too weak)',\n",
    "        'SKIP (too weak)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\")\n",
    "print(correlation_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb9c78-1225-483b-a208-479418dab620",
   "metadata": {},
   "source": [
    "## Environmental Activity EDA & Feature Engineering\n",
    "**For this portion of Environmental Activity analysis, please refer the details at \"test notebooks/EnvAct2_EDA&Features.ipynb\" file, we will import our final dataset we extract from the Environmental Activity table below for training Purpose.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec91aab-325d-4df9-b8c2-318193bab36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "environmental_activities_feature_df = pd.read_csv('../test notebooks/env_act_relatedFeatures.csv')\n",
    "environmental_activities_feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623464ea-85ae-4413-aefb-25986d46c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "environmental_activities_feature_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6258470-5f17-4501-9ee5-7d16e410cfb1",
   "metadata": {},
   "source": [
    "## Sustainable Goal EDA & Feature Engineering\n",
    "**For this portion please refer to \"SDG.ipynb\" file, we will import our final dataset we extract from the Environmental Activity table below for training Purpose.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f145c00f-62d4-4122-a1a9-99f458b2df9e",
   "metadata": {},
   "source": [
    "## Model Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f417b21f-6f51-4b1f-b13f-1fd179b39c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff48ebeb-92c4-40ea-96d1-6628d28e8569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 10b: APPLY DUAN'S SMEARING CORRECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 10b: APPLYING DUAN'S SMEARING CORRECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def duans_smearing_correction(y_true_log, y_pred_log):\n",
    "    \"\"\"Apply Duan's smearing correction for log-transformed data\"\"\"\n",
    "    residuals = y_true_log - y_pred_log\n",
    "    smearing_factor = np.mean(np.expm1(residuals))\n",
    "    return smearing_factor\n",
    "\n",
    "# Calculate correction factors on training data\n",
    "smearing_scope1 = duans_smearing_correction(y1_train, y1_train_pred)\n",
    "smearing_scope2 = duans_smearing_correction(y2_train, y2_train_pred)\n",
    "\n",
    "print(f\"Scope 1 smearing factor: {smearing_scope1:.4f}\")\n",
    "print(f\"Scope 2 smearing factor: {smearing_scope2:.4f}\")\n",
    "\n",
    "# Apply correction to validation predictions\n",
    "y1_val_pred_orig_corrected = np.expm1(y1_val_pred) * smearing_scope1\n",
    "y2_val_pred_orig_corrected = np.expm1(y2_val_pred) * smearing_scope2\n",
    "\n",
    "print(\"\\n📊 CORRECTED PERFORMANCE (ORIGINAL SCALE):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Scope 1 R² (corrected):  {r2_score(y1_val_orig, y1_val_pred_orig_corrected):.4f}\")\n",
    "print(f\"Scope 2 R² (corrected):  {r2_score(y2_val_orig, y2_val_pred_orig_corrected):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc3cc6-2184-4ac3-924e-00314c74de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CORRECT MERGE: FIXING THE log_revenue DUPLICATE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "PROBLEM IDENTIFIED:\n",
    "===================\n",
    "\n",
    "Revenue distribution features:\n",
    "  • entity_id\n",
    "  • log_revenue  ← DUPLICATE!\n",
    "  • revenue_x_high_emission_pct\n",
    "  • high_emission_pct\n",
    "  • manufacturing_pct\n",
    "  • is_manufacturing\n",
    "  • dominant_nace_l1\n",
    "  • is_mining\n",
    "  • mining_pct\n",
    "  • log_revenue_x_high_emission_pct\n",
    "  • revenue_x_manufacturing_pct\n",
    "\n",
    "Train features:\n",
    "  • entity_id\n",
    "  • log_revenue  ← DUPLICATE!\n",
    "  • country_frequency\n",
    "  • revenue_country_zscore\n",
    "  • is_giant_in_country\n",
    "  • region_name\n",
    "\n",
    "SOLUTION: Drop log_revenue from train_features_standard before merge\n",
    "          (Keep the one from revenue distribution)\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: CHECK CURRENT DATAFRAMES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: VERIFY SOURCE DATAFRAMES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📊 final_revenue_features_df:\")\n",
    "print(f\"   Shape: {final_revenue_features_df.shape}\")\n",
    "print(f\"   Columns: {list(final_revenue_features_df.columns)}\")\n",
    "\n",
    "print(f\"\\n📊 train_features_standard:\")\n",
    "print(f\"   Shape: {train_features_standard.shape}\")\n",
    "print(f\"   Columns: {list(train_features_standard.columns)}\")\n",
    "\n",
    "# Identify overlap\n",
    "overlap = set(final_revenue_features_df.columns) & set(train_features_standard.columns)\n",
    "overlap.discard('entity_id')  # entity_id should overlap for merge\n",
    "\n",
    "print(f\"\\n⚠️ Overlapping columns (besides entity_id): {overlap}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: CLEAN MERGE - DROP DUPLICATE FROM TRAIN FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: CLEAN MERGE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Drop log_revenue from train_features_standard (keep revenue distribution version)\n",
    "train_features_no_dup = train_features_standard.drop(columns=['log_revenue'], errors='ignore')\n",
    "\n",
    "print(f\"\\n✅ Removed duplicate from train_features_standard:\")\n",
    "print(f\"   Original columns: {list(train_features_standard.columns)}\")\n",
    "print(f\"   After removing log_revenue: {list(train_features_no_dup.columns)}\")\n",
    "\n",
    "# Merge\n",
    "combined_features_fixed = final_revenue_features_df.merge(\n",
    "    train_features_no_dup,\n",
    "    on='entity_id',\n",
    "    how='inner',\n",
    "    validate='1:1'\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Merge successful!\")\n",
    "print(f\"   Revenue features: {final_revenue_features_df.shape}\")\n",
    "print(f\"   Train features (no dup): {train_features_no_dup.shape}\")\n",
    "print(f\"   Combined: {combined_features_fixed.shape}\")\n",
    "\n",
    "# Verify perfect merge\n",
    "if len(combined_features_fixed) == len(final_revenue_features_df) == len(train_features_no_dup):\n",
    "    print(f\"\\n✅ Perfect merge - all {len(combined_features_fixed)} entities matched!\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Merge issue - check entity_id alignment\")\n",
    "\n",
    "# Check for any remaining duplicates\n",
    "duplicate_cols = combined_features_fixed.columns[combined_features_fixed.columns.duplicated()].tolist()\n",
    "if duplicate_cols:\n",
    "    print(f\"\\n❌ Still have duplicate columns: {duplicate_cols}\")\n",
    "else:\n",
    "    print(f\"\\n✅ No duplicate columns!\")\n",
    "\n",
    "print(f\"\\n📊 COMBINED FEATURES ({len(combined_features_fixed.columns)} columns):\")\n",
    "print(list(combined_features_fixed.columns))\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: ADD TARGETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: ADD TARGETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Add targets\n",
    "combined_features_fixed = combined_features_fixed.merge(\n",
    "    train_df[['entity_id', 'target_scope_1', 'target_scope_2']],\n",
    "    on='entity_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Log-transform targets\n",
    "combined_features_fixed['log_target_scope_1'] = np.log1p(combined_features_fixed['target_scope_1'])\n",
    "combined_features_fixed['log_target_scope_2'] = np.log1p(combined_features_fixed['target_scope_2'])\n",
    "\n",
    "print(f\"\\n✅ Targets added\")\n",
    "print(f\"   Shape: {combined_features_fixed.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: ENCODE CATEGORICAL FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: ONE-HOT ENCODE CATEGORICAL FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = []\n",
    "\n",
    "if 'region_name' in combined_features_fixed.columns:\n",
    "    categorical_cols.append('region_name')\n",
    "    print(f\"   • region_name: {combined_features_fixed['region_name'].nunique()} categories\")\n",
    "    \n",
    "if 'dominant_nace_l1' in combined_features_fixed.columns:\n",
    "    categorical_cols.append('dominant_nace_l1')\n",
    "    print(f\"   • dominant_nace_l1: {combined_features_fixed['dominant_nace_l1'].nunique()} categories\")\n",
    "\n",
    "if categorical_cols:\n",
    "    print(f\"\\n🔧 One-hot encoding: {categorical_cols}\")\n",
    "    \n",
    "    encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "    encoded_cats = encoder.fit_transform(combined_features_fixed[categorical_cols])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    encoded_col_names = encoder.get_feature_names_out(categorical_cols)\n",
    "    encoded_df = pd.DataFrame(\n",
    "        encoded_cats,\n",
    "        columns=encoded_col_names,\n",
    "        index=combined_features_fixed.index\n",
    "    )\n",
    "    \n",
    "    # Drop original categorical and add encoded\n",
    "    combined_features_fixed = combined_features_fixed.drop(columns=categorical_cols).join(encoded_df)\n",
    "    \n",
    "    print(f\"   ✅ Created {len(encoded_col_names)} dummy variables\")\n",
    "    print(f\"   New shape: {combined_features_fixed.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: DEFINE FINAL FEATURE SET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: DEFINE FEATURE SET FOR MODELING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Exclude ID and targets\n",
    "exclude_cols = ['entity_id', 'target_scope_1', 'target_scope_2', \n",
    "                'log_target_scope_1', 'log_target_scope_2']\n",
    "\n",
    "# All feature columns\n",
    "feature_cols_final = [col for col in combined_features_fixed.columns \n",
    "                      if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\n📊 TOTAL FEATURES: {len(feature_cols_final)}\")\n",
    "\n",
    "# Verify no duplicates\n",
    "from collections import Counter\n",
    "feature_counts = Counter(feature_cols_final)\n",
    "duplicates = {col: count for col, count in feature_counts.items() if count > 1}\n",
    "\n",
    "if duplicates:\n",
    "    print(f\"\\n❌ ERROR: Found duplicates in feature list:\")\n",
    "    for col, count in duplicates.items():\n",
    "        print(f\"   • {col}: {count} times\")\n",
    "else:\n",
    "    print(f\"\\n✅ NO DUPLICATES in feature list!\")\n",
    "\n",
    "# Categorize features for clarity\n",
    "print(\"\\n📊 FEATURE BREAKDOWN:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Revenue & scale features\n",
    "revenue_features = [col for col in feature_cols_final if 'revenue' in col.lower()]\n",
    "print(f\"\\n1. REVENUE & SCALE ({len(revenue_features)} features):\")\n",
    "for feat in revenue_features:\n",
    "    print(f\"   • {feat}\")\n",
    "\n",
    "# Sector features\n",
    "sector_features = [col for col in feature_cols_final if any(x in col.lower() \n",
    "                   for x in ['emission', 'manufacturing', 'mining', 'nace'])]\n",
    "print(f\"\\n2. SECTOR COMPOSITION ({len(sector_features)} features):\")\n",
    "for feat in sector_features[:15]:  # Show first 15\n",
    "    print(f\"   • {feat}\")\n",
    "if len(sector_features) > 15:\n",
    "    print(f\"   ... and {len(sector_features)-15} more\")\n",
    "\n",
    "# Geographic features\n",
    "geo_features = [col for col in feature_cols_final if any(x in col.lower() \n",
    "                for x in ['country', 'region'])]\n",
    "print(f\"\\n3. GEOGRAPHIC ({len(geo_features)} features):\")\n",
    "for feat in geo_features:\n",
    "    print(f\"   • {feat}\")\n",
    "\n",
    "# Binary flags\n",
    "binary_features = [col for col in feature_cols_final if col.startswith('is_')]\n",
    "print(f\"\\n4. BINARY FLAGS ({len(binary_features)} features):\")\n",
    "for feat in binary_features:\n",
    "    print(f\"   • {feat}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"FEATURE SUMMARY:\")\n",
    "print(f\"  • Revenue/Scale:  {len(revenue_features)}\")\n",
    "print(f\"  • Sector:         {len(sector_features)}\")\n",
    "print(f\"  • Geographic:     {len(geo_features)}\")\n",
    "print(f\"  • Binary flags:   {len(binary_features)}\")\n",
    "print(f\"  • TOTAL:          {len(feature_cols_final)}\")\n",
    "print(f\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: PREPARE X AND Y\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: PREPARE X (FEATURES) AND Y (TARGETS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Features\n",
    "X_final = combined_features_fixed[feature_cols_final].copy()\n",
    "\n",
    "# Check for issues\n",
    "print(f\"\\n📊 Data quality check:\")\n",
    "missing = X_final.isnull().sum().sum()\n",
    "print(f\"   Missing values: {missing}\")\n",
    "\n",
    "if missing > 0:\n",
    "    print(f\"   Columns with missing values:\")\n",
    "    missing_cols = X_final.isnull().sum()\n",
    "    print(missing_cols[missing_cols > 0])\n",
    "    print(f\"\\n   Filling with median...\")\n",
    "    X_final = X_final.fillna(X_final.median())\n",
    "\n",
    "inf_count = np.isinf(X_final.values).sum()\n",
    "print(f\"   Infinite values: {inf_count}\")\n",
    "\n",
    "if inf_count > 0:\n",
    "    print(f\"   Replacing infinite values with median...\")\n",
    "    X_final = X_final.replace([np.inf, -np.inf], np.nan).fillna(X_final.median())\n",
    "\n",
    "# Targets (log-transformed)\n",
    "y_scope1_final = combined_features_fixed['log_target_scope_1'].copy()\n",
    "y_scope2_final = combined_features_fixed['log_target_scope_2'].copy()\n",
    "\n",
    "print(f\"\\n✅ Data prepared:\")\n",
    "print(f\"   X shape: {X_final.shape}\")\n",
    "print(f\"   y_scope1 shape: {y_scope1_final.shape}\")\n",
    "print(f\"   y_scope2 shape: {y_scope2_final.shape}\")\n",
    "print(f\"   Features: {len(feature_cols_final)} (NO DUPLICATES ✅)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: TRAIN-VALIDATION SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 7: SPLIT DATA (80% TRAIN, 20% VALIDATION)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_final, X_val_final, y1_train_final, y1_val_final, y2_train_final, y2_val_final = train_test_split(\n",
    "    X_final, y_scope1_final, y_scope2_final,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Split complete:\")\n",
    "print(f\"   Train: {X_train_final.shape[0]} samples × {X_train_final.shape[1]} features\")\n",
    "print(f\"   Validation: {X_val_final.shape[0]} samples × {X_val_final.shape[1]} features\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: TRAIN RANDOM FOREST MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 8: TRAIN RANDOM FOREST MODELS (CLEAN - NO DUPLICATES)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "print(\"\\n🌲 Training Random Forest for Scope 1...\")\n",
    "\n",
    "rf_scope1_final = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "rf_scope1_final.fit(X_train_final, y1_train_final)\n",
    "print(\"   ✅ Scope 1 model trained!\")\n",
    "\n",
    "print(\"\\n🌲 Training Random Forest for Scope 2...\")\n",
    "\n",
    "rf_scope2_final = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "rf_scope2_final.fit(X_train_final, y2_train_final)\n",
    "print(\"   ✅ Scope 2 model trained!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: EVALUATE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 9: EVALUATE MODEL PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Predictions\n",
    "y1_train_pred_final = rf_scope1_final.predict(X_train_final)\n",
    "y1_val_pred_final = rf_scope1_final.predict(X_val_final)\n",
    "\n",
    "y2_train_pred_final = rf_scope2_final.predict(X_train_final)\n",
    "y2_val_pred_final = rf_scope2_final.predict(X_val_final)\n",
    "\n",
    "# Performance on log scale\n",
    "print(\"\\n📊 PERFORMANCE (LOG SCALE):\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nSCOPE 1:\")\n",
    "print(f\"  Train R²:       {r2_score(y1_train_final, y1_train_pred_final):.4f}\")\n",
    "print(f\"  Validation R²:  {r2_score(y1_val_final, y1_val_pred_final):.4f}\")\n",
    "print(f\"  Train RMSE:     {np.sqrt(mean_squared_error(y1_train_final, y1_train_pred_final)):.4f}\")\n",
    "print(f\"  Val RMSE:       {np.sqrt(mean_squared_error(y1_val_final, y1_val_pred_final)):.4f}\")\n",
    "print(f\"  Val MAE:        {mean_absolute_error(y1_val_final, y1_val_pred_final):.4f}\")\n",
    "\n",
    "print(\"\\nSCOPE 2:\")\n",
    "print(f\"  Train R²:       {r2_score(y2_train_final, y2_train_pred_final):.4f}\")\n",
    "print(f\"  Validation R²:  {r2_score(y2_val_final, y2_val_pred_final):.4f}\")\n",
    "print(f\"  Train RMSE:     {np.sqrt(mean_squared_error(y2_train_final, y2_train_pred_final)):.4f}\")\n",
    "print(f\"  Val RMSE:       {np.sqrt(mean_squared_error(y2_val_final, y2_val_pred_final)):.4f}\")\n",
    "print(f\"  Val MAE:        {mean_absolute_error(y2_val_final, y2_val_pred_final):.4f}\")\n",
    "\n",
    "# Convert to original scale\n",
    "y1_train_orig_final = np.expm1(y1_train_final)\n",
    "y1_val_orig_final = np.expm1(y1_val_final)\n",
    "y1_train_pred_orig_final = np.expm1(y1_train_pred_final)\n",
    "y1_val_pred_orig_final = np.expm1(y1_val_pred_final)\n",
    "\n",
    "y2_train_orig_final = np.expm1(y2_train_final)\n",
    "y2_val_orig_final = np.expm1(y2_val_final)\n",
    "y2_train_pred_orig_final = np.expm1(y2_train_pred_final)\n",
    "y2_val_pred_orig_final = np.expm1(y2_val_pred_final)\n",
    "\n",
    "print(\"\\n📊 PERFORMANCE (ORIGINAL SCALE - NAIVE):\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nSCOPE 1:\")\n",
    "print(f\"  Train R²:       {r2_score(y1_train_orig_final, y1_train_pred_orig_final):.4f}\")\n",
    "print(f\"  Validation R²:  {r2_score(y1_val_orig_final, y1_val_pred_orig_final):.4f}\")\n",
    "print(f\"  Train RMSE:     {np.sqrt(mean_squared_error(y1_train_orig_final, y1_train_pred_orig_final)):,.0f}\")\n",
    "print(f\"  Val RMSE:       {np.sqrt(mean_squared_error(y1_val_orig_final, y1_val_pred_orig_final)):,.0f}\")\n",
    "\n",
    "print(\"\\nSCOPE 2:\")\n",
    "print(f\"  Train R²:       {r2_score(y2_train_orig_final, y2_train_pred_orig_final):.4f}\")\n",
    "print(f\"  Validation R²:  {r2_score(y2_val_orig_final, y2_val_pred_orig_final):.4f}\")\n",
    "print(f\"  Train RMSE:     {np.sqrt(mean_squared_error(y2_train_orig_final, y2_train_pred_orig_final)):,.0f}\")\n",
    "print(f\"  Val RMSE:       {np.sqrt(mean_squared_error(y2_val_orig_final, y2_val_pred_orig_final)):,.0f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 10: FEATURE IMPORTANCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 10: FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "importance_scope1_final = pd.DataFrame({\n",
    "    'feature': feature_cols_final,\n",
    "    'importance': rf_scope1_final.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n📊 TOP 15 FEATURES FOR SCOPE 1:\")\n",
    "print(\"-\" * 80)\n",
    "print(importance_scope1_final.head(15).to_string(index=False))\n",
    "\n",
    "importance_scope2_final = pd.DataFrame({\n",
    "    'feature': feature_cols_final,\n",
    "    'importance': rf_scope2_final.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n📊 TOP 15 FEATURES FOR SCOPE 2:\")\n",
    "print(\"-\" * 80)\n",
    "print(importance_scope2_final.head(15).to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ CLEAN MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "FINAL SUMMARY:\n",
    "==============\n",
    "\n",
    "Data:\n",
    "  ✅ Merged features properly (NO duplicates)\n",
    "  ✅ {len(combined_features_fixed)} entities\n",
    "  ✅ {len(feature_cols_final)} unique features\n",
    "  \n",
    "Features:\n",
    "  • Revenue/Scale:  {len(revenue_features)}\n",
    "  • Sector:         {len(sector_features)}\n",
    "  • Geographic:     {len(geo_features)}\n",
    "  • Binary flags:   {len(binary_features)}\n",
    "\n",
    "Models Trained:\n",
    "  ✅ Random Forest for Scope 1 (100 trees)\n",
    "  ✅ Random Forest for Scope 2 (100 trees)\n",
    "\n",
    "Performance (Validation, Log Scale):\n",
    "  • Scope 1 R²: {r2_score(y1_val_final, y1_val_pred_final):.4f}\n",
    "  • Scope 2 R²: {r2_score(y2_val_final, y2_val_pred_final):.4f}\n",
    "\n",
    "Performance (Validation, Original Scale):\n",
    "  • Scope 1 R²: {r2_score(y1_val_orig_final, y1_val_pred_orig_final):.4f}\n",
    "  • Scope 2 R²: {r2_score(y2_val_orig_final, y2_val_pred_orig_final):.4f}\n",
    "\n",
    "Top 3 Features (Scope 1):\n",
    "  1. {importance_scope1_final.iloc[0]['feature']}: {importance_scope1_final.iloc[0]['importance']:.4f}\n",
    "  2. {importance_scope1_final.iloc[1]['feature']}: {importance_scope1_final.iloc[1]['importance']:.4f}\n",
    "  3. {importance_scope1_final.iloc[2]['feature']}: {importance_scope1_final.iloc[2]['importance']:.4f}\n",
    "\n",
    "Saved Objects:\n",
    "  • combined_features_fixed (complete dataset)\n",
    "  • X_final, y_scope1_final, y_scope2_final (clean data)\n",
    "  • rf_scope1_final, rf_scope2_final (trained models)\n",
    "  • feature_cols_final (list of {len(feature_cols_final)} unique features)\n",
    "  • encoder (for categorical encoding)\n",
    "\n",
    "Next Steps:\n",
    "  1. ✅ Duplicates fixed!\n",
    "  2. 📊 Models trained with clean features\n",
    "  3. 🚀 Ready for test predictions\n",
    "  4. ⚡ Optional: Try hyperparameter tuning\n",
    "  5. 🎯 Optional: Try ensemble methods\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cb63fa-6304-472e-9f05-67945f001dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
