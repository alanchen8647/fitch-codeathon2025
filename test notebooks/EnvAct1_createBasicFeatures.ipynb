{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41ef09fe-cf32-4025-88cd-c21596985fe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "env_activities_df = pd.read_csv('./data/environmental_activities.csv')\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sector_df = pd.read_csv('./data/revenue_distribution_by_sector.csv')\n",
    "sdg_df = pd.read_csv('./data/sustainable_development_goals.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc3a2068-fb61-4d8e-a770-b9bdbc31b0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating comprehensive feature set...\n",
      "Total features created: 97\n",
      "Feature columns: 94\n",
      "\n",
      "Feature categories created:\n",
      "- Geographic features: 39\n",
      "- Sector features: 22\n",
      "- Environmental features: 19\n",
      "- SDG features: 2\n",
      "- Revenue features: 4\n",
      "- Sustainability features: 5\n"
     ]
    }
   ],
   "source": [
    "def create_comprehensive_features(train_data, sector_data, env_data, sdg_data):\n",
    "    \"\"\"Create a comprehensive set of engineered features\"\"\"\n",
    "    \n",
    "    # Start with base features\n",
    "    features = train_data.copy()\n",
    "    \n",
    "    # 1. Geographic Features (One-hot encoding)\n",
    "    if 'region_code' in features.columns:\n",
    "        region_dummies = pd.get_dummies(features['region_code'], prefix='region')\n",
    "        features = pd.concat([features, region_dummies], axis=1)\n",
    "    \n",
    "    # Country diversity (simplified)\n",
    "    if 'country_code' in features.columns:\n",
    "        country_dummies = pd.get_dummies(features['country_code'], prefix='country')\n",
    "        features = pd.concat([features, country_dummies], axis=1)\n",
    "    \n",
    "    # 2. Revenue-based features\n",
    "    if 'revenue' in features.columns:\n",
    "        features['log_revenue'] = np.log1p(features['revenue'])\n",
    "        features['revenue_millions'] = features['revenue'] / 1e6\n",
    "        features['revenue_squared'] = features['revenue'] ** 2\n",
    "    \n",
    "    # 3. Sustainability score interactions\n",
    "    score_columns = ['environmental_score', 'social_score', 'governance_score', 'overall_score']\n",
    "    available_scores = [col for col in score_columns if col in features.columns]\n",
    "    \n",
    "    if 'environmental_score' in features.columns and 'governance_score' in features.columns:\n",
    "        features['env_gov_interaction'] = features['environmental_score'] * features['governance_score']\n",
    "    \n",
    "    if 'overall_score' in features.columns and 'environmental_score' in features.columns:\n",
    "        # Avoid division by zero\n",
    "        features['overall_env_ratio'] = features['overall_score'] / features['environmental_score'].replace(0, np.nan)\n",
    "        features['overall_env_ratio'] = features['overall_env_ratio'].fillna(0)\n",
    "    \n",
    "    if all(score in features.columns for score in ['environmental_score', 'social_score', 'governance_score']):\n",
    "        features['weighted_sustainability'] = (\n",
    "            0.45 * features['environmental_score'] + \n",
    "            0.30 * features['social_score'] + \n",
    "            0.25 * features['governance_score']\n",
    "        )\n",
    "    \n",
    "    # 4. Sector-based features\n",
    "    if not sector_data.empty and 'entity_id' in sector_data.columns:\n",
    "        sector_pivot = sector_data.pivot_table(\n",
    "            values='revenue_pct',\n",
    "            index='entity_id',\n",
    "            columns='nace_level_1_code',\n",
    "            aggfunc='sum',\n",
    "            fill_value=0\n",
    "        ).add_prefix('sector_')\n",
    "        \n",
    "        # Sector diversity metrics\n",
    "        sector_counts = sector_data.groupby('entity_id').size().rename('sector_diversity')\n",
    "        sector_max_pct = sector_data.groupby('entity_id')['revenue_pct'].max().rename('max_sector_concentration')\n",
    "        \n",
    "        def calculate_entropy(x):\n",
    "            # Add small epsilon to avoid log(0)\n",
    "            x_normalized = x / (x.sum() + 1e-10)\n",
    "            return -np.sum(x_normalized * np.log(x_normalized + 1e-10))\n",
    "        \n",
    "        sector_entropy = sector_data.groupby('entity_id')['revenue_pct'].apply(\n",
    "            calculate_entropy\n",
    "        ).rename('sector_entropy')\n",
    "        \n",
    "        # Merge sector features\n",
    "        sector_features = [sector_pivot, sector_counts, sector_max_pct, sector_entropy]\n",
    "        for sector_feat in sector_features:\n",
    "            features = features.merge(sector_feat, left_on='entity_id', right_index=True, how='left')\n",
    "    \n",
    "   # 5. Environmental activities features\n",
    "    if not env_data.empty and 'entity_id' in env_data.columns:\n",
    "    \n",
    "        # Clean activity_type to avoid hidden characters\n",
    "        env_data = env_data.copy()\n",
    "        env_data['activity_type'] = (\n",
    "            env_data['activity_type']\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.replace('\\xa0', '', regex=False)\n",
    "        )\n",
    "    \n",
    "        # Aggregations\n",
    "        env_agg = env_data.groupby('entity_id').agg({\n",
    "            'env_score_adjustment': ['sum', 'mean', 'count', 'std']\n",
    "        }).fillna(0)\n",
    "        env_agg.columns = ['env_adj_sum', 'env_adj_mean', 'env_activities_count', 'env_adj_std']\n",
    "    \n",
    "        features = features.merge(env_agg, left_on='entity_id', right_index=True, how='left')\n",
    "    \n",
    "        # Unique activity types\n",
    "        env_types = env_data.groupby('entity_id')['activity_type'].nunique().rename('env_activity_types')\n",
    "        features = features.merge(env_types, left_on='entity_id', right_index=True, how='left')\n",
    "    \n",
    "        # Harmful / beneficial sums and counts\n",
    "        env_harmful = env_data[env_data['env_score_adjustment'] > 0].groupby('entity_id')['env_score_adjustment'].sum().rename('env_adj_harmful_sum')\n",
    "        env_beneficial = env_data[env_data['env_score_adjustment'] < 0].groupby('entity_id')['env_score_adjustment'].sum().rename('env_adj_beneficial_sum')\n",
    "        env_num_harmful = (env_data['env_score_adjustment'] > 0).groupby(env_data['entity_id']).sum().rename('env_num_harmful')\n",
    "        env_num_beneficial = (env_data['env_score_adjustment'] < 0).groupby(env_data['entity_id']).sum().rename('env_num_beneficial')\n",
    "    \n",
    "        env_net = env_harmful.add(env_beneficial, fill_value=0).rename('env_adj_net')\n",
    "    \n",
    "        for env_feat in [env_harmful, env_beneficial, env_net, env_num_harmful, env_num_beneficial]:\n",
    "            features = features.merge(env_feat, left_on='entity_id', right_index=True, how='left')\n",
    "    \n",
    "        # NEW: One-hot encode activity_type and aggregate per company\n",
    "        activity_dummies = pd.get_dummies(\n",
    "            env_data['activity_type'],\n",
    "            prefix='env_acttype',\n",
    "            dtype=int\n",
    "        )\n",
    "\n",
    "    # Add entity_id back to align\n",
    "    activity_dummies = pd.concat([env_data[['entity_id']], activity_dummies], axis=1)\n",
    "\n",
    "    # Sum per company (so multiple activities add up)\n",
    "    activity_counts = activity_dummies.groupby('entity_id').sum()\n",
    "\n",
    "    #  Avoid column overlap when merging\n",
    "    non_overlapping_cols = [col for col in activity_counts.columns if col not in features.columns]\n",
    "\n",
    "    activity_counts = activity_counts[non_overlapping_cols]\n",
    "\n",
    "    features = features.merge(activity_counts, left_on='entity_id', right_index=True, how='left')\n",
    "    \n",
    "    # 6. SDG features\n",
    "    if not sdg_data.empty and 'entity_id' in sdg_data.columns:\n",
    "        sdg_agg = sdg_data.groupby('entity_id').agg({\n",
    "            'sdg_id': ['count', 'nunique']\n",
    "        }).fillna(0)\n",
    "        sdg_agg.columns = ['sdg_commitments', 'unique_sdgs']\n",
    "        \n",
    "        # Climate-related SDGs (6, 7, 13, 14, 15)\n",
    "        if 'sdg_id' in sdg_data.columns:\n",
    "            climate_sdgs = sdg_data[sdg_data['sdg_id'].isin([6, 7, 13, 14, 15])]\n",
    "            climate_sdg_count = climate_sdgs.groupby('entity_id').size().rename('climate_sdg_count')\n",
    "            features = features.merge(climate_sdg_count, left_on='entity_id', right_index=True, how='left')\n",
    "        \n",
    "        features = features.merge(sdg_agg, left_on='entity_id', right_index=True, how='left')\n",
    "    \n",
    "    # Fill missing values for all newly created columns\n",
    "    # Identify newly created feature columns (excluding original train_data columns)\n",
    "    original_columns = set(train_data.columns)\n",
    "    new_columns = [col for col in features.columns if col not in original_columns]\n",
    "    \n",
    "    # Fill only the new feature columns with 0\n",
    "    features[new_columns] = features[new_columns].fillna(0)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "# Create comprehensive feature set\n",
    "print(\"Creating comprehensive feature set...\")\n",
    "feature_data = create_comprehensive_features(train_df, sector_df, env_activities_df, sdg_df)\n",
    "\n",
    "print(f\"Total features created: {feature_data.shape[1]}\")\n",
    "print(f\"Feature columns: {len([col for col in feature_data.columns if col not in ['entity_id', 'target_scope_1', 'target_scope_2']])}\")\n",
    "\n",
    "# Display feature summary\n",
    "print(\"\\nFeature categories created:\")\n",
    "print(f\"- Geographic features: {len([col for col in feature_data.columns if col.startswith(('region_', 'country_'))])}\")\n",
    "print(f\"- Sector features: {len([col for col in feature_data.columns if col.startswith('sector_')])}\")\n",
    "print(f\"- Environmental features: {len([col for col in feature_data.columns if col.startswith('env_')])}\")\n",
    "print(f\"- SDG features: {len([col for col in feature_data.columns if col.startswith(('sdg_', 'climate_'))])}\")\n",
    "print(f\"- Revenue features: {len([col for col in feature_data.columns if 'revenue' in col])}\")\n",
    "print(f\"- Sustainability features: {len([col for col in feature_data.columns if any(x in col for x in ['score', 'sustainability'])])}\")\n",
    "feature_data.to_csv('output_for_enviornmental_FeatureEngineering.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f6187-2991-4d4f-b1cb-f5e96666da29",
   "metadata": {},
   "source": [
    "#### Create and export featuring dataset for enviornmental_activities, then use this dataset to find correlations and proof hypothesis\n",
    "* move to EDAonEnvAct.ipyb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb81bd6-9d89-4ced-a252-f399d6156e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
